<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Training MXNet — part 2: CIFAR-10</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Training MXNet — part 2: CIFAR-10</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 1, we used the famous LeNet Convolutional Neural Network to reach 99+% validation accuracy in just 10 epochs. We also saw how to…
</section>
<section data-field="body" class="e-content">
<section name="6f57" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7a82" id="7a82" class="graf graf--h3 graf--leading graf--title">Training MXNet — part 2: CIFAR-10</h3><p name="1822" id="1822" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62" data-href="https://medium.com/@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62" class="markup--anchor markup--p-anchor" target="_blank">part 1</a>, we used the famous <a href="http://yann.lecun.com/exdb/lenet/" data-href="http://yann.lecun.com/exdb/lenet/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LeNet</a> Convolutional Neural Network to reach 99+% validation accuracy in just 10 epochs. We also saw how to use multiple GPUs to speed up training.</p><p name="340a" id="340a" class="graf graf--p graf-after--p">In this article, we’re going to tackle a more difficult data set: <a href="https://www.cs.toronto.edu/~kriz/cifar.html" data-href="https://www.cs.toronto.edu/~kriz/cifar.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CIFAR-10</a>. In the process, we’re going to learn a few new tricks. Read on :)</p><p name="8698" id="8698" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The CIFAR-10 data set</strong></p><p name="3dc9" id="3dc9" class="graf graf--p graf-after--p">The CIFAR-10 dataset consists of 60,000 32 x 32 colour images. They are divided in 10 classes containing 6,000 images each. There are 50,000 training images and 10,000 test images. Categories are stored in a separate metadata file.</p><figure name="2314" id="2314" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6XQqOifwnmplS22zCRRVaw.png" data-width="944" data-height="738" src="https://cdn-images-1.medium.com/max/800/1*6XQqOifwnmplS22zCRRVaw.png"><figcaption class="imageCaption">Samples images from the CIFAR-10 data set</figcaption></figure><p name="225a" id="225a" class="graf graf--p graf-after--figure">Let’s download the data set.</p><pre name="e164" id="e164" class="graf graf--pre graf-after--p">$ wget <a href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz" data-href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a><br>$ tar xfz cifar-10-python.tar.gz<br>$ ls cifar-10-batches-py/<br>batches.meta  data_batch_1  data_batch_2  data_batch_3  data_batch_4  data_batch_5  readme.html  test_batch</pre><p name="5e4f" id="5e4f" class="graf graf--p graf-after--pre">Each file contains 10,000 pickled images, which we need to turn into an array shaped (10000, 3, 32, 32). The ‘3’ dimension comes for the three RGB channels, remember? :)</p><p name="af2b" id="af2b" class="graf graf--p graf-after--p">Let’s open the first file, save its first 10 images to disk and print their category. Nothing really complicated here, except some OpenCV tricks (see comments in the code below).</p><figure name="6b21" id="6b21" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/273bef4c5b4490c687b2f92ee721b546.js"></script></figure><p name="ef6e" id="ef6e" class="graf graf--p graf-after--figure">Here’s the output.</p><pre name="6fd3" id="6fd3" class="graf graf--pre graf-after--p">(10000, 3, 32, 32)<br>10000<br>[&#39;frog&#39;, &#39;truck&#39;, &#39;truck&#39;, &#39;deer&#39;, &#39;automobile&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;cat&#39;]</pre><figure name="d78c" id="d78c" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*uHUyNXCFod1rrYsH2kU49A.png" data-width="336" data-height="35" src="https://cdn-images-1.medium.com/max/800/1*uHUyNXCFod1rrYsH2kU49A.png"><figcaption class="imageCaption">Frog, truck, truck, deer, automobile, automobile, bird, horse, ship, cat</figcaption></figure><p name="fac5" id="fac5" class="graf graf--p graf-after--figure">Now let’s load the data set.</p><h4 name="9781" id="9781" class="graf graf--h4 graf-after--p">Loading CIFAR-10 in <em class="markup--em markup--h4-em">NDArrays</em></h4><p name="46e9" id="46e9" class="graf graf--p graf-after--h4">Just like we did for the the MNIST data, the CIFAR-10 images and labels could be loaded in <em class="markup--em markup--p-em">NDArrays </em>and then fed to an <strong class="markup--strong markup--p-strong">iterator</strong>. This is how we would to it.</p><figure name="84a0" id="84a0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/ed9dd3b63cb180818368e937ce0f3e44.js"></script></figure><p name="547c" id="547c" class="graf graf--p graf-after--figure">Here’s the output.</p><pre name="984f" id="984f" class="graf graf--pre graf-after--p">(50000L, 3L, 32L, 32L)<br>(50000L,)<br>(10000L, 3L, 32L, 32L)<br>(10000L,)</pre><p name="8150" id="8150" class="graf graf--p graf-after--pre">The next logical step would be to <strong class="markup--strong markup--p-strong">bind</strong> these arrays to a <em class="markup--em markup--p-em">Module</em> and start training (just like we did for the MNIST data set). However, I’d like to show you another way to load image data: the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">RecordIO</em></strong> file.</p><h4 name="9b5c" id="9b5c" class="graf graf--h4 graf-after--p">Loading CIFAR-10 with <em class="markup--em markup--h4-em">RecordIO</em> files</h4><p name="23c9" id="23c9" class="graf graf--p graf-after--h4">Being able to load data efficiently is a very important part of MXNet: you’ll find architecture details <a href="http://mxnet.io/architecture/note_data_loading.html" data-href="http://mxnet.io/architecture/note_data_loading.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>. In a nutshell, RecordIO files allow large data sets to be <strong class="markup--strong markup--p-strong">packed</strong> and <strong class="markup--strong markup--p-strong">split</strong> in multiple files, which can then be loaded and processed in <strong class="markup--strong markup--p-strong">parallel</strong> by multiple servers for <strong class="markup--strong markup--p-strong">distributed</strong> training.</p><p name="9a0e" id="9a0e" class="graf graf--p graf-after--p">We won’t cover how to build these files today. Let’s use pre-existing files hosted on the MXNet website.</p><pre name="8856" id="8856" class="graf graf--pre graf-after--p">$ wget <a href="http://data.mxnet.io/data/cifar10/cifar10_val.rec" data-href="http://data.mxnet.io/data/cifar10/cifar10_val.rec" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://data.mxnet.io/data/cifar10/cifar10_val.rec</a><br>$ wget <a href="http://data.mxnet.io/data/cifar10/cifar10_train.rec" data-href="http://data.mxnet.io/data/cifar10/cifar10_train.rec" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://data.mxnet.io/data/cifar10/cifar10_train.rec</a></pre><p name="09c7" id="09c7" class="graf graf--p graf-after--pre">The first file contains 50,000 samples, which we’ll use for training. The second one contains 10,000 samples, which we’ll use for validation. Image resolution has been set to 28x28.</p><p name="f9b0" id="f9b0" class="graf graf--p graf-after--p">Loading these files and building an iterator is extremely simple. We just have to be careful to :</p><ul class="postList"><li name="1b3d" id="1b3d" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">match</strong> <em class="markup--em markup--li-em">data_name</em> and <em class="markup--em markup--li-em">label_name</em> with the names of the input and output layers.</li><li name="e6eb" id="e6eb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">shuffle</strong> samples in the training set in case they’ve been stored in some kind of sequence.</li></ul><figure name="fc78" id="fc78" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/3517c77503ad661b53aa5d25b0392768.js"></script></figure><p name="4983" id="4983" class="graf graf--p graf-after--figure">Data is ready for training. We’ve learned from previous examples that Convolutional Neural Networks are a good fit for object detection, so that’s where we should look.</p><h4 name="c7f2" id="c7f2" class="graf graf--h4 graf-after--p">Training vs. fine-tuning</h4><p name="1a6a" id="1a6a" class="graf graf--p graf-after--h4">In previous examples, we picked models from the <a href="http://mxnet.io/model_zoo/" data-href="http://mxnet.io/model_zoo/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model zoo</a> and retrained them <strong class="markup--strong markup--p-strong">from scratch</strong> on our data set. We’re going to do that again with the <a href="http://data.mxnet.io/models/imagenet/resnext/101-layers/" data-href="http://data.mxnet.io/models/imagenet/resnext/101-layers/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ResNext-101</a> model, but we’re going to try something different in parallel: <strong class="markup--strong markup--p-strong">fine-tuning</strong> the model.</p><p name="b51c" id="b51c" class="graf graf--p graf-after--p">Fine-tuning means that we’re going to keep all layers and pre-trained weights unchanged, except for the <strong class="markup--strong markup--p-strong">last layer</strong>: it will be removed and replaced by a new layer having the number of outputs of the new data set. Then, we will train the output layer on the new data set.</p><p name="f7e7" id="f7e7" class="graf graf--p graf-after--p">Since our model has been pre-trained on the large <a href="http://www.image-net.org/" data-href="http://www.image-net.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> data set, the rationale for fine-tuning is to benefit from the very large number of <strong class="markup--strong markup--p-strong">patterns</strong> that the model has learned while training on ImageNet. Although image sizes are quite different, it’s reasonable to expect that they will also apply to CIFAR-10.</p><h4 name="afd9" id="afd9" class="graf graf--h4 graf-after--p">Training on ResNext-101</h4><p name="a8e4" id="a8e4" class="graf graf--p graf-after--h4">Let’s first start by training the model from scratch. We’ve done this a few times before, so no difficulty here.</p><figure name="397f" id="397f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/452e3d83d09bac9cbc5e2b5ec2342c10.js"></script></figure><p name="ceb1" id="ceb1" class="graf graf--p graf-after--figure">This is the result after 100 epochs.</p><figure name="8241" id="8241" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3edr6zmvwIHxAM_NnVjYWg.png" data-width="1118" data-height="860" src="https://cdn-images-1.medium.com/max/800/1*3edr6zmvwIHxAM_NnVjYWg.png"></figure><h4 name="9622" id="9622" class="graf graf--h4 graf-after--figure">Fine-tuning on ResNext-101</h4><p name="e967" id="e967" class="graf graf--p graf-after--h4">Replacing layers sounds complicated, doesn’t it? Fortunately, the MXNet sources provide Python code to do this. It’s located in <em class="markup--em markup--p-em">example/image-classification/fine-tune.py. </em>Basically, it’s going to download the pre-trained model, remove its output layer, add a new one and start training.</p><p name="0233" id="0233" class="graf graf--p graf-after--p">This is how to use it:</p><pre name="852a" id="852a" class="graf graf--pre graf-after--p">$ python fine-tune.py <br>--pretrained-model resnext-101 --load-epoch 0000 <br>--gpus 0,1,2,3 --batch-size 128<br>--data-train cifar10_train.rec --data-val cifar10_val.rec <br>--num-examples 50000 --num-classes 10 --image-shape 3,28,28 <br>--num-epoch 300 --lr 0.05</pre><p name="da77" id="da77" class="graf graf--p graf-after--pre">This is going to download <em class="markup--em markup--p-em">resnext-101–0000.params</em> and <em class="markup--em markup--p-em">resnext-101-symbol.json</em> from the model zoo. Most of the parameters should be familiar. Here’s the result after 100 epochs.</p><figure name="3c8e" id="3c8e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gtTMHAi5--u1Fg3eUzsGaQ.png" data-width="1116" data-height="862" src="https://cdn-images-1.medium.com/max/800/1*gtTMHAi5--u1Fg3eUzsGaQ.png"><figcaption class="imageCaption">Comparing training and fine-tuning</figcaption></figure><p name="9167" id="9167" class="graf graf--p graf-after--figure">What do we see here?</p><p name="0ac0" id="0ac0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Early on</strong>, fine-tuning delivers much <strong class="markup--strong markup--p-strong">higher</strong> training and validation accuracy. This makes sense, since the model has been pre-trained. So, if you have limited time and resources, fine-tuning is definitely an interesting way to get quick results on a new data set.</p><p name="496a" id="496a" class="graf graf--p graf-after--p">Over time, fine-tuning delivers about 5% <strong class="markup--strong markup--p-strong">additional</strong> validation accuracy than training from scratch. I’m guessing that the pre-trained model generalizes better on new data thanks to the large ImageNet data set.</p><p name="e255" id="e255" class="graf graf--p graf-after--p">Last but not least, validation accuracy <strong class="markup--strong markup--p-strong">stops</strong> improving after 50 epochs or so. Surely, we can do something to improve this?</p><p name="b8aa" id="b8aa" class="graf graf--p graf-after--p graf--trailing">Yes, of course. We’ll see how in part 3 :)</p></div></div></section><section name="6704" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="5993" id="5993" class="graf graf--p graf--leading">Next:</p><ul class="postList"><li name="b55c" id="b55c" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" data-href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" class="markup--anchor markup--li-anchor" target="_blank">Part 3 — CIFAR-10 redux</a></li><li name="b7a1" id="b7a1" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" data-href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" class="markup--anchor markup--li-anchor" target="_blank">Part 4 — Distributed training</a></li><li name="9244" id="9244" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" data-href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" class="markup--anchor markup--li-anchor" target="_blank">Part 5 — Distributed training, EFS edition</a></li></ul><figure name="2979" id="2979" class="graf graf--figure graf--iframe graf-after--li"><iframe src="https://upscri.be/8f5f8b?as_embed=true" width="700" height="350" frameborder="0" scrolling="no"></iframe></figure></div><div class="section-inner sectionLayout--outsetColumn"><figure name="e80b" id="e80b" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure"><img class="graf-image" data-image-id="1*bQlRSzFHJEmF4Q7PyrLgng.gif" data-width="725" data-height="71" src="https://cdn-images-1.medium.com/max/1200/1*bQlRSzFHJEmF4Q7PyrLgng.gif"></figure></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="3"><figure name="c8b7" id="c8b7" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 33.333%;"><a href="https://medium.com/becoming-human/artificial-intelligence-communities-c305f28e674c" data-href="https://medium.com/becoming-human/artificial-intelligence-communities-c305f28e674c" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"><img class="graf-image" data-image-id="1*2f7OqE2AJK1KSrhkmD9ZMw.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*2f7OqE2AJK1KSrhkmD9ZMw.png"></a></figure><figure name="f924" id="f924" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.333%;"><a href="https://upscri.be/8f5f8b" data-href="https://upscri.be/8f5f8b" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*v-PpfkSWHbvlWWamSVHHWg.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*v-PpfkSWHbvlWWamSVHHWg.png"></a></figure><figure name="cf51" id="cf51" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure graf--trailing" style="width: 33.333%;"><a href="https://medium.com/becoming-human/write-for-us-48270209de63" data-href="https://medium.com/becoming-human/write-for-us-48270209de63" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"><img class="graf-image" data-image-id="1*Wt2auqISiEAOZxJ-I7brDQ.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*Wt2auqISiEAOZxJ-I7brDQ.png"></a></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/c7b0b729c33c"><time class="dt-published" datetime="2017-04-24T07:33:19.064Z">April 24, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>