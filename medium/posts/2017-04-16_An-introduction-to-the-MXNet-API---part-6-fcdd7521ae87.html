<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>An introduction to the MXNet API — part 6</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">An introduction to the MXNet API — part 6</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 5, we used three different pre-trained models for object detection and compared them using a couple of images.
</section>
<section data-field="body" class="e-content">
<section name="1275" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2601" id="2601" class="graf graf--h3 graf--leading graf--title">An introduction to the MXNet API — part 6</h3><p name="7b00" id="7b00" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" class="markup--anchor markup--p-anchor" target="_blank">part 5</a>, we used three different pre-trained models for object detection and compared them using a couple of images.</p><p name="b3fe" id="b3fe" class="graf graf--p graf-after--p">One of the things we learned is that models have <strong class="markup--strong markup--p-strong">very different memory requirements</strong>, the most frugal model being Inception v3 with “only” 43MB. Obviously, this begs the question: “can we run this on something really small, say a Raspberry Pi?”. Well, let’s find out!</p><h4 name="1a3a" id="1a3a" class="graf graf--h4 graf-after--p">Building MXNet on a Pi</h4><p name="aab7" id="aab7" class="graf graf--p graf-after--h4">There’s an <a href="http://mxnet.io/get_started/raspbian_setup.html" data-href="http://mxnet.io/get_started/raspbian_setup.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">official tutorial</a>, but I found it to be missing some steps, so here’s my version. It works fine on a Raspberry Pi 3 running the latest Raspbian.</p><pre name="2e9d" id="2e9d" class="graf graf--pre graf-after--p">$ uname -a<br>Linux raspberrypi 4.4.50-v7+ #970 SMP Mon Feb 20 19:18:29 GMT 2017 armv7l GNU/Linux</pre><p name="28a7" id="28a7" class="graf graf--p graf-after--pre">First, let’s add all necessary <strong class="markup--strong markup--p-strong">dependencies</strong>.</p><pre name="95d3" id="95d3" class="graf graf--pre graf-after--p">$ sudo apt-get update<br>$ sudo apt-get -y install git cmake build-essential g++-4.8 c++-4.8 liblapack* libblas* libopencv* python-opencv libssl-dev screen</pre><p name="2c6d" id="2c6d" class="graf graf--p graf-after--pre">Then, let’s <strong class="markup--strong markup--p-strong">clone</strong> the MXNet repository and <strong class="markup--strong markup--p-strong">checkout</strong> the latest stable release. Don’t miss this last step, as I found HEAD to be broken most of the time (<em class="markup--em markup--p-em">Update 30/04/17: the MXNet dev team got in touch and informed me that Continuous Integration is now in place. I can confirm that HEAD now builds fine. Well done, guys</em>).</p><pre name="d513" id="d513" class="graf graf--pre graf-after--p">$ git clone https://github.com/dmlc/mxnet.git --recursive<br>$ cd mxnet<br># List tags: v0.9.3a is the latest at the time of writing<br>$ git tag -l<br>$ git checkout tags/v0.9.3a</pre><p name="9f8b" id="9f8b" class="graf graf--p graf-after--pre">MXNet is able to load and save data in S3, so let’s enable this feature, it might come in handy later on. MXNet also supports HDFS but you need to install Hadoop locally, so… no :)</p><p name="8f81" id="8f81" class="graf graf--p graf-after--p">We could just run <em class="markup--em markup--p-em">make</em> but given the limited processing power of the Pi, the build is gonna take a while: you don’t want to it to be interrupted if your SSH session times out! <a href="https://www.gnu.org/software/screen/" data-href="https://www.gnu.org/software/screen/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Screen</em></a> is going to solve this.</p><p name="52cf" id="52cf" class="graf graf--p graf-after--p">To speed things up a little, we can run a parallel make on 2 cores (out of 4). I wouldn’t recommend using more, as my Pi became unresponsive when I tried it.</p><pre name="d345" id="d345" class="graf graf--pre graf-after--p">$ export USE_S3=1<br>$ screen make -j2</pre><p name="e44a" id="e44a" class="graf graf--p graf-after--pre">This should last about an hour. The last step is to <strong class="markup--strong markup--p-strong">install</strong> the library and its Python bindings.</p><pre name="9e19" id="9e19" class="graf graf--pre graf-after--p">$ cd python<br>$ sudo python setup.py install<br>$ python<br>Python 2.7.9 (default, Sep 17 2016, 20:26:04)<br>[GCC 4.9.2] on linux2<br>Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.<br>&gt;&gt;&gt; import mxnet as mx<br>&gt;&gt;&gt; mx.__version__<br>&#39;0.9.3a&#39;</pre><h4 name="848f" id="848f" class="graf graf--h4 graf-after--pre">Loading models</h4><p name="9129" id="9129" class="graf graf--p graf-after--h4">Once we’ve copied the model files to the Pi, we need to make sure that we can actually <strong class="markup--strong markup--p-strong">load</strong> them. Let’s reuse the exact same code we wrote in <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" class="markup--anchor markup--p-anchor" target="_blank">part 5</a>. For the record, the Pi is in CLI mode with about 580MB of free memory. All data is stored on a 32GB SD card.</p><p name="4ff0" id="4ff0" class="graf graf--p graf-after--p">Let’s try to load VGG16.</p><pre name="eef1" id="eef1" class="graf graf--pre graf-after--p">&gt;&gt;&gt; vgg16,categories = init(&quot;vgg16&quot;)<br>terminate called after throwing an instance of &#39;std::bad_alloc&#39;<br>  what():  std::bad_alloc</pre><p name="e2d3" id="e2d3" class="graf graf--p graf-after--pre">Ouch! VGG16 is <strong class="markup--strong markup--p-strong">too large</strong> to fit in memory. Let’s try ResNet-152.</p><pre name="90d5" id="90d5" class="graf graf--pre graf-after--p">&gt;&gt;&gt; resnet152,categories = init(&quot;resnet-152&quot;)<br>Loaded in 11056.10 milliseconds</pre><pre name="257f" id="257f" class="graf graf--pre graf-after--pre">&gt;&gt; print predict(&quot;kreator.jpg&quot;,resnet152,categories,5)<br>Predicted in 7.98 milliseconds<br>[(0.87835813, &#39;n04296562 stage&#39;), (0.045634001, &#39;n03759954 microphone, mike&#39;), (0.035906471, &#39;n03272010 electric guitar&#39;), (0.021166906, &#39;n04286575 spotlight, spot&#39;), (0.0054096784, &#39;n02676566 acoustic guitar&#39;)]</pre><p name="6604" id="6604" class="graf graf--p graf-after--pre">ResNet-152 loads successfully in about 10 seconds and predicts in less than 10 milliseconds. Let’s move on to Inception v3.</p><pre name="47da" id="47da" class="graf graf--pre graf-after--p">&gt;&gt;&gt; inceptionv3,categories = init(&quot;Inception-BN&quot;)<br>Loaded in 2137.62 milliseconds</pre><pre name="4ae1" id="4ae1" class="graf graf--pre graf-after--pre">&gt;&gt; print predict(&quot;kreator.jpg&quot;,resnet152,categories,5)<br>Predicted in 2.35 milliseconds<br>[(0.4685601, &#39;n04296562 stage&#39;), (0.40474886, &#39;n03272010 electric guitar&#39;), (0.073685646, &#39;n04456115 torch&#39;), (0.011639798, &#39;n03250847 drumstick&#39;), (0.011014056, &#39;n02676566 acoustic guitar&#39;)]</pre><p name="bfa0" id="bfa0" class="graf graf--p graf-after--pre">On a constrained device like the Pi, model differences are much more obvious! Inception v3 loads <strong class="markup--strong markup--p-strong">much faster</strong> iand predicts in a few milliseconds. Even when the model is loaded, there’s <strong class="markup--strong markup--p-strong">plenty</strong> of RAM left on the PI to run an actual application, so it’s definitely an interesting candidate for embedded apps. Let’s keep going :)</p><h4 name="dd3c" id="dd3c" class="graf graf--h4 graf-after--p">Capturing images using the Pi camera</h4><p name="0e46" id="0e46" class="graf graf--p graf-after--h4">One of the best gadgets you can add to the Raspberry Pi is a <a href="https://www.raspberrypi.org/learning/getting-started-with-picamera/" data-href="https://www.raspberrypi.org/learning/getting-started-with-picamera/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">camera module</strong></a>. It couldn’t be simpler to use!</p><pre name="4851" id="4851" class="graf graf--pre graf-after--p">&gt;&gt;&gt; inceptionv3,categories = init(&quot;Inception-BN&quot;)</pre><pre name="2bc1" id="2bc1" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; import picamera<br>&gt;&gt;&gt; camera = picamera.PiCamera()<br>&gt;&gt;&gt; filename = &#39;/home/pi/cap.jpg&#39;</pre><pre name="3bd9" id="3bd9" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; print predict(filename, inceptionv3, categories, 5)</pre><p name="7fe8" id="7fe8" class="graf graf--p graf-after--pre">Here’s an example.</p><figure name="6587" id="6587" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZRHWR2Bzb-S0mccRVTQcwQ.jpeg" data-width="1280" data-height="720" src="https://cdn-images-1.medium.com/max/800/1*ZRHWR2Bzb-S0mccRVTQcwQ.jpeg"></figure><pre name="9798" id="9798" class="graf graf--pre graf-after--figure">Predicted in 12.90 milliseconds<br>[(0.95071173, &#39;n04074963 remote control, remote&#39;), (0.013508897, &#39;n04372370 switch, electric switch, electrical switch&#39;), (0.013224524, &#39;n03602883 joystick&#39;), (0.00399205, &#39;n04009552 projector&#39;), (0.0036674738, &#39;n03777754 modem&#39;)]</pre><p name="7461" id="7461" class="graf graf--p graf-after--pre">Really cool!</p><h4 name="b439" id="b439" class="graf graf--h4 graf-after--p">Adding a couple of Amazon AI services, because why not?</h4><p name="f085" id="f085" class="graf graf--p graf-after--h4">Of course, I cannot resist running the same picture through <a href="https://aws.amazon.com/rekognition/" data-href="https://aws.amazon.com/rekognition/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon Rekognition</a> using the Python scripts I wrote a while ago (<a href="https://medium.com/@julsimon/a-hands-on-look-at-the-amazon-rekognition-api-e30e19e7d88b" data-href="https://medium.com/@julsimon/a-hands-on-look-at-the-amazon-rekognition-api-e30e19e7d88b" class="markup--anchor markup--p-anchor" target="_blank">article</a>, <a href="https://github.com/juliensimon/aws/tree/master/rekognition" data-href="https://github.com/juliensimon/aws/tree/master/rekognition" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">code</a>).</p><pre name="3e57" id="3e57" class="graf graf--pre graf-after--p">$ ./rekognitionDetect.py jsimon-public cap.jpg copy<br>Label Remote Control, confidence: 94.7508468628</pre><p name="999f" id="999f" class="graf graf--p graf-after--pre">Good job, Rekognition. Now… wouldn’t it be nice it the Pi actually told us what the picture is about? It’s not too complicated to add <a href="https://aws.amazon.com/polly/" data-href="https://aws.amazon.com/polly/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon Polly</a> to the mix (<a href="https://medium.com/@julsimon/amazon-polly-hello-world-literally-812de2c620f4" data-href="https://medium.com/@julsimon/amazon-polly-hello-world-literally-812de2c620f4" class="markup--anchor markup--p-anchor" target="_blank">article</a>).</p><blockquote name="84c9" id="84c9" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">Amazon Rekognition and Amazon Polly are <strong class="markup--strong markup--blockquote-strong">managed services</strong> based on Deep Learning technology. We don’t have to worry about models or infrastructure: all we have to do is to invoke an API.</blockquote><p name="0b10" id="0b10" class="graf graf--p graf-after--blockquote">So, here’s a video of my Raspberry Pi performing real-time object detection with the Inception v3 model running in MXNet, and describing what it sees with Amazon Polly.</p><figure name="9ec7" id="9ec7" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/eKGYFfr9MKI?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section><section name="deff" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="dfc1" id="dfc1" class="graf graf--p graf--leading">Well, we’ve come a long way! In these 6 articles, we learned how to:</p><ul class="postList"><li name="d81b" id="d81b" class="graf graf--li graf-after--p">manage data with <em class="markup--em markup--li-em">NDArrays</em>,</li><li name="ae42" id="ae42" class="graf graf--li graf-after--li">define models with <em class="markup--em markup--li-em">Symbols</em>,</li><li name="6fbc" id="6fbc" class="graf graf--li graf-after--li">run predictions with <em class="markup--em markup--li-em">Modules</em>,</li><li name="9e62" id="9e62" class="graf graf--li graf-after--li">load and compare pre-trained models for object detection,</li><li name="029a" id="029a" class="graf graf--li graf-after--li">use a pre-trained model on a Raspberry Pi in real-time.</li></ul><p name="f6b6" id="f6b6" class="graf graf--p graf-after--li">We focused on Convolutional Neural Networks for object detection, but there is much more to MXNet, so expect more articles!</p><p name="6e76" id="6e76" class="graf graf--p graf-after--p graf--trailing">This concludes this series. I hope you enjoyed it and learned useful stuff along the way.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/fcdd7521ae87"><time class="dt-published" datetime="2017-04-16T14:21:04.136Z">April 16, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>