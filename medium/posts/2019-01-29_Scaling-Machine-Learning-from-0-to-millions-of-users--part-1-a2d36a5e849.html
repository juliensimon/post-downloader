<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Scaling Machine Learning from 0 to millions of users, part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Scaling Machine Learning from 0 to millions of users, part 1</h1>
</header>
<section data-field="subtitle" class="p-summary">
Breaking out of the laptop
</section>
<section data-field="body" class="e-content">
<section name="029b" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0a14" id="0a14" class="graf graf--h3 graf--leading graf--title">Scaling Machine Learning from 0 to millions of users — part 1</h3><h4 name="29c3" id="29c3" class="graf graf--h4 graf-after--h3 graf--subtitle">Breaking out of the laptop</h4><p name="8eca" id="8eca" class="graf graf--p graf-after--h4">I suppose most Machine Learning (ML) models are conceived on a whiteboard or a napkin, and born on a laptop. As the fledgling creatures start babbling their first predictions, we’re filled with pride and high hopes for their future abilities. Alas, we know deep down in our heart that that not all of them will be successful, far from it.</p><p name="1a62" id="1a62" class="graf graf--p graf-after--p">A small number fail us quickly as we build them. Others look promising, and demonstrate some level of predictive power. We are then faced with the grim challenge of deploying them in a production environment, where they’ll either prove their legendary valour or die an inglorious death.</p><figure name="213f" id="213f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*at743M34p8AdMI8S.jpg" data-width="890" data-height="430" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*at743M34p8AdMI8S.jpg"><figcaption class="imageCaption">One day, your models will rule the world… if you read all these posts and pay attention ;)</figcaption></figure><p name="5949" id="5949" class="graf graf--p graf-after--figure">In this series of opinionated posts, we’ll discuss <strong class="markup--strong markup--p-strong">how to train ML models and deploy them to production, from humble beginnings to world domination</strong>. Along the way, we’ll try to take justified and reasonable steps, fighting the evil forces of over-engineering, Hype Driven Development and “why don’t you just use XYZ?”.</p><blockquote name="c4f5" id="c4f5" class="graf graf--pullquote graf-after--p">Enjoy the safe comfort of your Data Science sandbox while you can, and prepare yourself for the cold, harsh world of production.</blockquote><h3 name="7260" id="7260" class="graf graf--h3 graf-after--pullquote"><strong class="markup--strong markup--h3-strong">Day 0</strong></h3><p name="afd8" id="afd8" class="graf graf--p graf-after--h3">So you want to build a ML model. Hmmm. Let’s pause for a minute and consider this:</p><ul class="postList"><li name="c0f3" id="c0f3" class="graf graf--li graf-after--p">Could your business problem be addressed by a <strong class="markup--strong markup--li-strong">high-level AWS service</strong>, such as <a href="http://aws.amazon.com/rekognition" data-href="http://aws.amazon.com/rekognition" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon Rekognition</a>, <a href="http://aws.amazon.com/rekognition" data-href="http://aws.amazon.com/rekognition" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon Polly</a>, etc.?</li><li name="cabe" id="cabe" class="graf graf--li graf-after--li">Or by the <a href="https://medium.com/@julsimon/applying-machine-learning-to-aws-services-9768f926f11f" data-href="https://medium.com/@julsimon/applying-machine-learning-to-aws-services-9768f926f11f" class="markup--anchor markup--li-anchor" target="_blank">growing list of applied ML features</a> embedded in other AWS services?</li></ul><p name="aeb3" id="aeb3" class="graf graf--p graf-after--li">Don’t wave this off: <strong class="markup--strong markup--p-strong">no Machine Learning is easier to manage than no Machine Learning</strong>. Figuring a way to use high-level services could save you weeks of work, maybe months.</p><h4 name="a08e" id="a08e" class="graf graf--h4 graf-after--p">If the answer is “yes”</h4><p name="914a" id="914a" class="graf graf--p graf-after--h4">Please ask yourself:</p><ul class="postList"><li name="7e89" id="7e89" class="graf graf--li graf-after--p">Why would you go through all the trouble of building a redundant custom solution?</li><li name="a8c6" id="a8c6" class="graf graf--li graf-after--li">Are you really “<em class="markup--em markup--li-em">missing features</em>”? What’s the <strong class="markup--strong markup--li-strong">real</strong> business impact?</li><li name="3394" id="3394" class="graf graf--li graf-after--li">Do you really need “<em class="markup--em markup--li-em">more accuracy</em>” How do you know <strong class="markup--strong markup--li-strong">you</strong> could reach it?</li></ul><p name="8a4f" id="8a4f" class="graf graf--p graf-after--li">If you’re unsure, why not run a <strong class="markup--strong markup--p-strong">quick PoC </strong>with your own data? These services are fully-managed (no… more… servers) and very easy to integrate in any application. It shouldn’t take a lot of time to figure them out, and you would then have solid data to make an <strong class="markup--strong markup--p-strong">educated decision</strong> on whether you really need to train your own model or not.</p><blockquote name="a400" id="a400" class="graf graf--blockquote graf-after--p">If these services work well enough for you, congratulations, you’re mostly done! If you decide to build, I’d love to hear your feedback. Please get in touch.</blockquote><h4 name="4cf6" id="4cf6" class="graf graf--h4 graf-after--blockquote"><strong class="markup--strong markup--h4-strong">If this answer is “no”</strong></h4><p name="69c4" id="69c4" class="graf graf--p graf-after--h4">Please ask yourself the question again! Most of us have an amazing capability to twist reality and deceive ourselves :) If the honest answer is really “no”, then I’d still recommend thinking about <strong class="markup--strong markup--p-strong">subprocesses</strong> where you could use the high-level services, e.g. :</p><ul class="postList"><li name="7176" id="7176" class="graf graf--li graf-after--p">using <a href="http://aws.amazon.com/translate" data-href="http://aws.amazon.com/translate" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon Translate</a> for supported language pairs and using your own solution for the rest.</li><li name="2f73" id="2f73" class="graf graf--li graf-after--li">using <a href="http://aws.amazon.com/rekognition" data-href="http://aws.amazon.com/rekognition" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon Rekognition</a> to detect faces before feeding them to your model,</li><li name="147b" id="147b" class="graf graf--li graf-after--li">using <a href="http://aws.amazon.com/textract" data-href="http://aws.amazon.com/textract" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon Textract</a> to extract text before feeding it to your NLP model.</li></ul><p name="3885" id="3885" class="graf graf--p graf-after--li">This isn’t about pitching AWS services (do I look like a salesperson?). I’m simply <strong class="markup--strong markup--p-strong">trying to save you from reinventing the wheel </strong>(or parts of the wheel): you should really be <strong class="markup--strong markup--p-strong">focusing on the business problem</strong> at hand, instead of building a house of cards that you read about in a blog post or saw at a conference. Yes, it may look great on your resume, and the wheel is initially a fun merry-go-round… and then, it turns into <strong class="markup--strong markup--p-strong">the Wheel of Pain, you’re chained to it and someone else is holding the whip</strong>.</p><figure name="4e63" id="4e63" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*MK8gZmjzXiAIkTee.png" data-width="1200" data-height="800" src="https://cdn-images-1.medium.com/max/800/0*MK8gZmjzXiAIkTee.png"><figcaption class="imageCaption">Why did I blindly trust that meetup talk? Crom! Help me escape and bash that guy’s skull with his laptop.</figcaption></figure><p name="9e54" id="9e54" class="graf graf--p graf-after--figure">Anyway, enough negativity :) You do need a model, let’s move on.</p><h3 name="0adf" id="0adf" class="graf graf--h3 graf-after--p">Day 1: one user (you)</h3><p name="06ad" id="06ad" class="graf graf--p graf-after--h3">We’ll start our journey at the stage where you’ve trained a model on your local machine (or a local dev server), using a popular open source library like <a href="https://scikit-learn.org" data-href="https://scikit-learn.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">scikit-learn</a>, <a href="https://www.tensorflow.org" data-href="https://www.tensorflow.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> or <a href="https://mxnet.incubator.apache.org" data-href="https://mxnet.incubator.apache.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a>. Maybe you’ve even implemented your own algorithm (Data scientists, you devils).</p><p name="2766" id="2766" class="graf graf--p graf-after--p">You’ve measured the model’s accuracy using your test set, and things look good. Now you’d like to deploy the model to production in order to check its actual behaviour, run A/B tests, etc. Where to start?</p><h4 name="e3c4" id="e3c4" class="graf graf--h4 graf-after--p">Batch prediction or real-time prediction?</h4><p name="bee5" id="bee5" class="graf graf--p graf-after--h4">First, you should figure out whether your application requires <strong class="markup--strong markup--p-strong">batch prediction</strong> (i.e. collect a large number data points, process them periodically and store results somewhere), or <strong class="markup--strong markup--p-strong">real-time prediction</strong> (i.e. send a data point to a web service and receive an immediate prediction ). The reason why I bring this point early on is because it has a large impact on deployment complexity.</p><p name="bd97" id="bd97" class="graf graf--p graf-after--p">At first sight, real-time prediction sounds more appealing (because… real-time, yeah!), but it also comes with stronger requirements, inherent in web services: high availability, the ability to handle traffic bursts, etc. Batch is more relaxed, as it only needs to run every now and then: as long as you don’t lose data, no one will see if it’s broken in between ;)</p><p name="9357" id="9357" class="graf graf--p graf-after--p">Scaling is not a concern right now: all you care about is deploying your model, kicking the tires, running some performance tests, etc. From my experience, <strong class="markup--strong markup--p-strong">you’ve probably taken the shortest route and deployed everything to a single Amazon EC2 instance</strong>. Everybody knows a bit of Linux CLI, and you read somewhere that using “IaaS will protect you from evil vendor lock-in”. Ha! EC2 it is, then!</p><blockquote name="49ea" id="49ea" class="graf graf--blockquote graf-after--p">I hear screams of horror and disbelief across the AWS time-space continuum, and maybe some snarky comments along the lines of “oh this is totally stupid, no one actually does that!”. Well, I’ll put money on the fact that this is by far how most people get started. Congrats if you didn’t, but please let me show these good people which way is out before they really hurt themselves ;)</blockquote><p name="f8c6" id="f8c6" class="graf graf--p graf-after--blockquote">And so, staring into my magic mirror, I see…</p><h4 name="75f3" id="75f3" class="graf graf--h4 graf-after--p">Batch prediction</h4><p name="bff1" id="bff1" class="graf graf--p graf-after--h4">You’ve copied your model, your batch script and your application to an EC2 instance. Your batch script runs periodically as a cron job, and saves predicted data to local storage. Your application loads both the model and initial predicted data at startup, and uses it to do whatever it has to do. It also periodically checks for updated predictions, and loads them whenever they’re available.</p><h4 name="b9cb" id="b9cb" class="graf graf--h4 graf-after--p">Real-time prediction</h4><p name="3b3d" id="3b3d" class="graf graf--p graf-after--h4">You’ve embedded the model in your application, loading it at startup and serving predictions using all kinds of data (user input, files, APIs, etc.).</p><p name="6938" id="6938" class="graf graf--p graf-after--p">One way or the other, you’re now running predictions in the cloud, and life is good. You celebrate with a pint of stout… or maybe gluten-free, fair-trade, organic soy milk latte, because it’s 2019 after all.</p><h3 name="a128" id="a128" class="graf graf--h3 graf-after--p">Week 1: one sorry user (you)</h3><p name="eac3" id="eac3" class="graf graf--p graf-after--h3">The model predicts nicely, and you’d like to invest more time in collecting more data and adding features. Unfortunately, it didn’t take long for things to go wrong and you’re now <strong class="markup--strong markup--p-strong">bogged down in all kinds of issues</strong> (non exhaustive list below):</p><ul class="postList"><li name="8262" id="8262" class="graf graf--li graf-after--p">Training on your laptop and deploying manually to the cloud is painful and error-prone.</li><li name="9249" id="9249" class="graf graf--li graf-after--li">You accidentally terminated your EC2 instance and had to reinstall everything from scratch.</li><li name="a64c" id="a64c" class="graf graf--li graf-after--li">You ‘<em class="markup--em markup--li-em">pip install</em>’-ed a Python library and now your EC2 instance is all messed up.</li><li name="83ef" id="83ef" class="graf graf--li graf-after--li">You had to manually install two other instances for your colleagues, and now you can’t really be sure that you’re all using identical environments.</li><li name="59c4" id="59c4" class="graf graf--li graf-after--li">Your first load test failed, but you’re not sure what to blame: application? model? the ancients wizards of Acheron?</li><li name="a93b" id="a93b" class="graf graf--li graf-after--li">You’d like to implement the same algorithm in TensorFlow, and maybe Apache MXNet too: more environments, more deployments. No time for that.</li><li name="c3a8" id="c3a8" class="graf graf--li graf-after--li">And of course, everyone’s favorite: Sales have heard that “your product now has AI capabilities”. You’re terrified that they could sell it to a customer and ask you to go live at scale next week.</li></ul><p name="ebb5" id="ebb5" class="graf graf--p graf-after--li">The list goes on. It would be funny if it wasn’t real (feel free to add your own examples in the comments). All of the sudden, this ML adventure doesn’t sound as exciting, does it? <strong class="markup--strong markup--p-strong">You’re spending most of your time on firefighting, not on building the best possible model</strong>. This can’t go on!</p><figure name="9908" id="9908" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*e2Y4pLSeeGOT0tKv" data-width="1200" data-height="777" src="https://cdn-images-1.medium.com/max/800/0*e2Y4pLSeeGOT0tKv"><figcaption class="imageCaption">I’ve revoked your IAM credentials on ‘<em class="markup--em markup--figure-em">TerminateInstances</em>’. Yes, even in the dev account. Any questions?</figcaption></figure><h3 name="2343" id="2343" class="graf graf--h3 graf-after--figure">Week 2: fighting back</h3><p name="fede" id="fede" class="graf graf--p graf-after--h3">Someone on the team watched this really cool AWS video, featuring a new ML service called <a href="http://aws.amazon.com/sagemaker" data-href="http://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a>. You make a mental note of it, but right now, there’s no time to rebuild everything: Sales is breathing down your neck, you have a customer demo in a few days, and you need to harden the existing solution.</p><p name="03fb" id="03fb" class="graf graf--p graf-after--p">Chances are, you don’t have a mountain of data yet: training can wait. You need to focus on making prediction reliable. Here are some solid techniques measures that won’t take more than a few days to implement.</p><h4 name="c5d0" id="c5d0" class="graf graf--h4 graf-after--p">Use the Deep Learning AMI</h4><p name="d4f3" id="d4f3" class="graf graf--p graf-after--h4">Maintained by AWS, this <a href="https://aws.amazon.com/machine-learning/amis/" data-href="https://aws.amazon.com/machine-learning/amis/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon Machine Image</a> comes <strong class="markup--strong markup--p-strong">pre-installed</strong> with a lot of tools and libraries that you’ll probably need: open source, NVIDIA drivers, etc. Not having to manage them will save you a lot of time, and will also guarantee that your multiple instances run with the same setup.</p><p name="33ce" id="33ce" class="graf graf--p graf-after--p">The AMI also comes with the <a href="https://conda.io/en/latest/" data-href="https://conda.io/en/latest/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Conda</a> <strong class="markup--strong markup--p-strong">dependency and environment manager</strong>, which lets you quickly and easily create many isolated environments: that’s a great way to test your code with different Python versions or different libraries, without unexpectedly clobbering everything.</p><p name="114a" id="114a" class="graf graf--p graf-after--p">Last but not least, this AMI is <strong class="markup--strong markup--p-strong">free of charge</strong>, and just like any other AMI, you can customize if you *really* have to.</p><h4 name="38de" id="38de" class="graf graf--h4 graf-after--p">Break the monolith</h4><p name="8b5e" id="8b5e" class="graf graf--p graf-after--h4">Your application code and your prediction code have <strong class="markup--strong markup--p-strong">different requirements</strong>. Unless you have a compelling reason to do so (ultra low latency might be one), they shouldn’t live under the same roof. Let’s look at some reasons why:</p><ul class="postList"><li name="7372" id="7372" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Deployment</strong>: do you want to restart or update your app every time you update the model? Or ping your app to reload it or whatever? No no no no. Keep it simple: when it comes to decoupling, <strong class="markup--strong markup--li-strong">nothing beats building separate services</strong>.</li><li name="7674" id="7674" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Performance</strong>: what if your application code runs best on memory-intensive instances and your ML model requires a GPU? How will you handle that trade-off? Why would you favour one or the other? Separating them lets you <strong class="markup--strong markup--li-strong">pick the best instance type for each use case</strong>.</li><li name="3df6" id="3df6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Scalability</strong>: what if your application code and your model have different scalability profiles? It would be a shame to scale out on GPU instances because a small piece of your application code is running hot… Again, it’s better to keep things separated, this will help take the most <strong class="markup--strong markup--li-strong">appropriate scaling decisions</strong> as well as reduce cost.</li></ul><p name="f5da" id="f5da" class="graf graf--p graf-after--li">Now, what about pre-processing / post-processing code, i.e. actions that you need to take on the data just before and just after predicting. Where should it go? It’s hard to come up with a definitive answer: I’d say that <strong class="markup--strong markup--p-strong">model-independent actions</strong> (formatting, logging, etc.) should stay in the application, whereas <strong class="markup--strong markup--p-strong">model-dependent actions</strong> (feature engineering) should stay close to the model to avoid deployment inconsistencies.</p><h4 name="238c" id="238c" class="graf graf--h4 graf-after--p">Build a prediction service</h4><p name="d274" id="d274" class="graf graf--p graf-after--h4">Separating the prediction code from the application code doesn’t have to be painful, and you can reuse <strong class="markup--strong markup--p-strong">solid, scalable tools</strong> to build a prediction service. Let’s look at some options:</p><ul class="postList"><li name="ba89" id="ba89" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Scikit-learn</strong>: when it comes to building web services in Python, I’m a big fan of <a href="http://flask.pocoo.org" data-href="http://flask.pocoo.org" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Flask</a>. It’s neat, simple and it scales well. No need to look further IMHO. You code would look something like that.</li></ul><pre name="9f64" id="9f64" class="graf graf--pre graf-after--li"># My awesome API<br>from flask import Flask<br>import pickle</pre><pre name="c1a2" id="c1a2" class="graf graf--pre graf-after--pre">app = Flask(__name__)<br>model = pickle.load(open(&quot;my_awesome_model.sav&quot;, &#39;rb&#39;))<br>...<br><code class="markup--code markup--pre-code">@app.route(&#39;/predict&#39;, methods=[&#39;POST&#39;])<br></code>def predict():<br>    # Grab data from the HTTP request<br>    ...<br>    model.predict(...)<br>    ...</pre><ul class="postList"><li name="1225" id="1225" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">TensorFlow</strong>: no coding required! You can use <a href="https://www.tensorflow.org/serving/" data-href="https://www.tensorflow.org/serving/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">TensorFlow Serving</strong></a> to serve predictions at scale. Once you’ve trained your model and saved it to the proper format, all it takes to serve predictions is:</li></ul><pre name="34b5" id="34b5" class="graf graf--pre graf-after--li"><code class="markup--code markup--pre-code">docker run -p 8500:8500 \<br>--mount type=bind,source=/tmp/myModel,target=/models/myModel \<br>-e MODEL_NAME=myModel -t tensorflow/serving &amp;</code></pre><ul class="postList"><li name="2b60" id="2b60" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">Apache MXNet</strong>: in a similar way, Apache MXNet provides a <a href="https://github.com/awslabs/mxnet-model-server" data-href="https://github.com/awslabs/mxnet-model-server" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">model server</strong>,</a> able to serve MXNet and <a href="https://onnx.ai/" data-href="https://onnx.ai/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">ONNX</strong></a> models (the latter is a common format supported by PyTorch, Caffe2 and more). It can either run as a stand-alone application, or <a href="https://github.com/awslabs/mxnet-model-server/blob/master/docker/README.md" data-href="https://github.com/awslabs/mxnet-model-server/blob/master/docker/README.md" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">inside a Docker container</a>.</li></ul><p name="5ab6" id="5ab6" class="graf graf--p graf-after--li">Both model servers are pre-installed on the <strong class="markup--strong markup--p-strong">Deep Learning AMI: </strong>that’s another reason to use it. To keep things simple, you could leave your pre/post-processing in the application and invoke the model deployed by the model server. A word of warning, however: these models servers implement neither authentication nor throttling, so please make sure not to expose them directly to Internet traffic.</p><ul class="postList"><li name="0e66" id="0e66" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Anything else</strong>: if you’re using another environment (say, custom code) or non-web architectures (say, message passing), the same pattern should apply: build a separate service that can be <strong class="markup--strong markup--li-strong">deployed and scaled independently</strong>.</li></ul><h4 name="0d4e" id="0d4e" class="graf graf--h4 graf-after--li">(Optional) Containerize your application</h4><p name="77ca" id="77ca" class="graf graf--p graf-after--h4">Since you’ve decided to split your code, I would definitely recommend that you use the opportunity to package the different pieces in Docker containers: one for <strong class="markup--strong markup--p-strong">training</strong>, one for <strong class="markup--strong markup--p-strong">prediction</strong>, one (or more) for the <strong class="markup--strong markup--p-strong">application</strong>. It’s not strictly necessary at this stage, but if you can spare the time, I believe the premature investment is worth it.</p><blockquote name="4746" id="4746" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">If you’ve been living under a rock or never really paid attention to containers, now’s probably the time to catch up:) I highly recommend running the <a href="https://docs.docker.com/get-started/" data-href="https://docs.docker.com/get-started/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Docker tutorial</a>, which will teach you everything you need to know for our purpose.</blockquote><p name="fa9c" id="fa9c" class="graf graf--p graf-after--blockquote">Containers make it easy to <strong class="markup--strong markup--p-strong">move code across different environments</strong> (dev, test, prod, etc.) and instances. They solve all kinds of dependency issues, which tend to pop up even if you’re only managing a small number of instances. Later on, containers will also be a pre-requisite for larger-scale solutions such as Docker clusters or Amazon SageMaker.</p><h3 name="2d30" id="2d30" class="graf graf--h3 graf-after--p">End of week 2</h3><p name="a52d" id="a52d" class="graf graf--p graf-after--h3">After a rough start, things are looking up!</p><ul class="postList"><li name="7da4" id="7da4" class="graf graf--li graf-after--p">The Deep Learning AMI provides a stable, well-maintained foundation to build on.</li><li name="47a3" id="47a3" class="graf graf--li graf-after--li">Containers help you move and deploy your application with much less infrastructure drama than before.</li><li name="98b1" id="98b1" class="graf graf--li graf-after--li">Prediction now lives outside of your application, making testing, deployment and scaling simpler.</li><li name="4c6a" id="4c6a" class="graf graf--li graf-after--li">If you can use them, model servers save you most of the trouble of writing a prediction service.</li></ul><p name="de8a" id="de8a" class="graf graf--p graf-after--li">Still, don’t get too excited. Yes, we’re back on track and ready to for bigger things, but there’s still a ton of work to do. What about <strong class="markup--strong markup--p-strong">scaling prediction to multiple instances,</strong> <strong class="markup--strong markup--p-strong">high availability</strong>, <strong class="markup--strong markup--p-strong">managing cost</strong>, etc. And what should we do when mountains of training data start piling up? <strong class="markup--strong markup--p-strong">Face it, we’ve barely scratched the surface</strong>.</p><p name="e815" id="e815" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“<em class="markup--em markup--p-em">Old fool! Load balancers! Auto Scaling! Automation!</em>”, I hear you cry. Oh, you mean you’re in a hurry to manage infrastructure again? I thought you guys wanted to to Machine Learning ;)</p><p name="c500" id="c500" class="graf graf--p graf-after--p">On this bombshell, it’s time to call it a day. <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-2-80b0d1d7fc61" data-href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-2-80b0d1d7fc61" class="markup--anchor markup--p-anchor" target="_blank">In the next post</a>, we’ll start comparing and challenging options for larger-scale ML training: <strong class="markup--strong markup--p-strong">EC2</strong> vs. <strong class="markup--strong markup--p-strong">ECS/EKS</strong> vs <strong class="markup--strong markup--p-strong">SageMaker</strong>. An epic battle, no doubt.</p><p name="614c" id="614c" class="graf graf--p graf-after--p graf--trailing">Thanks for reading. Agree? Disagree? Great! Happy to discuss here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="915e" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="edb2" id="edb2" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">Best movie soundtrack ever. Yes, The Fellowship of the Ring only comes second :)</em></p><figure name="0a75" id="0a75" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/j7VKwS5K-Hc?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/a2d36a5e849"><time class="dt-published" datetime="2019-01-29T20:37:38.177Z">January 29, 2019</time></a>.</p><p><a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>