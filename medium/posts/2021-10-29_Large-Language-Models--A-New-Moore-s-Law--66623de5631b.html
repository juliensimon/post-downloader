<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Large Language Models: A New Moore’s Law?</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Large Language Models: A New Moore’s Law?</h1>
</header>
<section data-field="subtitle" class="p-summary">
A few days ago, Microsoft and NVIDIA introduced Megatron-Turing NLG 530B, a Transformer-based model hailed as “the world’s largest and most…
</section>
<section data-field="body" class="e-content">
<section name="d371" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f369" id="f369" class="graf graf--h3 graf--leading graf--title">Large Language Models: A New Moore’s Law?</h3><p name="3ab0" id="3ab0" class="graf graf--p graf-after--h3">A few days ago, Microsoft and NVIDIA <a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" data-href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">introduced</a> Megatron-Turing NLG 530B, a Transformer-based model hailed as “<em class="markup--em markup--p-em">the world’s largest and most powerful generative language model</em>.”</p><p name="f6ea" id="f6ea" class="graf graf--p graf-after--p">This is an impressive show of Machine Learning engineering, no doubt about it. Yet, should we be excited about this mega-model trend? I, for one, am not. Here’s why.</p><figure name="f46c" id="f46c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*dneNaoT4gPuO17_9.jpg" data-width="1956" data-height="1262" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*dneNaoT4gPuO17_9.jpg"></figure><h3 name="c718" id="c718" class="graf graf--h3 graf-after--figure">This is your Brain on Deep Learning</h3><p name="64a1" id="64a1" class="graf graf--p graf-after--h3">Researchers estimate that the human brain contains an average of <a href="https://pubmed.ncbi.nlm.nih.gov/19226510/" data-href="https://pubmed.ncbi.nlm.nih.gov/19226510/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">86 billion neurons</a> and 100 trillion synapses. It’s safe to assume that not all of them are dedicated to language either. Interestingly, GPT-4 is <a href="https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/" data-href="https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">expected</a> to have about 100 trillion parameters… As crude as this analogy is, shouldn’t we wonder whether building language models that are about the size of the human brain is the best long-term approach?</p><p name="0381" id="0381" class="graf graf--p graf-after--p">Of course, our brain is a marvelous device, produced by millions of years of evolution, while Deep Learning models are only a few decades old. Still, our intuition should tell us that something doesn’t compute (pun intended).</p><h3 name="6283" id="6283" class="graf graf--h3 graf-after--p">Deep Learning, Deep Pockets?</h3><p name="674c" id="674c" class="graf graf--p graf-after--h3">As you would expect, training a 530-billion parameter model on humongous text datasets requires a fair bit of infrastructure. In fact, Microsoft and NVIDIA used hundreds of DGX A100 multi-GPU servers. At $199,000 a piece, and factoring in networking equipment, hosting costs, etc., anyone looking to replicate this experiment would have to spend close to $100 million dollars. Want fries with that?</p><p name="068e" id="068e" class="graf graf--p graf-after--p">Seriously, which organizations have business use cases that would justify spending $100 million on Deep Learning infrastructure? Or even $10 million? Very few. So who are these models for, really?</p><h3 name="d5f8" id="d5f8" class="graf graf--h3 graf-after--p">That Warm Feeling is your GPU Cluster</h3><p name="40e5" id="40e5" class="graf graf--p graf-after--h3">For all its engineering brilliance, training Deep Learning models on GPUs is a brute force technique. According to the spec sheet, each DGX server can consume up to 6.5 kilowatts. Of course, you’ll need at least as much cooling power in your datacenter (or your server closet). Unless you’re the Starks and need to keep Winterfell warm in winter, that’s another problem you’ll have to deal with.</p><p name="f999" id="f999" class="graf graf--p graf-after--p">In addition, as public awareness grows on climate and social responsibility issues, organizations need to account for their carbon footprint. According to this 2019 <a href="https://arxiv.org/pdf/1906.02243.pdf" data-href="https://arxiv.org/pdf/1906.02243.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">study</a> from the University of Massachusetts, “<em class="markup--em markup--p-em">training BERT on GPU is roughly equivalent to a trans-American flight</em>”.</p><p name="27a7" id="27a7" class="graf graf--p graf-after--p">BERT-Large has 340 million parameters. One can only extrapolate what the footprint of Megatron-Turing could be… People who know me wouldn’t call me a bleeding-heart environmentalist. Still, some numbers are hard to ignore.</p><h3 name="198d" id="198d" class="graf graf--h3 graf-after--p">So?</h3><p name="6b1b" id="6b1b" class="graf graf--p graf-after--h3">Am I excited by Megatron-Turing NLG 530B and whatever beast is coming next? No. Do I think that the (relatively small) benchmark improvement is worth the added cost, complexity and carbon footprint? No. Do I think that building and promoting these huge models is helping organizations understand and adopt Machine Learning ? No.</p><p name="d5f5" id="d5f5" class="graf graf--p graf-after--p">I’m left wondering what’s the point of it all. Science for the sake of science? Good old marketing? Technological supremacy? Probably a bit of each. I’ll leave them to it, then.</p><p name="1a57" id="1a57" class="graf graf--p graf-after--p">Instead, let me focus on pragmatic and actionable techniques that you can all use to build high quality Machine Learning solutions.</p><h3 name="508c" id="508c" class="graf graf--h3 graf-after--p">Use Pretrained Models</h3><p name="4ea9" id="4ea9" class="graf graf--p graf-after--h3">In the vast majority of cases, you won’t need a custom model architecture. Maybe you’ll <em class="markup--em markup--p-em">want</em> a custom one (which is a different thing), but there be dragons. Experts only!</p><p name="3b2f" id="3b2f" class="graf graf--p graf-after--p">A good starting point is to look for <a href="https://huggingface.co/models" data-href="https://huggingface.co/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">models</a> that have been pretrained for the task you’re trying to solve (say, <a href="https://huggingface.co/models?language=en&amp;pipeline_tag=summarization&amp;sort=downloads" data-href="https://huggingface.co/models?language=en&amp;pipeline_tag=summarization&amp;sort=downloads" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">summarizing English text</a>).</p><p name="79b7" id="79b7" class="graf graf--p graf-after--p">Then, you should quickly try out a few models to predict your own data. If metrics tell you that one works well enough, you’re done! If you need a little more accuracy, you should consider fine-tuning the model (more on this in a minute).</p><h3 name="9d3c" id="9d3c" class="graf graf--h3 graf-after--p">Use Smaller Models</h3><p name="5cce" id="5cce" class="graf graf--p graf-after--h3">When evaluating models, you should pick the smallest one that can deliver the accuracy you need. It will predict faster and require fewer hardware resources for training and inference. Frugality goes a long way.</p><p name="d6f7" id="d6f7" class="graf graf--p graf-after--p">It’s nothing new either. Computer Vision practitioners will remember when <a href="https://arxiv.org/abs/1602.07360" data-href="https://arxiv.org/abs/1602.07360" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SqueezeNet</a> came out in 2017, achieving a 50x reduction in model size compared to <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" data-href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AlexNet</a>, while meeting or exceeding its accuracy. How clever that was!</p><p name="52c5" id="52c5" class="graf graf--p graf-after--p">Downsizing efforts are also under way in the Natural Language Processing community, using transfer learning techniques such as <a href="https://en.wikipedia.org/wiki/Knowledge_distillation" data-href="https://en.wikipedia.org/wiki/Knowledge_distillation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">knowledge distillation</a>. <a href="https://arxiv.org/abs/1910.01108" data-href="https://arxiv.org/abs/1910.01108" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">DistilBERT</a> is perhaps its most widely known achievement. Compared to the original BERT model, it retains 97% of language understanding while being 40% smaller and 60% faster. You can try it <a href="https://huggingface.co/distilbert-base-uncased" data-href="https://huggingface.co/distilbert-base-uncased" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>. The same approach has been applied to other models, such as Facebook’s <a href="https://arxiv.org/abs/1910.13461" data-href="https://arxiv.org/abs/1910.13461" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">BART</a>, and you can try DistilBART <a href="https://huggingface.co/models?search=distilbart" data-href="https://huggingface.co/models?search=distilbart" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="bd94" id="bd94" class="graf graf--p graf-after--p">Recent models from the <a href="https://bigscience.huggingface.co/" data-href="https://bigscience.huggingface.co/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Big Science</a> project are also very impressive. As visible in this graph included in the <a href="https://arxiv.org/abs/2110.08207" data-href="https://arxiv.org/abs/2110.08207" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research paper</a>, their T0 model outperforms GPT-3 on many tasks while being 16x smaller.</p><figure name="8540" id="8540" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Knb1TZrdMII5XixQ.png" data-width="785" data-height="409" src="https://cdn-images-1.medium.com/max/800/0*Knb1TZrdMII5XixQ.png"></figure><p name="f469" id="f469" class="graf graf--p graf-after--figure">You can try T0 <a href="https://huggingface.co/bigscience/T0pp" data-href="https://huggingface.co/bigscience/T0pp" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>. This is the kind of research we need more of!</p><h3 name="768a" id="768a" class="graf graf--h3 graf-after--p">Fine-Tune Models</h3><p name="aeda" id="aeda" class="graf graf--p graf-after--h3">If you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should fine-tune it, that is to say train it only for a few epochs on your own data. If you’re short on data, maybe of one these <a href="https://huggingface.co/datasets" data-href="https://huggingface.co/datasets" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">datasets</a> can get you started.</p><p name="b210" id="b210" class="graf graf--p graf-after--p">You guessed it, that’s another way to do transfer learning, and it’ll help you save on everything!</p><ul class="postList"><li name="a630" id="a630" class="graf graf--li graf-after--p">Less data to collect, store, clean and annotate,</li><li name="6847" id="6847" class="graf graf--li graf-after--li">Faster experiments and iterations,</li><li name="5add" id="5add" class="graf graf--li graf-after--li">Fewer resources required in production.</li></ul><p name="13ba" id="13ba" class="graf graf--p graf-after--li">In other words: save time, save money, save hardware resources, save the world!</p><p name="f1ae" id="f1ae" class="graf graf--p graf-after--p">If you need a tutorial, the Hugging Face <a href="https://huggingface.co/course" data-href="https://huggingface.co/course" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">course</a> will get you started in no time.</p><h3 name="7b56" id="7b56" class="graf graf--h3 graf-after--p">Use Cloud-Based Infrastructure</h3><p name="288b" id="288b" class="graf graf--p graf-after--h3">Like them or not, cloud companies know how to build efficient infrastructure. Sustainability studies show that cloud-based infrastructure is more energy and carbon efficient than the alternative: see <a href="https://sustainability.aboutamazon.com/environment/the-cloud" data-href="https://sustainability.aboutamazon.com/environment/the-cloud" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS</a>, <a href="https://azure.microsoft.com/en-us/global-infrastructure/sustainability" data-href="https://azure.microsoft.com/en-us/global-infrastructure/sustainability" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Azure</a>, and <a href="https://cloud.google.com/sustainability" data-href="https://cloud.google.com/sustainability" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google</a>. Earth.org <a href="https://earth.org/environmental-impact-of-cloud-computing/" data-href="https://earth.org/environmental-impact-of-cloud-computing/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">says</a> that while cloud infrastructure is not perfect, “[<em class="markup--em markup--p-em">it’s] more energy efficient than the alternative and facilitates environmentally beneficial services and economic growth.</em>”</p><p name="fc33" id="fc33" class="graf graf--p graf-after--p">Cloud certainly has a lot going for it when it comes to ease of use, flexibility and pay as you go. It’s also a little greener than you probably thought. If you’re short on GPUs, why not try fine-tune your Hugging Face models on <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a>, AWS’ managed service for Machine Learning? We’ve got <a href="https://huggingface.co/docs/sagemaker/train" data-href="https://huggingface.co/docs/sagemaker/train" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">plenty of examples</a> for you.</p><h3 name="9cd8" id="9cd8" class="graf graf--h3 graf-after--p">Optimize Your Models</h3><p name="4197" id="4197" class="graf graf--p graf-after--h3">From compilers to virtual machines, software engineers have long used tools that automatically optimize their code for whatever hardware they’re running on.</p><p name="8712" id="8712" class="graf graf--p graf-after--p">However, the Machine Learning community is still struggling with this topic, and for good reason. Optimizing models for size and speed is a devilishly complex task, which involves techniques such as:</p><ul class="postList"><li name="be8e" id="be8e" class="graf graf--li graf-after--p">Specialized hardware that speeds up training (<a href="https://www.graphcore.ai/" data-href="https://www.graphcore.ai/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Graphcore</a>, <a href="https://habana.ai/" data-href="https://habana.ai/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Habana</a>) and inference (<a href="https://cloud.google.com/tpu" data-href="https://cloud.google.com/tpu" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Google TPU</a>, <a href="https://aws.amazon.com/machine-learning/inferentia/" data-href="https://aws.amazon.com/machine-learning/inferentia/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">AWS Inferentia</a>).</li><li name="c3aa" id="c3aa" class="graf graf--li graf-after--li">Pruning: remove model parameters that have little or no impact on the predicted outcome.</li><li name="088f" id="088f" class="graf graf--li graf-after--li">Fusion: merge model layers (say, convolution and activation).</li><li name="0cfe" id="0cfe" class="graf graf--li graf-after--li">Quantization: storing model parameters in smaller values (say, 8 bits instead of 32 bits)</li></ul><p name="bce8" id="bce8" class="graf graf--p graf-after--li">Fortunately, automated tools are starting to appear, such as the <a href="https://huggingface.co/hardware" data-href="https://huggingface.co/hardware" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Optimum</a> open source library, and <a href="https://huggingface.co/infinity" data-href="https://huggingface.co/infinity" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Infinity</a>, a containerized solution that delivers Transformers accuracy at 1-millisecond latency.</p><h3 name="c228" id="c228" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="9590" id="9590" class="graf graf--p graf-after--h3">Large language model size has been increasing 10x every year for the last few years. This is starting to look like another <a href="https://en.wikipedia.org/wiki/Moore%27s_law" data-href="https://en.wikipedia.org/wiki/Moore%27s_law" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Moore’s Law</a>.</p><p name="c35e" id="c35e" class="graf graf--p graf-after--p">We’ve been there before, and we should know that this road leads to diminishing returns, higher cost, more complexity, and new risks. Exponentials tend not to end well. Remember <a href="https://meltdownattack.com/" data-href="https://meltdownattack.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Meltdown and Spectre</a>? Do we want to find out what that looks like for AI?</p><p name="9026" id="9026" class="graf graf--p graf-after--p graf--trailing">Instead of chasing trillion-parameter models (place your bets), wouldn’t all be better off if we built practical and efficient solutions that all developers can use to solve real-world problems?</p></div></div></section><section name="1f3e" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="ff2b" id="ff2b" class="graf graf--p graf--leading graf--trailing"><em class="markup--em markup--p-em">Interested in how Hugging Face can help your organization build and deploy production-grade Machine Learning solutions? Get in touch at </em><a href="mailto:julsimon@huggingface.co" data-href="mailto:julsimon@huggingface.co" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">julsimon@huggingface.co</em></a><em class="markup--em markup--p-em"> (no recruiters, no sales pitches, please).</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/66623de5631b"><time class="dt-published" datetime="2021-10-29T08:24:13.663Z">October 29, 2021</time></a>.</p><p><a href="https://medium.com/@julsimon/large-language-models-a-new-moores-law-66623de5631b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
