<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Building a movie recommender with Factorization Machines on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Building a movie recommender with Factorization Machines on Amazon SageMaker</h1>
</header>
<section data-field="subtitle" class="p-summary">
Recommendation is one of the most popular applications in Machine Learning. In this post, you will learn how to build a movie…
</section>
<section data-field="body" class="e-content">
<section name="0ed1" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="dc7b" id="dc7b" class="graf graf--h3 graf--leading graf--title">Building a movie recommender with Factorization Machines on Amazon SageMaker</h3><p name="cb2e" id="cb2e" class="graf graf--p graf-after--h3"><a href="https://en.wikipedia.org/wiki/Recommender_system" data-href="https://en.wikipedia.org/wiki/Recommender_system" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Recommendation</a> is one of the most popular applications in Machine Learning. In this post, you will learn how to build a movie recommendation model based on Factorization Machines — one of the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">built-in algorithms</a> of <a href="https://aws.amazon.com/sagemaker" data-href="https://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a> — and the popular <a href="https://grouplens.org/datasets/movielens/" data-href="https://grouplens.org/datasets/movielens/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MovieLens</a> dataset.</p><figure name="965c" id="965c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pK2j_V3gvTH_2QhXvDyUcg.png" data-width="649" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*pK2j_V3gvTH_2QhXvDyUcg.png"><figcaption class="imageCaption">Preparing a data set for recommendation, no doubt.</figcaption></figure><p name="52eb" id="52eb" class="graf graf--p graf-after--figure">At the time of writing, this use case is not covered by the very nice collection of <a href="https://github.com/awslabs/amazon-sagemaker-examples" data-href="https://github.com/awslabs/amazon-sagemaker-examples" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker sample notebooks</a>, so I figured it could help many of you out there if I put one together. Get ready to learn about one-hot encoding, sparse matrices, protobuf files and more :)</p><blockquote name="3990" id="3990" class="graf graf--blockquote graf-after--p">As usual, a <a href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/03-Factorization-Machines-Movielens.ipynb" data-href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/03-Factorization-Machines-Movielens.ipynb" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Jupyter notebook</a> is available on Github.</blockquote><p name="94fb" id="94fb" class="graf graf--p graf-after--blockquote">Several of my AWS colleagues provided excellent advice as well as debugging tips, so please let me thank <strong class="markup--strong markup--p-strong">Sireesha Muppala</strong>, <strong class="markup--strong markup--p-strong">Yuri Astashanok</strong>, <strong class="markup--strong markup--p-strong">David Arpin</strong> and <strong class="markup--strong markup--p-strong">Guy Ernest</strong>.</p><h4 name="f1ce" id="f1ce" class="graf graf--h4 graf-after--p">A word about Factorization Machines</h4><p name="13fa" id="13fa" class="graf graf--p graf-after--h4">Factorization Machines (FM) are a <strong class="markup--strong markup--p-strong">supervised Machine Learning technique</strong> introduced in 2010 (<a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" data-href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research paper</a>, PDF). FM get their name from their ability to <strong class="markup--strong markup--p-strong">reduce problem dimensionality</strong> thanks to <strong class="markup--strong markup--p-strong">matrix factorization</strong>.</p><p name="41eb" id="41eb" class="graf graf--p graf-after--p">They can be used for <strong class="markup--strong markup--p-strong">classification</strong> or <strong class="markup--strong markup--p-strong">regression</strong> and are much more <strong class="markup--strong markup--p-strong">computationally efficient </strong>on <strong class="markup--strong markup--p-strong">large sparse data sets</strong> than traditional algorithms like linear regression.This property is why FM are widely used for <strong class="markup--strong markup--p-strong">recommendation</strong>: user count and item count are typically very large although the actual number of recommendations is very small (users don’t rate all available items!).</p><p name="0558" id="0558" class="graf graf--p graf-after--p">Here’s a toy example, where a <strong class="markup--strong markup--p-strong">sparse rating matrix</strong> (dimension 4x4) is factored into a <strong class="markup--strong markup--p-strong">dense user matrix</strong> (dimension 4x2) and a <strong class="markup--strong markup--p-strong">dense item matrix</strong> (2x4). As you can see, the <strong class="markup--strong markup--p-strong">number of factors</strong> (2) is smaller than the number of columns of the rating matrix (4). In addition, this multiplication also lets us <strong class="markup--strong markup--p-strong">fill all blank values</strong> in the rating matrix, which we can then use to <strong class="markup--strong markup--p-strong">recommend new items</strong> to any user.</p><figure name="7240" id="7240" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PefuBiYr9Bp7lo_zotGj0Q.png" data-width="1682" data-height="690" src="https://cdn-images-1.medium.com/max/800/1*PefuBiYr9Bp7lo_zotGj0Q.png"><figcaption class="imageCaption">Source: data-artisans.com</figcaption></figure><h4 name="b588" id="b588" class="graf graf--h4 graf-after--figure">The MovieLens dataset</h4><p name="f4af" id="f4af" class="graf graf--p graf-after--h4">This <a href="https://grouplens.org/datasets/movielens/" data-href="https://grouplens.org/datasets/movielens/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dataset</a> is a great starting point for recommendation. It comes in multiples sizes and in this post, we’ll use <strong class="markup--strong markup--p-strong">ml100k</strong>:<strong class="markup--strong markup--p-strong"> 100,000 ratings</strong> from <strong class="markup--strong markup--p-strong">943 users</strong> on <strong class="markup--strong markup--p-strong">1682 movies</strong>. As you can see, the ml100k rating matrix is quite sparse (<strong class="markup--strong markup--p-strong">93.6%</strong> to be precise) as it only holds <strong class="markup--strong markup--p-strong">100,000</strong> ratings out of a possible <strong class="markup--strong markup--p-strong">1,586,126</strong> (943*1682).</p><p name="3250" id="3250" class="graf graf--p graf-after--p">Here are the first 10 lines in the data set: user #754 gave movie #595 a 2-star rating and so on.</p><pre name="cb58" id="cb58" class="graf graf--pre graf-after--p"># user id, movie id, rating, timestamp<br>754	595	2	879452073<br>932	157	4	891250667<br>751	100	4	889132252<br>101	820	3	877136954<br>606	1277	3	878148493<br>581	475	4	879641850<br>13	50	5	882140001<br>457	59	5	882397575<br>111	321	3	891680076<br>123	657	4	879872066</pre><h4 name="efd5" id="efd5" class="graf graf--h4 graf-after--pre">Data set preparation</h4><p name="5a56" id="5a56" class="graf graf--p graf-after--h4">As explained earlier, FM work best on <strong class="markup--strong markup--p-strong">high-dimension</strong> datasets. As a consequence, we’re going to <a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science" data-href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">one-hot encode</strong></a> user ids and movie ids (we’ll ignore timestamps). Thus, each sample in our data set will be a <strong class="markup--strong markup--p-strong">2,625 boolean vector</strong> (943+1682) with only two values set to 1 with respect to the user id and movie id.</p><p name="bb70" id="bb70" class="graf graf--p graf-after--p">We’re going to build a <strong class="markup--strong markup--p-strong">binary recommender</strong> (i.e. like/don’t like). 3-star, 4-star and 5-star ratings are set to 1. Lower ratings are set to 0.</p><p name="6b01" id="6b01" class="graf graf--p graf-after--p">One last thing: the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">FM implementation</a> in SageMaker requires training and test data to be stored in <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">float32</em> tensors in </strong><a href="https://developers.google.com/protocol-buffers/" data-href="https://developers.google.com/protocol-buffers/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">protobuf</strong></a><strong class="markup--strong markup--p-strong"> format</strong>. Yes, that sounds complicated :) However, the <a href="https://github.com/aws/sagemaker-python-sdk" data-href="https://github.com/aws/sagemaker-python-sdk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker SDK</a> provides a convenient utility function that takes cares of this, so don’t worry too much about it.</p><h4 name="bef8" id="bef8" class="graf graf--h4 graf-after--p">The high-level view</h4><p name="5068" id="5068" class="graf graf--p graf-after--h4">OK, then: here are the steps we need to implement:</p><ul class="postList"><li name="a406" id="a406" class="graf graf--li graf-after--p">load the MovieLens training set and test set from disk,</li><li name="02fc" id="02fc" class="graf graf--li graf-after--li">for each set, build a sparse matrix holding one-hot encoded data samples,</li><li name="fbb1" id="fbb1" class="graf graf--li graf-after--li">for each set, build a label vector holding ratings,</li><li name="3457" id="3457" class="graf graf--li graf-after--li">write both sets to protobuf-encoded files,</li><li name="6306" id="6306" class="graf graf--li graf-after--li">copy these files to an S3 bucket,</li><li name="576c" id="576c" class="graf graf--li graf-after--li">configure and run a Factorization Machines training job on SageMaker,</li><li name="4a51" id="4a51" class="graf graf--li graf-after--li">deploy the corresponding model to an endpoint,</li><li name="2a2f" id="2a2f" class="graf graf--li graf-after--li">run some predictions.</li></ul><p name="0328" id="0328" class="graf graf--p graf-after--li">Let’s get going!</p><h4 name="e4d2" id="e4d2" class="graf graf--h4 graf-after--p">Loading the MovieLens dataset</h4><p name="9876" id="9876" class="graf graf--p graf-after--h4">ml-100k contains multiple text files, but we’re only going to use two of them to build our model:</p><ul class="postList"><li name="7216" id="7216" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">ua.base</em></strong> (90,570 samples) will be our training set.</li><li name="4640" id="4640" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">ua.test</em></strong> (9,430 samples) will be our test set.</li></ul><p name="56a4" id="56a4" class="graf graf--p graf-after--li">Both files have the same <strong class="markup--strong markup--p-strong">tab-separated format</strong>:</p><ul class="postList"><li name="c3f3" id="c3f3" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">user id</strong> (integer between 1 and 943),</li><li name="be58" id="be58" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">movie id</strong> (integer between 1 and 1682),</li><li name="712a" id="712a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">rating</strong> (integer between 1 and 5),</li><li name="dec5" id="dec5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">timestamp</strong> (epoch-based integer).</li></ul><p name="3c29" id="3c29" class="graf graf--p graf-after--li">As a consequence, we’re going to build the following data structures:</p><ul class="postList"><li name="095e" id="095e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">a training sparse matrix</strong>: 90,570 lines and 2,625 columns (943 one-hot encoded features for the user id, plus 1682 one-hot encoded features for the movie id),</li><li name="18a9" id="18a9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">a training label array</strong>: 90,570 ratings,</li><li name="5225" id="5225" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">a test sparse matrix</strong>: 9,430 lines and 2,625 columns,</li><li name="a4e5" id="a4e5" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">a test label array</strong>: 9,430 ratings.</li></ul><blockquote name="7a0b" id="7a0b" class="graf graf--blockquote graf-after--li">Reminder: each sample must be a single one-hot encoded feature vector. Yes, you do need to <strong class="markup--strong markup--blockquote-strong">concatenate the one-hot encoded values for user id, movie id and any additional feature you may add</strong>: building a list of distinct vectors (one for the user id, one for the movie id, etc.) isn’t the right way.</blockquote><p name="15e3" id="15e3" class="graf graf--p graf-after--blockquote">Our training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is <strong class="markup--strong markup--p-strong">99.92% sparse</strong>. Storing this as a dense matrix would be a massive waste of both <strong class="markup--strong markup--p-strong">storage</strong> and <strong class="markup--strong markup--p-strong">computing power</strong>!</p><p name="1fe5" id="1fe5" class="graf graf--p graf-after--p">To avoid this, let’s use a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html" data-href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">scipy.lil_matrix</em></a><em class="markup--em markup--p-em"> </em><strong class="markup--strong markup--p-strong">sparse matrix for samples</strong> and a <a href="http://www.numpy.org" data-href="http://www.numpy.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">numpy</em></a> <strong class="markup--strong markup--p-strong">array for labels</strong>.</p><figure name="b378" id="b378" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/e57c320f630b4ee804cdf33076955704.js"></script></figure><p name="29ef" id="29ef" class="graf graf--p graf-after--figure">We should check that we have approximately the same number of samples per class. An unbalanced data set is a serious problem for classifiers.</p><pre name="85ab" id="85ab" class="graf graf--pre graf-after--p">print(np.count_nonzero(Y_train)/nbRatingsTrain)<br>0.55<br>print(np.count_nonzero(Y_test)/nbRatingsTest)<br>0.58</pre><p name="6e64" id="6e64" class="graf graf--p graf-after--pre">Slightly unbalanced, but nothing bad. Let’s move on!</p><h4 name="8630" id="8630" class="graf graf--h4 graf-after--p">Writing to protobuf files</h4><p name="f04e" id="f04e" class="graf graf--p graf-after--h4">Next, we’re going to write the training set and the test set to two <strong class="markup--strong markup--p-strong">protobuf</strong> files stored in <strong class="markup--strong markup--p-strong">S3</strong>. Fortunately, we can rely on the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">write_spmatrix_to_sparse_tensor()</em></strong> utility function: it writes our samples and labels into an in-memory protobuf-encoded sparse multi-dimensional array (aka tensor).</p><p name="83be" id="83be" class="graf graf--p graf-after--p">Then, we commit the buffer to <strong class="markup--strong markup--p-strong">S3</strong>. Once this step is complete, we’re done with data preparation and can now focus on our training job.</p><figure name="f334" id="f334" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/d2c5b1c0d996cb35a961891cc6a106d6.js"></script></figure><blockquote name="dd2c" id="dd2c" class="graf graf--blockquote graf-after--figure">There (could) be dragons. Here are some troubleshooting tips:<br>- are both samples and labels float32 values?<br>- are samples stored in a sparse matrix (not a numpy array or anything else)?<br>- are labels stored in a vector (not any kind of matrix)?<br>- write_spmatrix_to_sparse_tensor() undefined? It was added in <a href="https://github.com/aws/sagemaker-python-sdk/blob/master/CHANGELOG.rst" data-href="https://github.com/aws/sagemaker-python-sdk/blob/master/CHANGELOG.rst" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">SDK 1.0.2</a> and you probably need to upgrade the SageMaker SDK: see the appendix at the end of the post.</blockquote><p name="d0ef" id="d0ef" class="graf graf--p graf-after--blockquote">Here’s our training set in S3: only <strong class="markup--strong markup--p-strong">5.5MB.</strong> Sparse matrices FTW!</p><pre name="f485" id="f485" class="graf graf--pre graf-after--p">$ aws s3 ls s3://jsimon-sagemaker-us/sagemaker/fm-movielens/train/train.protobuf</pre><pre name="a229" id="a229" class="graf graf--pre graf-after--pre">2018-01-28 16:50:29    5796480 train.protobuf</pre><h4 name="f09c" id="f09c" class="graf graf--h4 graf-after--pre">Running the training job</h4><p name="f07d" id="f07d" class="graf graf--p graf-after--h4">Let’s start by creating an <strong class="markup--strong markup--p-strong">Estimator</strong> based on the FM container available in our region. Then, we have to set some FM-specific <strong class="markup--strong markup--p-strong">hyper-parameters</strong> (full list in the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentation</a>):</p><ul class="postList"><li name="e0fd" id="e0fd" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">feature_dim</em></strong>: the number of features in each sample (2,625 in our case).</li><li name="1304" id="1304" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">predictor_type</em></strong>: ‘<em class="markup--em markup--li-em">binary_classifier</em>’ is what we’re going to use.</li><li name="8fe0" id="8fe0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">num_factors</em></strong>: the common dimension for the user and item matrices (as explained in the example at the start of the post).</li></ul><p name="fa86" id="fa86" class="graf graf--p graf-after--li">The other ones used here are optional (and quite self-explanatory).</p><p name="7335" id="7335" class="graf graf--p graf-after--p">Finally, let’s run the training job: calling the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">fit()</em></strong> API is all it takes, passing both the training and test sets hosted in S3. Simple and elegant.</p><figure name="bba1" id="bba1" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/fea671b5f6e91f9bdab2126e2d8b20e5.js"></script></figure><p name="241d" id="241d" class="graf graf--p graf-after--figure">A few minutes later, training is complete and we can check out the <strong class="markup--strong markup--p-strong">training log</strong> either in the notebook or in CloudWatch Logs (in the <em class="markup--em markup--p-em">/aws/sagemaker/trainingjobs </em>log group).</p><p name="7f55" id="7f55" class="graf graf--p graf-after--p">After 50 epochs, <strong class="markup--strong markup--p-strong">test accuracy is 71.5%</strong> and the <a href="https://en.wikipedia.org/wiki/F1_score" data-href="https://en.wikipedia.org/wiki/F1_score" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">F1 score</strong></a><strong class="markup--strong markup--p-strong"> </strong>(a typical metric for binary classifier) is <strong class="markup--strong markup--p-strong">0.75 </strong>(1 indicates a perfect classifier). Not great but with all that sparse matrix and protobuf excitement, I didn’t spend much time tuning hyper-parameters. Surely you can do better :)</p><pre name="be34" id="be34" class="graf graf--pre graf-after--p">[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--strong markup--pre-strong">binary_classification_accuracy</strong><br>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--strong markup--pre-strong">0.7159</strong><br>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : binary_classification_cross_entropy<br>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : 0.581087609863<br>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--strong markup--pre-strong">binary_f_1</strong>.000<br>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--strong markup--pre-strong">0.74558968389</strong></pre><p name="ad2b" id="ad2b" class="graf graf--p graf-after--pre">We have one last step to cover: <strong class="markup--strong markup--p-strong">model deployment</strong>.</p><h4 name="cb90" id="cb90" class="graf graf--h4 graf-after--p">Deploying the model</h4><p name="1963" id="1963" class="graf graf--p graf-after--h4">All it takes to deploy the model is a <strong class="markup--strong markup--p-strong">simple API call</strong>. In the old days (2 months or so ago), this would have required quite a bit of work, even on AWS. Here, just call <strong class="markup--strong markup--p-strong">deploy()</strong> and voila!</p><figure name="420a" id="420a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/96a1a63468a4fb52e838e09378d44726.js"></script></figure><p name="c980" id="c980" class="graf graf--p graf-after--figure">We’re now ready to invoke the model’s HTTP endpoint thanks to the <em class="markup--em markup--p-em">predict() </em>API. The format for both <strong class="markup--strong markup--p-strong">request and response data</strong> is <strong class="markup--strong markup--p-strong">JSON</strong>, which requires us to provide a simple <strong class="markup--strong markup--p-strong">serializer</strong> to convert our sparse matrix samples to JSON.</p><figure name="f7a5" id="f7a5" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/e8ca5a3cf2d768ac8962d1902c4d39fa.js"></script></figure><p name="b45a" id="b45a" class="graf graf--p graf-after--figure">We’re now able to classify any movie for any user: just build a new data set, process it the same way as the training and test set and use <em class="markup--em markup--p-em">predict()</em> to get results. You should also experiment with different <strong class="markup--strong markup--p-strong">prediction thresholds (</strong>set prediction to 1 above a given score and to 0 under it) and see what value gives you the most efficient recommendations. The MovieLens data set also includes movie titles, so there’s plenty more to explore :)</p><h4 name="67a9" id="67a9" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="54fb" id="54fb" class="graf graf--p graf-after--h4">As you can see, built-in algorithms are a great way to get the job done quickly, without having to write any training code. There’s quite a bit of data preparation involved, but as we saw, it’s key to make very large training jobs <strong class="markup--strong markup--p-strong">fast</strong> and <strong class="markup--strong markup--p-strong">scalable</strong>.</p><p name="9e53" id="9e53" class="graf graf--p graf-after--p">If you’re curious about other<strong class="markup--strong markup--p-strong"> SageMaker built-in algorithms</strong>, here are a couple of previous posts on:</p><ul class="postList"><li name="9d00" id="9d00" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" data-href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" class="markup--anchor markup--li-anchor" target="_blank">spam classification with XGBoost</a>,</li><li name="69fa" id="69fa" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" data-href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" class="markup--anchor markup--li-anchor" target="_blank">image classification with Deep Learning</a>.</li></ul><p name="3ec9" id="3ec9" class="graf graf--p graf-after--li">In addition, if you’d like to know more about <strong class="markup--strong markup--p-strong">recommendation systems</strong>, here are a few resources you may find interesting.</p><ul class="postList"><li name="9279" id="9279" class="graf graf--li graf--startsWithDoubleQuote graf-after--p">“<a href="https://www.computer.org/csdl/mags/ic/2017/03/mic2017030012.html" data-href="https://www.computer.org/csdl/mags/ic/2017/03/mic2017030012.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">Two Decades of Recommender Systems at Amazon.com</em></a>” — Research paper.</li><li name="07fa" id="07fa" class="graf graf--li graf-after--li"><a href="https://github.com/amzn/amazon-dsstne" data-href="https://github.com/amzn/amazon-dsstne" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine</a> — Github.</li><li name="758c" id="758c" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“<a href="https://aws.amazon.com/blogs/big-data/generating-recommendations-at-amazon-scale-with-apache-spark-and-amazon-dsstne/" data-href="https://aws.amazon.com/blogs/big-data/generating-recommendations-at-amazon-scale-with-apache-spark-and-amazon-dsstne/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE</em></a>” — AWS blog.</li><li name="7863" id="7863" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“<a href="https://www.youtube.com/watch?v=TjaIKijl-IY" data-href="https://www.youtube.com/watch?v=TjaIKijl-IY" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">A quick demo of Amazon DSSTNE</em></a>” — YouTube video.</li><li name="9600" id="9600" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“<a href="https://www.youtube.com/watch?v=cftJAuwKWkA" data-href="https://www.youtube.com/watch?v=cftJAuwKWkA" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">Using MXNet for Recommendation Modeling at Scale (MAC306)</em></a>” — AWS re:Invent 2016 video.</li><li name="11af" id="11af" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“<a href="https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017" data-href="https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">Building Content Recommendation Systems Using Apache MXNet and Gluon (MCL402)</em></a>” — AWS re:Invent 2017 presentation.</li></ul><p name="0e22" id="0e22" class="graf graf--p graf-after--li graf--trailing">As always, thank you for reading. Happy to answer questions on <a href="https://twitter.com/julsimon/" data-href="https://twitter.com/julsimon/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="f9b8" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="e4bc" id="e4bc" class="graf graf--h4 graf--leading">Appendix: upgrading to the latest SageMaker SDK</h4><ul class="postList"><li name="03a9" id="03a9" class="graf graf--li graf-after--h4">Open your notebook instance.</li><li name="f03c" id="f03c" class="graf graf--li graf-after--li">On your instance, open a Jupyter terminal.</li><li name="0989" id="0989" class="graf graf--li graf-after--li">Activate the Conda environment where you’d like to upgrade the SDK, e.g:</li></ul><pre name="e965" id="e965" class="graf graf--pre graf-after--li">source activate mxnet_p27</pre><ul class="postList"><li name="7523" id="7523" class="graf graf--li graf-after--pre">Install the latest <em class="markup--em markup--li-em">pip</em> package for SageMaker.</li></ul><pre name="6d4f" id="6d4f" class="graf graf--pre graf-after--li">pip install sagemaker --upgrade</pre><ul class="postList"><li name="7671" id="7671" class="graf graf--li graf-after--pre">Alternatively, you could also clone the SageMaker SDK and install it. This is the best way to grab the very latest version.</li></ul><pre name="1e1e" id="1e1e" class="graf graf--pre graf-after--li graf--trailing">git clone <a href="https://github.com/aws/sagemaker-python-sdk.git" data-href="https://github.com/aws/sagemaker-python-sdk.git" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">https://github.com/aws/sagemaker-python-sdk.git</a><br>cd <a href="https://github.com/aws/sagemaker-python-sdk.git" data-href="https://github.com/aws/sagemaker-python-sdk.git" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">sagemaker-python-sdk</a><br>python setup.py sdist<br>pip install dist/sagemaker-1.x.x.tar.gz --upgrade</pre></div></div></section><section name="d80d" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="bc3d" id="bc3d" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">This post was completed during a flight from Paris to Las Vegas, so obviously…</em></p><figure name="0c06" id="0c06" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/gikKRv1hUjk?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/cedbfc8c93d8"><time class="dt-published" datetime="2018-01-29T14:02:32.374Z">January 29, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
