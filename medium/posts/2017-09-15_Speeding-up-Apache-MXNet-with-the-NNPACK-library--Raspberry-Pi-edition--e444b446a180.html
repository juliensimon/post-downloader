<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Speeding up Apache MXNet with the NNPACK library (Raspberry Pi edition)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Speeding up Apache MXNet with the NNPACK library (Raspberry Pi edition)</h1>
</header>
<section data-field="subtitle" class="p-summary">
In a previous post, I showed you how to add the NNPACK library to Apache MXNet and how this did speed up CPU-based inference for networks…
</section>
<section data-field="body" class="e-content">
<section name="9931" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="08e7" id="08e7" class="graf graf--h3 graf--leading graf--title">Speeding up Apache MXNet with the NNPACK library (Raspberry Pi edition)</h3><p name="4f07" id="4f07" class="graf graf--p graf-after--h3">In a <a href="https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-7427f367490f" data-href="https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-7427f367490f" class="markup--anchor markup--p-anchor" target="_blank">previous post</a>, I showed you how to add the <a href="https://github.com/Maratyszcza/NNPACK" data-href="https://github.com/Maratyszcza/NNPACK" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NNPACK</a> library to <a href="http://mxnet.io" data-href="http://mxnet.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> and how this did speed up CPU-based inference for networks such as Alexnet or VGG by a factor of 2 to 3. Definitely worth the effort.</p><p name="dede" id="dede" class="graf graf--p graf-after--p">I also mentioned that NNPACK supports ARM 7 processors with the <a href="https://en.wikipedia.org/wiki/ARM_architecture#Advanced_SIMD_.28NEON.29" data-href="https://en.wikipedia.org/wiki/ARM_architecture#Advanced_SIMD_.28NEON.29" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NEON</a> instruction set (similar to <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions" data-href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AVX</a> for Intel chips), as well as ARM v8 processors (which include NEON by default).</p><p name="fb67" id="fb67" class="graf graf--p graf-after--p">As the Raspberry Pi 3 is built on the <a href="https://en.wikipedia.org/wiki/ARM_Cortex-A53" data-href="https://en.wikipedia.org/wiki/ARM_Cortex-A53" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Cortex-A53</a> CPU (ARM v8-based), I figured I’d give it a go and see how NNPACK could help us for embedded applications.</p><figure name="5f89" id="5f89" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*n4jsuYqylu2dYittuoyfTg.png" data-width="2240" data-height="924" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*n4jsuYqylu2dYittuoyfTg.png"><figcaption class="imageCaption">Yeah, it’s on fire but it’s FASTER!</figcaption></figure><h4 name="764e" id="764e" class="graf graf--h4 graf-after--figure">Setup</h4><p name="48d2" id="48d2" class="graf graf--p graf-after--h4">The steps required to build NNPACK and MXNet are strictly identical to the ones we used on our AWS instance, so please refer to the <a href="https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-7427f367490f" data-href="https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-7427f367490f" class="markup--anchor markup--p-anchor" target="_blank">previous article</a>.</p><h4 name="3cc6" id="3cc6" class="graf graf--h4 graf-after--p">Benchmarks</h4><p name="7db6" id="7db6" class="graf graf--p graf-after--h4">Given the limited RAM available on the Raspberry Pi 3, we won’t be able to load larger models like VGG16 (as I found out when I <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--p-anchor" target="_blank">first tried</a> loading models on the Pi). Thus, I had to stick to the multi-layer perceptron and the Alexnet convolutional network, with the largest batch sizes possible.</p><p name="f33d" id="f33d" class="graf graf--p graf-after--p">Let’s start with the MLP.</p><figure name="30ad" id="30ad" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SARcDjWT412yFoJjVIP0Cw.png" data-width="993" data-height="808" src="https://cdn-images-1.medium.com/max/800/1*SARcDjWT412yFoJjVIP0Cw.png"><figcaption class="imageCaption">batch size vs image per seconds for Multi-Layer Perceptron</figcaption></figure><p name="468a" id="468a" class="graf graf--p graf-after--figure">Wow. When processing a single image, we get a <strong class="markup--strong markup--p-strong">consistent 4x speedup</strong>. Then, things tend to level (not sure why) before improving again when we really start processing larger batch sizes: 4x speedup is reached again at batch size 256.</p><p name="a7cb" id="a7cb" class="graf graf--p graf-after--p">Now, let’s try Alexnet now and see what performance boost we get for convolutional networks.</p><figure name="6f00" id="6f00" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*C2Ff612E-C5depMwj7hMyA.png" data-width="994" data-height="941" src="https://cdn-images-1.medium.com/max/800/1*C2Ff612E-C5depMwj7hMyA.png"><figcaption class="imageCaption">batch size vs image per seconds for Alexnet</figcaption></figure><p name="3f67" id="3f67" class="graf graf--p graf-after--figure">Very impressive. We still get <strong class="markup--strong markup--p-strong">4x speedup</strong> for single-image prediction and exceed <strong class="markup--strong markup--p-strong">6x speedup</strong> for bulk prediction.</p><h4 name="2fee" id="2fee" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="7610" id="7610" class="graf graf--p graf-after--h4">We did get a nice speedup on Intel processors, but the optimised code of NNPACK really shines on the less powerful ARM v8. MXNet inference gets a massive performance boost for both single images and larger batches. Kudos to Marat Dukhan, the author of NNPACK.</p><blockquote name="bb92" id="bb92" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">From this day, NNPACK will be a mandatory build option for MXNet on my Pi. I will definitely keep an eye on future releases :)</blockquote><p name="ed69" id="ed69" class="graf graf--p graf-after--blockquote graf--trailing">That’s it for today, thanks for reading!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/e444b446a180"><time class="dt-published" datetime="2017-09-15T15:41:53.441Z">September 15, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-raspberry-pi-edition-e444b446a180" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>