<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>In this video, we run local inference on an Apple M3 MacBook with llama.cpp</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">In this video, we run local inference on an Apple M3 MacBook with llama.cpp</h1>
</header>
<section data-field="subtitle" class="p-summary">
First, we download the two models from the Hugging Face hub with the Hugging Face CLI. Then, we go through the step-by-step installation…
</section>
<section data-field="body" class="e-content">
<section name="bee4" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b588" id="b588" class="graf graf--h3 graf--leading graf--title">Local inference shootout: Llama.cpp vs. MLX on 10B and 32B Arcee SLMs</h3><p name="74cc" id="74cc" class="graf graf--p graf-after--h3">In this video, we run local inference on an Apple M3 MacBook with llama.cpp and MLX, two projects that optimize and accelerate SLMs on CPU platforms. For this purpose, we use two new Arcee open-source models distilled from DeepSeek-v3: Virtuoso Lite 10B and Virtuoso Medium v2 32B.</p><figure name="1e77" id="1e77" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uioC6Q_C7ousk9wCxu3P6w.png" data-width="1912" data-height="1068" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*uioC6Q_C7ousk9wCxu3P6w.png"></figure><p name="d4a8" id="d4a8" class="graf graf--p graf-after--figure">First, we download the two models from the Hugging Face hub with the Hugging Face CLI. Then, we go through the step-by-step installation procedure for llama.cpp and MLX. Next, we optimize and quantize the models to 4-bit precision for maximum acceleration. Finally, we run inference and look at performance numbers. So, who’s fastest? Watch and find out!</p><figure name="8b17" id="8b17" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/Vk1W6evtsjE?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/76de0855f5b0"><time class="dt-published" datetime="2025-02-05T13:58:50.755Z">February 5, 2025</time></a>.</p><p><a href="https://medium.com/@julsimon/in-this-video-we-run-local-inference-on-an-apple-m3-macbook-with-llama-cpp-76de0855f5b0" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
