<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Training MXNet — part 1: MNIST</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Training MXNet — part 1: MNIST</h1>
</header>
<section data-field="subtitle" class="p-summary">
In a previous series, we discovered how we could use the MXNet library and pre-trained models for object detection. In this series, we’re…
</section>
<section data-field="body" class="e-content">
<section name="aacd" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="5efb" id="5efb" class="graf graf--h3 graf--leading graf--title">Training MXNet — part 1: MNIST</h3><p name="990f" id="990f" class="graf graf--p graf-after--h3">In a <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" class="markup--anchor markup--p-anchor" target="_blank">previous series</a>, we discovered how we could use the <a href="http://mxnet.io/" data-href="http://mxnet.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MXNet</a> library and <a href="http://mxnet.io/model_zoo/" data-href="http://mxnet.io/model_zoo/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">pre-trained models</a> for object detection. In this series, we’re going to focus on <strong class="markup--strong markup--p-strong">training models</strong> with a number of different data sets.</p><p name="9a53" id="9a53" class="graf graf--p graf-after--p">Let’s start with the famous <strong class="markup--strong markup--p-strong">MNIST</strong> data set.</p><p name="d41a" id="d41a" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Please note that is an updated and expanded version of this </em><a href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" data-href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">tutorial:</em></a><em class="markup--em markup--p-em"> I’m using the Module API (instead of the deprecated Model API) as well as the MNIST data iterator.</em></p><h4 name="6d43" id="6d43" class="graf graf--h4 graf-after--p">The MNIST data set</h4><p name="d98b" id="d98b" class="graf graf--p graf-after--h4">This <a href="http://yann.lecun.com/exdb/mnist/" data-href="http://yann.lecun.com/exdb/mnist/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">data</a> is a set of 28x28 greyscale images representing <strong class="markup--strong markup--p-strong">handwritten digits</strong> (0 to 9).</p><figure name="dd69" id="dd69" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yBdJCRwIJGoM7pwU-LNW6Q.png" data-width="479" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*yBdJCRwIJGoM7pwU-LNW6Q.png"><figcaption class="imageCaption">Samples from the MNIST data set</figcaption></figure><p name="c1ca" id="c1ca" class="graf graf--p graf-after--figure">The <strong class="markup--strong markup--p-strong">training set</strong> has 60,000 samples and the <strong class="markup--strong markup--p-strong">test set</strong> has 10,000 examples. Let’s download them right away.</p><pre name="9edd" id="9edd" class="graf graf--pre graf-after--p"># Training set: images and labels<br>$ wget <a href="http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz" data-href="http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz</a><br>$ wget <a href="http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz" data-href="http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz</a></pre><pre name="4b04" id="4b04" class="graf graf--pre graf-after--pre"># Validation set: images and labels<br>$ wget <a href="http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz" data-href="http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz</a><br>$ wget <a href="http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz" data-href="http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz</a></pre><pre name="6992" id="6992" class="graf graf--pre graf-after--pre">$ gzip -d *</pre><p name="85c8" id="85c8" class="graf graf--p graf-after--pre">How about we take a look <strong class="markup--strong markup--p-strong">inside</strong> these files? We’ll start with the labels. They are stored as a <strong class="markup--strong markup--p-strong">serialized </strong><a href="http://www.numpy.org" data-href="http://www.numpy.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">numpy</strong></a><strong class="markup--strong markup--p-strong"> array</strong> holding 60,000 unsigned bytes.</p><p name="7cb1" id="7cb1" class="graf graf--p graf-after--p">The file starts with a big-endian packed structure, holding 2 integers: magic number, number of labels.</p><pre name="2e09" id="2e09" class="graf graf--pre graf-after--p">&gt;&gt;&gt; import struct<br>&gt;&gt;&gt; import numpy as np<br>&gt;&gt;&gt; import cv2</pre><pre name="df93" id="df93" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; labelfile = open(&quot;train-labels-idx1-ubyte&quot;)<br>&gt;&gt;&gt; magic, num = struct.unpack(&quot;&gt;II&quot;, labelfile.read(8))<br>&gt;&gt;&gt; labelarray = np.fromstring(labelfile.read(), dtype=np.int8)</pre><pre name="6e9c" id="6e9c" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; print labelarray.shape<br>&gt;&gt;&gt; print labelarray[0:10]</pre><pre name="1c8c" id="1c8c" class="graf graf--pre graf-after--pre">(60000,)<br>[5 0 4 1 9 2 1 3 1 4]</pre><p name="dcfe" id="dcfe" class="graf graf--p graf-after--pre">Let’s now extract some images. Again, they are stored as a serialized <a href="http://www.numpy.org" data-href="http://www.numpy.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">numpy</a> array, which we will <strong class="markup--strong markup--p-strong">reshape</strong> to build 28x28 images. Each pixel is stored as an unsigned byte (0 for black, 255 for white).</p><p name="d0d0" id="d0d0" class="graf graf--p graf-after--p">The file starts with a big-endian packed structure, holding 4 integers: magic number, number of images, number of rows and number of columns.</p><pre name="2276" id="2276" class="graf graf--pre graf-after--p">&gt;&gt;&gt; imagefile = open(&quot;train-images-idx3-ubyte&quot;)<br>&gt;&gt;&gt; magic, num, rows, cols = struct.unpack(&quot;&gt;IIII&quot;, imagefile.read(16))<br>&gt;&gt;&gt; imagearray = np.fromstring(imagefile.read(), dtype=np.uint8)<br>&gt;&gt;&gt; print imagearray.shape<br>(47040000,)</pre><pre name="0e87" id="0e87" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; imagearray = imagearray.reshape(num, rows, cols)<br>&gt;&gt;&gt; print imagearray.shape<br>(60000, 28, 28)</pre><p name="43ba" id="43ba" class="graf graf--p graf-after--pre">Let’s save the <strong class="markup--strong markup--p-strong">first 10 images</strong> to disk.</p><pre name="05d9" id="05d9" class="graf graf--pre graf-after--p">&gt;&gt;&gt; for i in range(0,10):<br>        img = imagearray[i]<br>        imgname = &quot;img&quot;+(str)(i)+&quot;.png&quot;<br>        cv2.imwrite(imgname, img)</pre><pre name="d0f0" id="d0f0" class="graf graf--pre graf-after--pre">$ ls img?.png<br>img0.png img1.png img2.png img3.png img4.png img5.png img6.png img7.png img8.png img9.png</pre><p name="8363" id="8363" class="graf graf--p graf-after--pre">This is how they look.</p><figure name="238b" id="238b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3q5hPhgJV17TevewNlt6qw.png" data-width="476" data-height="49" src="https://cdn-images-1.medium.com/max/800/1*3q5hPhgJV17TevewNlt6qw.png"></figure><p name="8b39" id="8b39" class="graf graf--p graf-after--figure">Ok, now that we understand the data, let’s build a <strong class="markup--strong markup--p-strong">model</strong>.</p><h4 name="4dbe" id="4dbe" class="graf graf--h4 graf-after--p">Building a model</h4><p name="046a" id="046a" class="graf graf--p graf-after--h4">We’re going to use a simple <strong class="markup--strong markup--p-strong">multi-layer perceptron</strong> (similar to what we built <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" class="markup--anchor markup--p-anchor" target="_blank">here</a>) : 784 → 128 → 64 → 10</p><ul class="postList"><li name="e0ba" id="e0ba" class="graf graf--li graf-after--p">Input layer: an array of <strong class="markup--strong markup--li-strong">784</strong> pixel values (28x28).</li><li name="9aaf" id="9aaf" class="graf graf--li graf-after--li">First layer: <strong class="markup--strong markup--li-strong">128</strong> neurons activated by the <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" data-href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">rectifier linear unit</a> function.</li><li name="0a2d" id="0a2d" class="graf graf--li graf-after--li">Second layer: <strong class="markup--strong markup--li-strong">64</strong> neurons activated by the same function.</li><li name="60d4" id="60d4" class="graf graf--li graf-after--li">Output layer: <strong class="markup--strong markup--li-strong">10</strong> neurons (for our 10 categories), activated by the <a href="https://en.wikipedia.org/wiki/Softmax_function" data-href="https://en.wikipedia.org/wiki/Softmax_function" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">softmax function</a> in order to transform the 10 outputs into 10 values between 0 and 1 that add up to 1. Each value represents the <strong class="markup--strong markup--li-strong">predicted probability for each category</strong>, the largest one pointing at the most likely category.</li></ul><pre name="110e" id="110e" class="graf graf--pre graf-after--li">data = mx.sym.Variable(&#39;data&#39;)<br>data = mx.sym.Flatten(data=data)<br>fc1  = mx.sym.FullyConnected(data=data, name=&#39;fc1&#39;, num_hidden=128)<br>act1 = mx.sym.Activation(data=fc1, name=&#39;relu1&#39;, act_type=&quot;relu&quot;)<br>fc2  = mx.sym.FullyConnected(data=act1, name=&#39;fc2&#39;, num_hidden = 64)<br>act2 = mx.sym.Activation(data=fc2, name=&#39;relu2&#39;, act_type=&quot;relu&quot;)<br>fc3  = mx.sym.FullyConnected(data=act2, name=&#39;fc3&#39;, num_hidden=10)<br>mlp  = mx.sym.SoftmaxOutput(data=fc3, name=&#39;softmax&#39;)mod = mx.mod.Module(mlp)</pre><h4 name="4441" id="4441" class="graf graf--h4 graf-after--pre">Building a data iterator</h4><p name="50ed" id="50ed" class="graf graf--p graf-after--h4">MXNet conveniently provides a <a href="http://mxnet.io/api/python/io.html#mxnet.io.MNISTIter" data-href="http://mxnet.io/api/python/io.html#mxnet.io.MNISTIter" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">data iterator</strong></a> for the MNIST data set. Thanks to this, we don’t have to open the files, build <em class="markup--em markup--p-em">NDArrays</em>, etc. It also has default parameters for filenames and so on. Very cool!</p><pre name="1613" id="1613" class="graf graf--pre graf-after--p">train_iter = mx.io.MNISTIter(shuffle=True)<br>val_iter = mx.io.MNISTIter(image=&quot;./t10k-images-idx3-ubyte&quot;, label=&quot;./t10k-labels-idx1-ubyte&quot;)</pre><p name="7bf3" id="7bf3" class="graf graf--p graf-after--pre">We can now <strong class="markup--strong markup--p-strong">bind</strong> the data to our model. Default batch size is 128.</p><pre name="697e" id="697e" class="graf graf--pre graf-after--p">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)</pre><p name="0498" id="0498" class="graf graf--p graf-after--pre">We’re now ready for training.</p><h4 name="240f" id="240f" class="graf graf--h4 graf-after--p">Training the model</h4><p name="5186" id="5186" class="graf graf--p graf-after--h4">Let’s start with default settings for weight initialization and optimization (aka <strong class="markup--strong markup--p-strong">hyperparameters</strong>) and hope for the best. Here we go!</p><pre name="1083" id="1083" class="graf graf--pre graf-after--p">&gt;&gt;&gt; import logging<br>&gt;&gt;&gt; logging.basicConfig(level=logging.INFO)<br>&gt;&gt;&gt; mod.fit(train_iter, num_epoch=10)</pre><pre name="6f7b" id="6f7b" class="graf graf--pre graf-after--pre">INFO:root:Epoch[0] Train-accuracy=0.111662<br>INFO:root:Epoch[0] Time cost=1.244<br>INFO:root:Epoch[1] Train-accuracy=0.112346<br>INFO:root:Epoch[1] Time cost=1.376<br>INFO:root:Epoch[2] Train-accuracy=0.112346<br>INFO:root:Epoch[2] Time cost=1.254<br>INFO:root:Epoch[3] Train-accuracy=0.112346<br>INFO:root:Epoch[3] Time cost=1.296<br>INFO:root:Epoch[4] Train-accuracy=0.112346<br>INFO:root:Epoch[4] Time cost=1.234<br>INFO:root:Epoch[5] Train-accuracy=0.112346<br>INFO:root:Epoch[5] Time cost=1.283<br>INFO:root:Epoch[6] Train-accuracy=0.112346<br>INFO:root:Epoch[6] Time cost=1.440<br>INFO:root:Epoch[7] Train-accuracy=0.112346<br>INFO:root:Epoch[7] Time cost=1.237<br>INFO:root:Epoch[8] Train-accuracy=0.112346<br>INFO:root:Epoch[8] Time cost=1.235<br>INFO:root:Epoch[9] Train-accuracy=0.112346<br>INFO:root:Epoch[9] Time cost=1.307</pre><p name="bbca" id="bbca" class="graf graf--p graf-after--pre">Hmm, things are not going well. It looks like the network is <strong class="markup--strong markup--p-strong">not learning</strong>. Actually, it is learning, but <strong class="markup--strong markup--p-strong">real slow</strong>: the default learning rate is <strong class="markup--strong markup--p-strong">0.01</strong>, which is too low. Let’s use a more reasonable value such as 0.1.</p><pre name="39c9" id="39c9" class="graf graf--pre graf-after--p">&gt;&gt;&gt; mod.init_params()<br>&gt;&gt;&gt; mod.init_optimizer(optimizer_params=((&#39;learning_rate&#39;, 0.1), ))<br>&gt;&gt;&gt; mod.fit(train_iter, num_epoch=10)</pre><pre name="20b6" id="20b6" class="graf graf--pre graf-after--pre">INFO:root:Epoch[0] Train-accuracy=0.111846<br>INFO:root:Epoch[0] Time cost=1.288<br>INFO:root:Epoch[1] Train-accuracy=0.427150<br>INFO:root:Epoch[1] Time cost=1.308<br>INFO:root:Epoch[2] Train-accuracy=0.842682<br>INFO:root:Epoch[2] Time cost=1.271<br>INFO:root:Epoch[3] Train-accuracy=0.900875<br>INFO:root:Epoch[3] Time cost=1.282<br>INFO:root:Epoch[4] Train-accuracy=0.928736<br>INFO:root:Epoch[4] Time cost=1.288<br>INFO:root:Epoch[5] Train-accuracy=0.944478<br>INFO:root:Epoch[5] Time cost=1.296<br>INFO:root:Epoch[6] Train-accuracy=0.953993<br>INFO:root:Epoch[6] Time cost=1.287<br>INFO:root:Epoch[7] Train-accuracy=0.960453<br>INFO:root:Epoch[7] Time cost=1.294<br>INFO:root:Epoch[8] Train-accuracy=0.965478<br>INFO:root:Epoch[8] Time cost=1.297<br>INFO:root:Epoch[9] Train-accuracy=0.969267<br>INFO:root:Epoch[9] Time cost=1.291</pre><p name="c8e4" id="c8e4" class="graf graf--p graf-after--pre">That’s more like it. We get to <strong class="markup--strong markup--p-strong">96.93%</strong> accuracy after 10 epochs. What about validation accuracy? Let’s create a metric and score our validation data set.</p><pre name="11fa" id="11fa" class="graf graf--pre graf-after--p">&gt;&gt; metric = mx.metric.Accuracy()<br>&gt;&gt; mod.score(val_iter, metric)<br>&gt;&gt; print metric.get()<br>(&#39;accuracy&#39;, 0.9654447115384616)</pre><p name="76b3" id="76b3" class="graf graf--p graf-after--pre">Pretty good accuracy at <strong class="markup--strong markup--p-strong">96.5</strong>%.</p><p name="494b" id="494b" class="graf graf--p graf-after--p">Still, the first few training epochs are not great: this is caused by default <strong class="markup--strong markup--p-strong">weight initialization</strong>. Let’s use something <a href="http://mxnet.io/api/python/optimization.html#the-mxnet-initializer-package" data-href="http://mxnet.io/api/python/optimization.html#the-mxnet-initializer-package" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">smarter</a>, like the Xavier technique.</p><pre name="e97b" id="e97b" class="graf graf--pre graf-after--p">&gt;&gt;&gt; mod.init_params(initializer=mx.init.Xavier(magnitude=2.))<br>&gt;&gt;&gt; mod.init_optimizer(optimizer_params=((&#39;learning_rate&#39;, 0.1), ))<br>&gt;&gt;&gt; mod.fit(train_iter, num_epoch=10)</pre><pre name="dd73" id="dd73" class="graf graf--pre graf-after--pre">INFO:root:Epoch[0] Train-accuracy=0.860994<br>INFO:root:Epoch[0] Time cost=1.338<br>INFO:root:Epoch[1] Train-accuracy=0.935797<br>INFO:root:Epoch[1] Time cost=1.325<br>INFO:root:Epoch[2] Train-accuracy=0.951840<br>INFO:root:Epoch[2] Time cost=1.273<br>INFO:root:Epoch[3] Train-accuracy=0.961438<br>INFO:root:Epoch[3] Time cost=1.264<br>INFO:root:Epoch[4] Train-accuracy=0.968066<br>INFO:root:Epoch[4] Time cost=1.250<br>INFO:root:Epoch[5] Train-accuracy=0.973174<br>INFO:root:Epoch[5] Time cost=1.299<br>INFO:root:Epoch[6] Train-accuracy=0.976846<br>INFO:root:Epoch[6] Time cost=1.374<br>INFO:root:Epoch[7] Train-accuracy=0.979601<br>INFO:root:Epoch[7] Time cost=1.407<br>INFO:root:Epoch[8] Train-accuracy=0.982121<br>INFO:root:Epoch[8] Time cost=1.336<br>INFO:root:Epoch[9] Train-accuracy=0.983958<br>INFO:root:Epoch[9] Time cost=1.343</pre><pre name="165b" id="165b" class="graf graf--pre graf-after--pre">&gt;&gt; metric = mx.metric.Accuracy()<br>&gt;&gt; mod.score(val_iter, metric)<br>&gt;&gt; print metric.get()<br>(&#39;accuracy&#39;, 0.9744591346153846)</pre><p name="cda8" id="cda8" class="graf graf--p graf-after--pre">That’s much better: we get to <strong class="markup--strong markup--p-strong">86%</strong> accuracy after only one epoch. We gain almost <strong class="markup--strong markup--p-strong">1.5%</strong> training accuracy and <strong class="markup--strong markup--p-strong">1%</strong> validation accuracy.</p><p name="2cbf" id="2cbf" class="graf graf--p graf-after--p">Can we get better results? Well, we could always try to train the model longer. Let’s try 50 epochs.</p><pre name="d456" id="d456" class="graf graf--pre graf-after--p">...<br>INFO:root:Epoch[39] Train-accuracy=0.999950<br>INFO:root:Epoch[39] Time cost=1.284<br>INFO:root:Epoch[40] Train-accuracy=0.999967<br>INFO:root:Epoch[40] Time cost=1.301<br>INFO:root:Epoch[41] Train-accuracy=0.999967<br>INFO:root:Epoch[41] Time cost=1.811<br>INFO:root:Epoch[42] Train-accuracy=1.000000<br>INFO:root:Epoch[42] Time cost=1.412<br>INFO:root:Epoch[43] Train-accuracy=1.000000<br>INFO:root:Epoch[43] Time cost=1.275<br>INFO:root:Epoch[44] Train-accuracy=1.000000<br>INFO:root:Epoch[44] Time cost=1.200<br>...<br>(&#39;accuracy&#39;, 0.9785657051282052)</pre><p name="13e1" id="13e1" class="graf graf--p graf-after--pre">As you can see, we hit 100% training accuracy after 42 epochs and there’s no point in going any further. In the process, we only manage to improve validation accuracy by 0.4%.</p><p name="a842" id="a842" class="graf graf--p graf-after--p">Is this the best we can do? We could try <a href="http://mxnet.io/api/python/model.html#optimizer-api-reference" data-href="http://mxnet.io/api/python/model.html#optimizer-api-reference" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">other optimizers</a>, but unless you really know what you’re doing, it’s probably safer to stick to SGD.</p><p name="7b2e" id="7b2e" class="graf graf--p graf-after--p">Maybe we simply need a bigger boat?</p><h4 name="f292" id="f292" class="graf graf--h4 graf-after--p">Using a deeper network</h4><p name="4f50" id="4f50" class="graf graf--p graf-after--h4">Let’s try this network and see what happens :</p><p name="d871" id="d871" class="graf graf--p graf-after--p">784 → 256 → 128 → 64 → 10</p><pre name="b555" id="b555" class="graf graf--pre graf-after--p">data = mx.sym.Variable(&#39;data&#39;)<br>data = mx.sym.Flatten(data=data)</pre><pre name="da1f" id="da1f" class="graf graf--pre graf-after--pre">fc1  = mx.sym.FullyConnected(data=data, name=&#39;fc1&#39;, num_hidden=256)<br>act1 = mx.sym.Activation(data=fc1, name=&#39;relu1&#39;, act_type=&quot;relu&quot;)</pre><pre name="6c4a" id="6c4a" class="graf graf--pre graf-after--pre">fc2  = mx.sym.FullyConnected(data=act1, name=&#39;fc2&#39;, num_hidden = 128)<br>act2 = mx.sym.Activation(data=fc2, name=&#39;relu2&#39;, act_type=&quot;relu&quot;)</pre><pre name="379f" id="379f" class="graf graf--pre graf-after--pre">fc3  = mx.sym.FullyConnected(data=act2, name=&#39;fc3&#39;, num_hidden = 64)<br>act3 = mx.sym.Activation(data=fc3, name=&#39;relu3&#39;, act_type=&quot;relu&quot;)</pre><pre name="defd" id="defd" class="graf graf--pre graf-after--pre">fc4  = mx.sym.FullyConnected(data=act3, name=&#39;fc4&#39;, num_hidden=10)<br>mlp  = mx.sym.SoftmaxOutput(data=fc4, name=&#39;softmax&#39;)</pre><pre name="f93f" id="f93f" class="graf graf--pre graf-after--pre">mod = mx.mod.Module(mlp)</pre><pre name="b69b" id="b69b" class="graf graf--pre graf-after--pre">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)<br>mod.init_params(initializer=mx.init.Xavier(magnitude=2.))<br>mod.init_optimizer(optimizer_params=((&#39;learning_rate&#39;, 0.1), ))<br>mod.fit(train_iter, num_epoch=50)</pre><p name="07fb" id="07fb" class="graf graf--p graf-after--pre">We hit 100% training accuracy after 25 epochs and get to <strong class="markup--strong markup--p-strong">97.99%</strong> validation accuracy, a <strong class="markup--strong markup--p-strong">modest 0.14% increase</strong> compared to the previous model. Clearly, a deeper multi-layer perceptron is not the way to go.</p><p name="95de" id="95de" class="graf graf--p graf-after--p">We need a better boat, then.</p><h4 name="24ad" id="24ad" class="graf graf--h4 graf-after--p">Using a Convolutional Neural Network</h4><p name="8669" id="8669" class="graf graf--p graf-after--h4">We’ve seen that these networks work <strong class="markup--strong markup--p-strong">very well</strong> for image processing. Let’s try a well-known CNN — called <a href="http://yann.lecun.com/exdb/lenet/" data-href="http://yann.lecun.com/exdb/lenet/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">LeNet </strong></a>— on this data set.</p><p name="4da2" id="4da2" class="graf graf--p graf-after--p">Here is the model definition, everything else is identical.</p><pre name="aabf" id="aabf" class="graf graf--pre graf-after--p">data = mx.symbol.Variable(&#39;data&#39;)<br><br>conv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)<br>tanh1 = mx.sym.Activation(data=conv1, act_type=&quot;tanh&quot;)<br>pool1 = mx.sym.Pooling(data=tanh1, pool_type=&quot;max&quot;, kernel=(2,2), stride=(2,2))<br><br>conv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)<br>tanh2 = mx.sym.Activation(data=conv2, act_type=&quot;tanh&quot;)<br>pool2 = mx.sym.Pooling(data=tanh2, pool_type=&quot;max&quot;, kernel=(2,2), stride=(2,2))<br><br>flatten = mx.sym.Flatten(data=pool2)<br>fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)<br>tanh3 = mx.sym.Activation(data=fc1, act_type=&quot;tanh&quot;)<br><br>fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)<br><br>lenet = mx.sym.SoftmaxOutput(data=fc2, name=&#39;softmax&#39;)</pre><pre name="d402" id="d402" class="graf graf--pre graf-after--pre">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)<br>mod.init_params(initializer=mx.init.Xavier(magnitude=2.))<br>mod.init_optimizer(optimizer_params=((&#39;learning_rate&#39;, 0.1), ))<br>mod.fit(train_iter, num_epoch=10)</pre><p name="03a6" id="03a6" class="graf graf--p graf-after--pre">Let’s train again.</p><pre name="6593" id="6593" class="graf graf--pre graf-after--p">INFO:root:Epoch[0] Train-accuracy=0.906634<br>INFO:root:Epoch[0] Time cost=46.034<br>INFO:root:Epoch[1] Train-accuracy=0.971989<br>INFO:root:Epoch[1] Time cost=47.089</pre><p name="1a44" id="1a44" class="graf graf--p graf-after--pre">This is <strong class="markup--strong markup--p-strong">painfully</strong> slow. About 45 seconds per epoch, about <strong class="markup--strong markup--p-strong">30 times slower</strong> than the multilayer perceptron. Now would be a good time to try these fancy GPUs, don’t you think?</p><h4 name="87f0" id="87f0" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Training on a single GPU</strong></h4><p name="b22d" id="b22d" class="graf graf--p graf-after--h4">By chance, I’ve running this on a <a href="https://aws.amazon.com/fr/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" data-href="https://aws.amazon.com/fr/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">g2.8xlarge instance</a>. It has 4 NVidia GPUs ready to crunch data :)</p><pre name="f9e1" id="f9e1" class="graf graf--pre graf-after--p">[ec2-user]$ nvidia-smi -L<br>GPU 0: GRID K520 (UUID: GPU-5134e206-9b30-e1a8-a949-3d9e637a6465)<br>GPU 1: GRID K520 (UUID: GPU-221cb85e-2d26-b615-b674-ad596a8c12ee)<br>GPU 2: GRID K520 (UUID: GPU-ec4584ae-08e9-036f-d94a-ab60c52cf6fd)<br>GPU 3: GRID K520 (UUID: GPU-9bd3fe35-ac18-5d1a-4fb1-d819c9265ec2)</pre><p name="7511" id="7511" class="graf graf--p graf-after--pre">All it takes to switch from CPU to GPU is this. Amazing!</p><pre name="6318" id="6318" class="graf graf--pre graf-after--p">#mod = mx.mod.Module(lenet)<br>mod = mx.mod.Module(lenet, context=mx.gpu(0))</pre><p name="f81a" id="f81a" class="graf graf--p graf-after--pre">Here we go again.</p><pre name="8a0d" id="8a0d" class="graf graf--pre graf-after--p">INFO:root:Epoch[0] Train-accuracy=0.906651<br>INFO:root:Epoch[0] Time cost=3.452<br>INFO:root:Epoch[1] Train-accuracy=0.972022<br>INFO:root:Epoch[1] Time cost=3.455<br>INFO:root:Epoch[2] Train-accuracy=0.980786<br>INFO:root:Epoch[2] Time cost=3.450<br>INFO:root:Epoch[3] Train-accuracy=0.985210<br>INFO:root:Epoch[3] Time cost=3.454<br>INFO:root:Epoch[4] Train-accuracy=0.987931<br>INFO:root:Epoch[4] Time cost=3.454<br>INFO:root:Epoch[5] Train-accuracy=0.989633<br>INFO:root:Epoch[5] Time cost=3.453<br>INFO:root:Epoch[6] Train-accuracy=0.991036<br>INFO:root:Epoch[6] Time cost=3.449<br>INFO:root:Epoch[7] Train-accuracy=0.992238<br>INFO:root:Epoch[7] Time cost=3.451<br>INFO:root:Epoch[8] Train-accuracy=0.993323<br>INFO:root:Epoch[8] Time cost=3.453<br>INFO:root:Epoch[9] Train-accuracy=0.994191<br>INFO:root:Epoch[9] Time cost=3.452<br>(&#39;accuracy&#39;, 0.9903846153846154)</pre><p name="0c9a" id="0c9a" class="graf graf--p graf-after--pre">Nice! Training time has been <strong class="markup--strong markup--p-strong">massively</strong> reduced. Accuracy is now <strong class="markup--strong markup--p-strong">99+%</strong> thanks to the more sophisticated model.</p><p name="a51a" id="a51a" class="graf graf--p graf-after--p">Did I mention that there are four GPUs in this box? How about using <strong class="markup--strong markup--p-strong">more</strong> than one?</p><h4 name="7530" id="7530" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Training on multiple GPUs</strong></h4><p name="e4b9" id="e4b9" class="graf graf--p graf-after--h4">Once again, this is pretty simple to set up.</p><pre name="530f" id="530f" class="graf graf--pre graf-after--p">#mod = mx.mod.Module(lenet, context=mx.gpu(0))<br>mod = mx.mod.Module(lenet, context=(mx.gpu(0), mx.gpu(1)))</pre><pre name="43ed" id="43ed" class="graf graf--pre graf-after--pre">INFO:root:Epoch[0] Train-accuracy=0.906701<br>INFO:root:Epoch[0] Time cost=2.592<br>INFO:root:Epoch[1] Train-accuracy=0.972055<br>INFO:root:Epoch[1] Time cost=2.329<br>INFO:root:Epoch[2] Train-accuracy=0.980819<br>INFO:root:Epoch[2] Time cost=2.302<br>INFO:root:Epoch[3] Train-accuracy=0.985193<br>INFO:root:Epoch[3] Time cost=2.302<br>INFO:root:Epoch[4] Train-accuracy=0.987981<br>INFO:root:Epoch[4] Time cost=2.297<br>INFO:root:Epoch[5] Train-accuracy=0.989583<br>INFO:root:Epoch[5] Time cost=2.302<br>INFO:root:Epoch[6] Train-accuracy=0.991119<br>INFO:root:Epoch[6] Time cost=2.305<br>INFO:root:Epoch[7] Train-accuracy=0.992238<br>INFO:root:Epoch[7] Time cost=2.303<br>INFO:root:Epoch[8] Train-accuracy=0.993273<br>INFO:root:Epoch[8] Time cost=2.297<br>INFO:root:Epoch[9] Train-accuracy=0.994174<br>INFO:root:Epoch[9] Time cost=2.307</pre><p name="e6e2" id="e6e2" class="graf graf--p graf-after--pre">We saved <strong class="markup--strong markup--p-strong">50%</strong> of training time. Let’s go for three GPUs.</p><pre name="12a9" id="12a9" class="graf graf--pre graf-after--p">#mod = mx.mod.Module(lenet, context=(mx.gpu(0), mx.gpu(1)))<br>mod = mx.mod.Module(lenet, context=(mx.gpu(0), mx.gpu(1), mx.gpu(2)))</pre><pre name="b637" id="b637" class="graf graf--pre graf-after--pre">INFO:root:Epoch[0] Train-accuracy=0.906667<br>INFO:root:Epoch[0] Time cost=1.938<br>INFO:root:Epoch[1] Train-accuracy=0.972055<br>INFO:root:Epoch[1] Time cost=1.924<br>INFO:root:Epoch[2] Train-accuracy=0.980836<br>INFO:root:Epoch[2] Time cost=1.916<br>INFO:root:Epoch[3] Train-accuracy=0.985193<br>INFO:root:Epoch[3] Time cost=1.903<br>INFO:root:Epoch[4] Train-accuracy=0.987997<br>INFO:root:Epoch[4] Time cost=1.910<br>INFO:root:Epoch[5] Train-accuracy=0.989600<br>INFO:root:Epoch[5] Time cost=1.910<br>INFO:root:Epoch[6] Train-accuracy=0.991052<br>INFO:root:Epoch[6] Time cost=1.912<br>INFO:root:Epoch[7] Train-accuracy=0.992288<br>INFO:root:Epoch[7] Time cost=1.921<br>INFO:root:Epoch[8] Train-accuracy=0.993339<br>INFO:root:Epoch[8] Time cost=1.934<br>INFO:root:Epoch[9] Train-accuracy=0.994157<br>INFO:root:Epoch[9] Time cost=1.937<br>(&#39;accuracy&#39;, 0.9904847756410257)</pre><p name="604d" id="604d" class="graf graf--p graf-after--pre">Another <strong class="markup--strong markup--p-strong">20%</strong> saved. Training time is now only 50% more than what it was for the CPU-version of the multi-layer perceptron.</p><p name="e155" id="e155" class="graf graf--p graf-after--p">Adding a fourth GPU won’t help. Yes, I tried :) Anyway, we’re pretty happy with our model, so let’s <strong class="markup--strong markup--p-strong">save</strong> it for future use.</p><h4 name="7dc1" id="7dc1" class="graf graf--h4 graf-after--p">Saving our model</h4><p name="e94f" id="e94f" class="graf graf--p graf-after--h4">Saving a model just requires a file name and an epoch number.</p><pre name="450c" id="450c" class="graf graf--pre graf-after--p">mod.save_checkpoint(&quot;lenet&quot;, 10)</pre><p name="bb33" id="bb33" class="graf graf--p graf-after--pre">This creates two files (which you should now be familiar with):</p><ul class="postList"><li name="6f8f" id="6f8f" class="graf graf--li graf-after--p">the <strong class="markup--strong markup--li-strong">symbol</strong> file, containing the model definition (3.5KB)</li><li name="5f9f" id="5f9f" class="graf graf--li graf-after--li">the <strong class="markup--strong markup--li-strong">parameter</strong> file, containing all our trained parameters (1.7MB)</li></ul><pre name="0a9a" id="0a9a" class="graf graf--pre graf-after--li">$ ls lenet*<br>lenet-0010.params  lenet-symbol.json</pre><h4 name="0522" id="0522" class="graf graf--h4 graf-after--pre">Reusing our model</h4><p name="7bfe" id="7bfe" class="graf graf--p graf-after--h4">Just like we did in previous articles, we’re now able to <strong class="markup--strong markup--p-strong">load</strong> this pre-trained model.</p><pre name="1f79" id="1f79" class="graf graf--pre graf-after--p">lenet, arg_params, aux_params = mx.model.load_checkpoint(&quot;lenet&quot;, 10)<br>mod = mx.mod.Module(lenet)<br>mod.bind(for_training=False, data_shapes=[(&#39;data&#39;, (1,1,28,28))])<br>mod.set_params(arg_params, aux_params)</pre><p name="23da" id="23da" class="graf graf--p graf-after--pre">Here are the ugly digits I created with Paintbrush :)</p><figure name="7171" id="7171" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5tCkNkh8wbDnxYdujOFgAg.png" data-width="280" data-height="28" src="https://cdn-images-1.medium.com/max/800/1*5tCkNkh8wbDnxYdujOFgAg.png"><figcaption class="imageCaption">My home-made digits</figcaption></figure><p name="56d3" id="56d3" class="graf graf--p graf-after--figure">I saved them as a 28x28 images, which I can now load as <em class="markup--em markup--p-em">numpy</em> arrays. I need to normalize pixels values and add two dimensions to reshape the array from (28, 28) to (1, 1, 28, 28) : batch size of one, one channel (greyscale), 28 x 28 pixels.</p><pre name="c166" id="c166" class="graf graf--pre graf-after--p">def loadImage(filename):<br>        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)<br>        img = img / 255<br>        print img<br>        img = np.expand_dims(img, axis=0)<br>        img = np.expand_dims(img, axis=0)<br>        return mx.nd.array(img)</pre><p name="f7d9" id="f7d9" class="graf graf--p graf-after--pre">We’ll predict image by image. To avoid building a data iterator, I’ll use the same trick we’ve seen before (using a <em class="markup--em markup--p-em">namedtuple</em> to provide a data attribute).</p><pre name="7d91" id="7d91" class="graf graf--pre graf-after--p">def predict(model, filename):<br>        array = loadImage(filename)<br>        Batch = namedtuple(&#39;Batch&#39;, [&#39;data&#39;])<br>        mod.forward(Batch([array]))<br>        pred = mod.get_outputs()[0].asnumpy()<br>        return pred</pre><p name="f97d" id="f97d" class="graf graf--p graf-after--pre">Now we’re ready. Let check these digits!</p><pre name="bcea" id="bcea" class="graf graf--pre graf-after--p">np.set_printoptions(precision=3, suppress=True)</pre><pre name="64b4" id="64b4" class="graf graf--pre graf-after--pre">mod = loadModel()<br>print predict(mod, &quot;./0.png&quot;)<br>print predict(mod, &quot;./1.png&quot;)<br>print predict(mod, &quot;./2.png&quot;)<br>print predict(mod, &quot;./3.png&quot;)<br>print predict(mod, &quot;./4.png&quot;)<br>print predict(mod, &quot;./5.png&quot;)<br>print predict(mod, &quot;./6.png&quot;)<br>print predict(mod, &quot;./7.png&quot;)<br>print predict(mod, &quot;./8.png&quot;)<br>print predict(mod, &quot;./9.png&quot;)</pre><p name="1c6a" id="1c6a" class="graf graf--p graf-after--pre">And here are the results.</p><pre name="39d6" id="39d6" class="graf graf--pre graf-after--p">[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]<br>[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]<br>[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]<br>[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]<br>[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]<br>[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]<br>[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]<br>[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]<br>[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]<br>[[ 0.  0.  0.  0.001  0.009  0.  0. 0.002  0. 0.988]]</pre><p name="9aae" id="9aae" class="graf graf--p graf-after--pre">Wow. Hardly any doubt on the first 9 digits (probabilities are <strong class="markup--strong markup--p-strong">99.99+%</strong>). Only my ugly 9 scores lower :)</p><p name="9c3c" id="9c3c" class="graf graf--p graf-after--p">Well, who thought that we’d have so much fun and that we’d cover so much ground using the MNIST dataset? Code and images are available on <a href="https://github.com/juliensimon/aws/tree/master/mxnet/mnist" data-href="https://github.com/juliensimon/aws/tree/master/mxnet/mnist" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a>. Hopefully, this will get you started on building and training networks on your own data.</p><p name="7791" id="7791" class="graf graf--p graf-after--p graf--trailing">That’s it for today. Stay tuned for part 2 where we’ll look at another data set!</p></div></div></section><section name="011d" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="155c" id="155c" class="graf graf--p graf--leading">Next:</p><ul class="postList"><li name="7aa4" id="7aa4" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" data-href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" class="markup--anchor markup--li-anchor" target="_blank">Part 2</a> : Training on CIFAR-10</li><li name="1a83" id="1a83" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" data-href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" class="markup--anchor markup--li-anchor" target="_blank">Part 3</a> : CIFAR-10 redux</li><li name="d482" id="d482" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" data-href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" class="markup--anchor markup--li-anchor" target="_blank">Part 4</a>: Distributed training</li><li name="c42d" id="c42d" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" data-href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" class="markup--anchor markup--li-anchor" target="_blank">Part 5</a>: Distributed training, EFS edition</li></ul><figure name="f741" id="f741" class="graf graf--figure graf--iframe graf-after--li"><iframe src="https://upscri.be/3e59ef?as_embed=true" width="700" height="350" frameborder="0" scrolling="no"></iframe></figure></div><div class="section-inner sectionLayout--outsetColumn"><figure name="9159" id="9159" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure"><img class="graf-image" data-image-id="1*bQlRSzFHJEmF4Q7PyrLgng.gif" data-width="725" data-height="71" src="https://cdn-images-1.medium.com/max/1200/1*bQlRSzFHJEmF4Q7PyrLgng.gif"></figure></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="3"><figure name="76a5" id="76a5" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 33.333%;"><a href="https://chatbotslife.com/bot-communities-mastermind-group-d2dae9876709#.53x0py6ou" data-href="https://chatbotslife.com/bot-communities-mastermind-group-d2dae9876709#.53x0py6ou" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*6XUspT9JOSq0w0Fi35HIaA.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*6XUspT9JOSq0w0Fi35HIaA.png"></a></figure><figure name="88b8" id="88b8" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.333%;"><a href="https://m.me/ChatbotsLife" data-href="https://m.me/ChatbotsLife" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*c1LDMH5vbnIz9rmAka8Hwg.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*c1LDMH5vbnIz9rmAka8Hwg.png"></a></figure><figure name="358a" id="358a" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure graf--trailing" style="width: 33.333%;"><a href="https://chatbotslife.com/how-to-get-a-free-chatbot-b1fb9dfe109#.z9dtp2sy0" data-href="https://chatbotslife.com/how-to-get-a-free-chatbot-b1fb9dfe109#.z9dtp2sy0" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*D0Jf3dI6ZThtqcfwDYY7mg.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*D0Jf3dI6ZThtqcfwDYY7mg.png"></a></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/6f0dc4210c62"><time class="dt-published" datetime="2017-04-18T17:46:34.758Z">April 18, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>