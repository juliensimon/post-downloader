<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Johnny Pi, I am your father — part 7: son, we need to talk</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Johnny Pi, I am your father — part 7: son, we need to talk</h1>
</header>
<section data-field="subtitle" class="p-summary">
Previously, we used an AWS IoT button to trigger object detection on our robot. However, this is a pretty poor way to interact. Wouldn’t it…
</section>
<section data-field="body" class="e-content">
<section name="33a0" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f294" id="f294" class="graf graf--h3 graf--leading graf--title">Johnny Pi, I am your father — part 7: son, we need to talk</h3><p name="e96d" id="e96d" class="graf graf--p graf-after--h3"><a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74" class="markup--anchor markup--p-anchor" target="_blank">Previously</a>, we used an <a href="https://aws.amazon.com/iotbutton/" data-href="https://aws.amazon.com/iotbutton/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS IoT button</a> to trigger object detection on our robot. However, this is a pretty poor way to interact. Wouldn’t it be just simpler and more fun to <strong class="markup--strong markup--p-strong">speak</strong> to it? Of course!</p><p name="15f9" id="15f9" class="graf graf--p graf-after--p">In this post, I’ll walk you through an <a href="https://developer.amazon.com/alexa-skills-kit" data-href="https://developer.amazon.com/alexa-skills-kit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Alexa skill</a> that I wrote to send voice commands to the robot.</p><figure name="008c" id="008c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dpqtvigAooKCJkBp8FTbYg.png" data-width="1490" data-height="772" src="https://cdn-images-1.medium.com/max/800/1*dpqtvigAooKCJkBp8FTbYg.png"></figure><h4 name="4042" id="4042" class="graf graf--h4 graf-after--figure">Why not Alexa on the Pi?</h4><p name="03e9" id="03e9" class="graf graf--p graf-after--h4">You can definitely set up Alexa on the Pi itself: the <a href="https://github.com/alexa-pi/AlexaPi" data-href="https://github.com/alexa-pi/AlexaPi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">AlexaPi</strong></a> project lets you connect a Pi to the <a href="https://developer.amazon.com/alexa-voice-service" data-href="https://developer.amazon.com/alexa-voice-service" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Alexa Voice Service</a> (AVS) and process voice commands, provided that you connected a microphone on one of the USB ports (duh).</p><p name="c2bf" id="c2bf" class="graf graf--p graf-after--p">The setup process is a little hairy, but I did get things to work. Here’s a quick demo.</p><figure name="7611" id="7611" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/nPc1PiG72GY?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="3302" id="3302" class="graf graf--p graf-after--figure">However, the overall results were pretty poor because my cheap USB microphone is absolute crap!</p><blockquote name="2763" id="2763" class="graf graf--blockquote graf-after--p">As a side note, this made me appreciate how amazingly good the Amazon Echo microphones are.</blockquote><p name="1c39" id="1c39" class="graf graf--p graf-after--blockquote">Anyway, even with a decent mike, the robot obviously needs to be close enough to hear you… and I wanted to be able to talk to a remote robot as well :) For these reasons, I decided not to go the AlexaPi way. Instead, I opted for an Amazon Echo device with a custom skill.</p><h4 name="cf99" id="cf99" class="graf graf--h4 graf-after--p">Designing the skill</h4><p name="6d81" id="6d81" class="graf graf--p graf-after--h4">As you probably know, an <strong class="markup--strong markup--p-strong">Alexa skill</strong> requires two components:</p><ul class="postList"><li name="7c0a" id="7c0a" class="graf graf--li graf-after--p">An <a href="https://developer.amazon.com/docs/custom-skills/custom-interaction-model-reference.html" data-href="https://developer.amazon.com/docs/custom-skills/custom-interaction-model-reference.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">interaction model</strong></a>, where you will focus on the conversation itself (utterances, intents, slots).</li><li name="6825" id="6825" class="graf graf--li graf-after--li">An <a href="https://aws.amazon.com/lambda/" data-href="https://aws.amazon.com/lambda/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">AWS Lambda</strong></a> function, which will be invoked during the conversation to maintain the session, perform input validation, trigger actions, etc.</li></ul><p name="e394" id="e394" class="graf graf--p graf-after--li">Here, the interaction model is pretty simple. I’ll either ask the robot to <strong class="markup--strong markup--p-strong">move around</strong> or to <strong class="markup--strong markup--p-strong">look at objects or faces</strong>. Thus, we’ll need to define two <strong class="markup--strong markup--p-strong">intents</strong>: <strong class="markup--strong markup--p-strong">DirectionIntent</strong> and <strong class="markup--strong markup--p-strong">SeeIntent</strong>.</p><p name="31e2" id="31e2" class="graf graf--p graf-after--p">As far as the Lambda function is concerned, it will use <a href="https://aws.amazon.com/iot" data-href="https://aws.amazon.com/iot" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS IoT</a> to send the proper MQTT messages to the robot, just like we’ve done with the joystick and the IoT button. It worked well so far and if it ain’t broken, I sure ain’t gonna try to fix it :*)</p><h4 name="5857" id="5857" class="graf graf--h4 graf-after--p">Interaction model: defining custom slots</h4><p name="b9cb" id="b9cb" class="graf graf--p graf-after--h4">The movement commands we’d like to send the robot are what you’d expect: right, left, forward, backward, hold, slower, faster. Let’s define a <strong class="markup--strong markup--p-strong">custom slot</strong>, called <strong class="markup--strong markup--p-strong">{Direction}</strong>, with all these values.</p><p name="4a6d" id="4a6d" class="graf graf--p graf-after--p">We’ll also ask the robot to look at objects and faces. For this purpose, let’s create a second <strong class="markup--strong markup--p-strong">custom slot</strong>, called <strong class="markup--strong markup--p-strong">{Target}</strong>, holding two possible values: ‘object’ and ‘faces’.</p><figure name="c1b1" id="c1b1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UcpZ4dDlgYM1x_OAKtOEig.png" data-width="1802" data-height="374" src="https://cdn-images-1.medium.com/max/800/1*UcpZ4dDlgYM1x_OAKtOEig.png"></figure><p name="4b85" id="4b85" class="graf graf--p graf-after--figure">Once you’re done, this is how your intent schema should look.</p><figure name="ded0" id="ded0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/33e94bb0a7a18bfb669662e09f3cf8db.js"></script></figure><h4 name="7f04" id="7f04" class="graf graf--h4 graf-after--figure">Interaction model: defining utterances for DirectionIntent</h4><p name="37e8" id="37e8" class="graf graf--p graf-after--h4">Then, we need to come up with a number of different <strong class="markup--strong markup--p-strong">utterances</strong> that we’re likely to use. Here are some examples:</p><pre name="3204" id="3204" class="graf graf--pre graf-after--p">DirectionIntent move {Direction}<br>DirectionIntent go {Direction}<br>DirectionIntent turn {Direction}<br>DirectionIntent now move {Direction}<br>DirectionIntent now go {Direction}<br>DirectionIntent now turn {Direction}<br>DirectionIntent just go {Direction}<br>DirectionIntent just move {Direction}<br>DirectionIntent just turn {Direction}<br>DirectionIntent I want you to go {Direction}<br>DirectionIntent I want you to move {Direction}<br>DirectionIntent I want you to turn {Direction}</pre><p name="e3cf" id="e3cf" class="graf graf--p graf-after--pre">Some combinations do sound a little weird, such as “just turn forward”, but nothing that would really require us to create different intents for left/right and forward/backward.</p><h4 name="b091" id="b091" class="graf graf--h4 graf-after--p">Interaction model: defining utterances for SeeIntent</h4><p name="7c63" id="7c63" class="graf graf--p graf-after--h4">Again, we have to create a <strong class="markup--strong markup--p-strong">custom slot</strong>, called <strong class="markup--strong markup--p-strong">{Target}</strong>, holding two possible values: ‘object’ and ‘faces’. Here are some of the utterances:</p><pre name="86b9" id="86b9" class="graf graf--pre graf-after--p">SeeIntent Look at the {Target}<br>SeeIntent Take a look at the {Target}<br>SeeIntent Just look at the {Target}<br>SeeIntent Tell me about the {Target} you see<br>SeeIntent Tell me about the {Target} in front of you<br>SeeIntent What is the {Target} in front of you<br>SeeIntent Do you see the {Target} in front of you<br>SeeIntent Do you see an {Target}<br>SeeIntent Do you see {Target}<br>SeeIntent How many {Target} do you see<br>SeeIntent Describe the {Target} you see</pre><p name="f0f8" id="f0f8" class="graf graf--p graf-after--pre">I’m sure you’ll have additional ideas, just add them to your list.</p><h4 name="60f3" id="60f3" class="graf graf--h4 graf-after--p">Implementing the Lambda function</h4><p name="fefb" id="fefb" class="graf graf--p graf-after--h4">We’re finished with the interaction model. Let’s now take care of the <strong class="markup--strong markup--p-strong">Lambda function</strong> which will perform the actual processing.</p><p name="cd15" id="cd15" class="graf graf--p graf-after--p">Starting from a vanilla skill and customizing is the simplest way to get this done. Thus, I will only highlight the parts that are specific to this skill.</p><h4 name="d620" id="d620" class="graf graf--h4 graf-after--p">Lambda function: connecting to AWS IoT</h4><p name="498d" id="498d" class="graf graf--p graf-after--h4">First, we need to create AWS <strong class="markup--strong markup--p-strong">IoT credentials</strong> for the skill (certificate, key pair, IAM policy). We’ve done this many times before and <a href="https://docs.aws.amazon.com/iot/latest/developerguide/iot-gs.html" data-href="https://docs.aws.amazon.com/iot/latest/developerguide/iot-gs.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the process</a> is always the same: just repeat the same steps and download all credentials on your local machine.</p><p name="e0df" id="e0df" class="graf graf--p graf-after--p">Here’s the <strong class="markup--strong markup--p-strong">IAM policy</strong> you should attach to the skill thing in AWS IoT and to the Lambda function itself.</p><figure name="df13" id="df13" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/5e6eb17ca2884a02e473d2856f1f7e3a.js"></script></figure><p name="1431" id="1431" class="graf graf--p graf-after--figure">Time to write the function itself. First, we have import the <strong class="markup--strong markup--p-strong">AWS IoT SDK</strong> and the <strong class="markup--strong markup--p-strong">IoT credentials</strong>. Let’s also define a couple of helper functions.</p><figure name="1da6" id="1da6" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/0a611aebf587a75d162d5c1f693b497d.js"></script></figure><h4 name="5d39" id="5d39" class="graf graf--h4 graf-after--figure">Lambda function: defining messages</h4><p name="6c56" id="6c56" class="graf graf--p graf-after--h4">This is pretty easy :) Just change the messages in the vanilla skill functions.</p><figure name="a824" id="a824" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7e88b5ba7625dd266521e798c9e613ab.js"></script></figure><h4 name="8852" id="8852" class="graf graf--h4 graf-after--figure">Lambda function: handling DirectionIntent</h4><p name="4ed7" id="4ed7" class="graf graf--p graf-after--h4">Nothing really complicated here:</p><ul class="postList"><li name="ae1a" id="ae1a" class="graf graf--li graf-after--p">Check that the <strong class="markup--strong markup--li-strong">Direction</strong> slot is present and valid,</li><li name="a7aa" id="a7aa" class="graf graf--li graf-after--li">Connect to AWS IoT,</li><li name="c075" id="c075" class="graf graf--li graf-after--li">Publish an <strong class="markup--strong markup--li-strong">MQTT message</strong> containing the direction in the ‘JohnnyPi/move’ topic,</li><li name="b3ea" id="b3ea" class="graf graf--li graf-after--li">Disconnect from AWS IoT.</li></ul><p name="7a76" id="7a76" class="graf graf--p graf-after--li">In this context, I found that it was more reliable to connect and disconnect every time. Not sure why, although the latency is hardly noticeable. Feel free to try something different.</p><p name="dfde" id="dfde" class="graf graf--p graf-after--p">There’s a tiny hack in this code that I was too lazy to fix. We can’t use the word ‘stop’, as it’s reserved by Alexa to stop the skill. That’s why I use ‘hold’ instead: although I’m actually sending ‘stop’ to the robot as this is the command I’ve implemented there ;)</p><figure name="7ef2" id="7ef2" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/283442847e16894b0f05e697bb9031e9.js"></script></figure><h4 name="ac6c" id="ac6c" class="graf graf--h4 graf-after--figure">Lambda function: handling SeeIntent</h4><p name="63b5" id="63b5" class="graf graf--p graf-after--h4">The handler function for SeeIntent has a similar structure, but it is a little more complex. Indeed, we need to handle two different cases:</p><ul class="postList"><li name="855f" id="855f" class="graf graf--li graf-after--p">looking at objects: we’ll perform a call to <a href="https://aws.amazon.com/rekognition/" data-href="https://aws.amazon.com/rekognition/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Amazon Rekognition</strong></a>.</li><li name="e4be" id="e4be" class="graf graf--li graf-after--li">looking at faces: we’ll use a local <a href="http://mxnet.incubator.apache.org/" data-href="http://mxnet.incubator.apache.org/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Apache MXNet</strong></a><strong class="markup--strong markup--li-strong"> model</strong>.</li></ul><p name="5bc2" id="5bc2" class="graf graf--p graf-after--li">In both cases, we’re also instruction the robot to tweet the picture. You can see past pictures <a href="https://twitter.com/@callmejohnnypi" data-href="https://twitter.com/@callmejohnnypi" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><figure name="8366" id="8366" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/f46c57873051d88792b8ae1b2be6de12.js"></script></figure><h4 name="1609" id="1609" class="graf graf--h4 graf-after--figure">Lambda function: dispatching the intents</h4><p name="dab7" id="dab7" class="graf graf--p graf-after--h4">The last bit we need to implement is the <strong class="markup--strong markup--p-strong">intent dispatcher</strong>. Pretty straightforward.</p><figure name="86aa" id="86aa" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/449c2e84461bed9032d7cdeab5ff2ad3.js"></script></figure><p name="669d" id="669d" class="graf graf--p graf-after--figure">We’re now done with the Lambda function. Time to <strong class="markup--strong markup--p-strong">package</strong> and <strong class="markup--strong markup--p-strong">deploy</strong> it.</p><h4 name="25e9" id="25e9" class="graf graf--h4 graf-after--p">Lambda function: packaging and deploying</h4><p name="fd0b" id="fd0b" class="graf graf--p graf-after--h4">Three actions are required:</p><ul class="postList"><li name="d532" id="d532" class="graf graf--li graf-after--p">install the AWS IoT SDK,</li><li name="41cf" id="41cf" class="graf graf--li graf-after--li">create a ZIP file holding the function and the SDK,</li><li name="fdfc" id="fdfc" class="graf graf--li graf-after--li">create the function in AWS Lambda.</li></ul><p name="cb4d" id="cb4d" class="graf graf--p graf-after--li">Here are the corresponding commands, which you need to run in the directory holding the function code.</p><pre name="af3d" id="af3d" class="graf graf--pre graf-after--p">$ pip install  AWSIoTPythonSDK -t .</pre><pre name="62ea" id="62ea" class="graf graf--pre graf-after--pre">$ ls -p<br>AWSIoTPythonSDK/<br>AWSIoTPythonSDK-1.2.0.dist-info/                 <br>certs/                           <br>lambda.py<br>iot_config.py</pre><pre name="95bf" id="95bf" class="graf graf--pre graf-after--pre">$ zip -9 lambda.zip .</pre><pre name="d206" id="d206" class="graf graf--pre graf-after--pre">$ aws lambda create-function --function-name alexaJohnnyPi \<br>--handler lambda.lambda_handler --zip-file fileb://lambda.zip \<br>--runtime python2.7 --memory-size 128 --region REGION_NAME \<br>--role arn:aws:iam::ACCOUNT_NAME:role/ROLE_NAME</pre><h4 name="278e" id="278e" class="graf graf--h4 graf-after--pre">Demo time!</h4><p name="5b6a" id="5b6a" class="graf graf--p graf-after--h4">All right, all the pieces are in place, let’s test them. Here’s a recent video from the AWS Loft in London. My session was an introduction to Deep Learning and the actual demo starts at the 59:30 mark.</p><figure name="4d29" id="4d29" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://player.twitch.tv/?video=v176362879&amp;!branding&amp;autoplay=false" width="500" height="281" frameborder="0" scrolling="no"></iframe></figure><p name="516e" id="516e" class="graf graf--p graf-after--figure">Pretty cool, don’t you think? As usual, you’ll find all code on <a href="https://github.com/juliensimon/johnnypi/tree/master/part7" data-href="https://github.com/juliensimon/johnnypi/tree/master/part7" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a>.</p><p name="3f93" id="3f93" class="graf graf--p graf-after--p">That’s it for today. In the next and possibly last part of this story, we’ll have the robot send text strings back to the Alexa skill instead of speaking through Amazon Polly. Until then, thank you for reading and keep building.</p><p name="b5aa" id="b5aa" class="graf graf--p graf-after--p">Part 0: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36" class="markup--anchor markup--p-anchor" target="_blank">a sneak preview</a></p><p name="4525" id="4525" class="graf graf--p graf-after--p">Part 1: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce" data-href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">moving around</a></p><p name="58a9" id="58a9" class="graf graf--p graf-after--p">Part 2: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86" data-href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">the joystick</a></p><p name="a175" id="a175" class="graf graf--p graf-after--p">Part 3: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76" class="markup--anchor markup--p-anchor" target="_blank">cloud-based speech</a></p><p name="7aa6" id="7aa6" class="graf graf--p graf-after--p">Part 4: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" class="markup--anchor markup--p-anchor" target="_blank">cloud-based vision</a></p><p name="8772" id="8772" class="graf graf--p graf-after--p">Part 5: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" class="markup--anchor markup--p-anchor" target="_blank">local vision</a></p><p name="7ba1" id="7ba1" class="graf graf--p graf-after--p graf--trailing">Part 6: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74" class="markup--anchor markup--p-anchor" target="_blank">the IoT button</a></p></div></div></section><section name="c832" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="66a3" id="66a3" class="graf graf--p graf--leading graf--trailing">This post was written while listening to this <a href="https://open.spotify.com/album/09ZD2k6FieGjzoVGrQdsAC" data-href="https://open.spotify.com/album/09ZD2k6FieGjzoVGrQdsAC" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Gamma Ray best-of</a>. German Power Metal be blessed \m/</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/5a910aa642d1"><time class="dt-published" datetime="2017-11-01T11:29:51.491Z">November 1, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-7-son-we-need-to-talk-5a910aa642d1" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
