<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tumbling down the SGD rabbit hole — part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tumbling down the SGD rabbit hole — part 2</h1>
</header>
<section data-field="subtitle" class="p-summary">
In the first part of this post, we studied the Stochastic Gradient Descent optimizer and discussed five problems that could hamper training…
</section>
<section data-field="body" class="e-content">
<section name="9b58" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3202" id="3202" class="graf graf--h3 graf--leading graf--title">Tumbling down the SGD rabbit hole — part 2</h3><p name="cfae" id="cfae" class="graf graf--p graf-after--h3">In the <a href="http://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7" data-href="http://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7" class="markup--anchor markup--p-anchor" target="_blank">first part</a> of this post, we studied the <strong class="markup--strong markup--p-strong">Stochastic Gradient Descent </strong>optimizer and discussed <strong class="markup--strong markup--p-strong">five problems</strong> that could hamper training of Deep Learning models:</p><p name="437f" id="437f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">1- Local minima,</strong></p><p name="32c6" id="32c6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2- Slow convergence,</strong></p><p name="1247" id="1247" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3- Different slopes,</strong></p><p name="e1fe" id="e1fe" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4- Saddle points,</strong></p><p name="e584" id="e584" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">5- Gradient size &amp; distributed training.</strong></p><p name="614f" id="614f" class="graf graf--p graf-after--p">In this post, we’ll discuss <strong class="markup--strong markup--p-strong">solutions</strong> to these problems and how to apply them.</p><figure name="61be" id="61be" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5Tx6uX4slh3oSxgYZRwmjg.png" data-width="813" data-height="494" src="https://cdn-images-1.medium.com/max/800/1*5Tx6uX4slh3oSxgYZRwmjg.png"><figcaption class="imageCaption">You know you’re having a bad day when the light at the end of the tunnel is actually a black hole…</figcaption></figure><h3 name="8f07" id="8f07" class="graf graf--h3 graf-after--figure">1- Local minima</h3><p name="650b" id="650b" class="graf graf--p graf-after--h3">The problem posed by local minima in deep neural networks has been debated for a long time. However, <strong class="markup--strong markup--p-strong">intuition tells us that they should be rare</strong>. Indeed, the number of parameters in such networks is very large (millions at least): for a point to be a local minimum, all dimensions should have a positive slope. The probability of this happening would be the inverse of 2 to the power of the number of dimension. The inverse of 2^1,000,000? Of 2^10,000,000? Pretty slim chance…</p><p name="2473" id="2473" class="graf graf--p graf-after--p">Based on this, one could also conclude that using larger-than-needed models would actually be a good idea: <strong class="markup--strong markup--p-strong">more parameters would mean lower probability for local minima</strong>, right? Indeed, a 2015 paper by Itay Safran and Ohad Shamir (“<a href="https://arxiv.org/abs/1511.04210" data-href="https://arxiv.org/abs/1511.04210" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">On the Quality of the Initial Basin in Overspecified Neural Networks</em></a>”) shows that this is the case: “<em class="markup--em markup--p-em">higher dimensions also means more potential directions of descent, so perhaps the gradient descent procedures used in practice are more unlikely to get stuck in poor local minima and plateaus</em>”.</p><p name="a25a" id="a25a" class="graf graf--p graf-after--p">Another paper published in 2014 paper by Ian J. Goodfellow, Oriol Vinyals and Andrew M. Sax (“<a href="https://arxiv.org/abs/1412.6544" data-href="https://arxiv.org/abs/1412.6544" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Qualitatively characterizing neural network optimization problems</em></a>”) concludes empirically that local minima are not a problem when training deep neural networks. <strong class="markup--strong markup--p-strong">Yes, they may exist but in practice they’re hardly ever encountered during SGD</strong>. Even when they are, it does just fine escaping them, thank you.</p><p name="f366" id="f366" class="graf graf--p graf-after--p">When all is said and done, please remember one thing: <strong class="markup--strong markup--p-strong">the purpose of the training process is not to find *the* global mimimum — </strong>this<strong class="markup--strong markup--p-strong"> </strong>is a NP-hard problem, so forget about it.<strong class="markup--strong markup--p-strong"> It is to find *a* minimum that generalizes well enough</strong>, yielding a test accuracy compatible with our business problem.</p><figure name="efa4" id="efa4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Y4BFjYTWen-byhNuCacTLQ.png" data-width="800" data-height="450" src="https://cdn-images-1.medium.com/max/800/1*Y4BFjYTWen-byhNuCacTLQ.png"><figcaption class="imageCaption">Stop worrying and learn to love SGD!</figcaption></figure><h3 name="f35d" id="f35d" class="graf graf--h3 graf-after--figure">2 &amp; 3 - Slow convergence and different slopes</h3><p name="7f7c" id="7f7c" class="graf graf--p graf-after--h3">These problems are related: for very high dimension problems such as deep neural networks, it will take a <strong class="markup--strong markup--p-strong">long time</strong> to reach an acceptable minimum if you use a <strong class="markup--strong markup--p-strong">small</strong> <strong class="markup--strong markup--p-strong">fixed learning rate.</strong></p><p name="d170" id="d170" class="graf graf--p graf-after--p">Over the years, a number of improvements were designed to speed up <strong class="markup--strong markup--p-strong">SGD</strong> (Robbins and Monro, 1951). Techniques like <strong class="markup--strong markup--p-strong">Momentum</strong> (Polyak, 1964) and <strong class="markup--strong markup--p-strong">Nesterov Accelerated Gradient</strong> (Nesterov, 1983) were designed to accelerate progress in the direction of steepest descent.</p><p name="aacb" id="aacb" class="graf graf--p graf-after--p">As Deep Learning gained popularity and as networks grow larger, researchers realized that some dimensions had <strong class="markup--strong markup--p-strong">steep slopes</strong> while some exhibited <strong class="markup--strong markup--p-strong">vast plateaus</strong>. Thus, they worked on adapt to these different conditions by not only <strong class="markup--strong markup--p-strong">modifying the learning rate</strong> but also by <strong class="markup--strong markup--p-strong">using different learning rates for different dimensions</strong>.</p><p name="0ae8" id="0ae8" class="graf graf--p graf-after--p">Enter the Ada* family!</p><figure name="5072" id="5072" class="graf graf--figure graf--startsWithDoubleQuote graf-after--p"><img class="graf-image" data-image-id="1*bAGwtu2t8YdYZisZfrVDtA.png" data-width="1024" data-height="581" src="https://cdn-images-1.medium.com/max/800/1*bAGwtu2t8YdYZisZfrVDtA.png"><figcaption class="imageCaption">“We’re a happy family, we’re a happy family, hey Mom and Daddy”</figcaption></figure><h4 name="16ab" id="16ab" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Adaptive optimizers</strong></h4><p name="33f9" id="33f9" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">AdaGrad</strong> (2011, <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" data-href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PDF</a>) came first. For a given parameter, it <strong class="markup--strong markup--p-strong">increases the learning rate if it receives small updates</strong> (i.e. speed up on plateaus) and <strong class="markup--strong markup--p-strong">decrease it if it receives larges updates</strong> (i.e. slow down on steep slopes). This is achieved by dividing the learning rate by the sum of all squared past gradient updates (aka the l2 norm): <strong class="markup--strong markup--p-strong">larger gradient updates will reduce the learning rate while tiny gradient updates will increase it</strong>.</p><p name="898c" id="898c" class="graf graf--p graf-after--p">AdaGrad has a problem, however: as the sum grows monotonically, the learning rates will end up converging to <strong class="markup--strong markup--p-strong">zero</strong> for large models and long trainings, effectively preventing any further progress.</p><p name="3072" id="3072" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">RMSProp</strong> (2012) and <a href="https://arxiv.org/abs/1212.5701" data-href="https://arxiv.org/abs/1212.5701" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">AdaDelta</strong></a> (2012) solve this problem by <strong class="markup--strong markup--p-strong">decaying the accumulated sum</strong> before adding the new gradient. This prevents the sum from exploding and the learning rates from going to zero.</p><p name="17cf" id="17cf" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1412.6980" data-href="https://arxiv.org/abs/1412.6980" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Adam</strong></a> (2015) adds <strong class="markup--strong markup--p-strong">momentum</strong> to AdaDelta, further focusing progress in the direction of steepest descent.</p><p name="0e77" id="0e77" class="graf graf--p graf-after--p">Here’s a great visualization. The adaptive optimizers race to the minimum, momentum and NAG go for a walk in the park before heading out to the right spot… and Grand Pa SGD gets there too but really slowly.</p><figure name="f458" id="f458" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XVFmo9NxLnwDr3SxzKy-rA.gif" data-width="620" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*XVFmo9NxLnwDr3SxzKy-rA.gif"><figcaption class="imageCaption">Source: Alec Radford</figcaption></figure><p name="e787" id="e787" class="graf graf--p graf-after--figure">All these optimizers are available in your favorite Deep Learning library. <strong class="markup--strong markup--p-strong">Adam is a popular choice as it seems to work well in most situations</strong>. Does it mean that it can’t be improved? Of course not: research never sleeps and this nice <a href="http://ruder.io/deep-learning-optimization-2017/" data-href="http://ruder.io/deep-learning-optimization-2017/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">post</a> by Sebastian Ruder lists the latest developments.</p><h4 name="32a5" id="32a5" class="graf graf--h4 graf-after--p">SGD strikes back?</h4><p name="817a" id="817a" class="graf graf--p graf-after--h4">One of these developments is surprising. Some researchers now question that adaptive optimizers are the best choice for deep neural networks, as illustrated by “<a href="https://arxiv.org/abs/1705.08292" data-href="https://arxiv.org/abs/1705.08292" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">The Marginal Value of Adaptive Gradient Methods in Machine Learning</em></a>” (Ashia Wilson et al., 2017).</p><p name="d6f1" id="d6f1" class="graf graf--p graf-after--p">This paper show that <strong class="markup--strong markup--p-strong">well-tuned SGD with learning rate decay at specific epochs ends up outperforming all adaptive optimizers</strong>: “<em class="markup--em markup--p-em">Despite the fact that our experimental evidence demonstrates that adaptive methods are not advantageous for machine learning, the Adam algorithm remains incredibly popular. We are not sure exactly as to why, but hope that our step-size tuning suggestions make it easier for practitioners to use standard stochastic gradient methods in their research</em>”.</p><p name="f7b7" id="f7b7" class="graf graf--p graf-after--p">Now, of course, figuring out the learning rate decay schedule is another complex problem in itself, so I wouldn’t bury Adam just yet :)</p><figure name="4d5d" id="4d5d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ENkgFKdWDoaRKyA956mUew.png" data-width="1280" data-height="720" src="https://cdn-images-1.medium.com/max/800/1*ENkgFKdWDoaRKyA956mUew.png"><figcaption class="imageCaption">And now for something completely different…</figcaption></figure><h4 name="1a03" id="1a03" class="graf graf--h4 graf-after--figure">The FTML optimizer</h4><p name="7602" id="7602" class="graf graf--p graf-after--h4">So far, we’ve only studied SGD and its variants. Of course, there are other techniques out there. One of them is the Follow the Moving Leader algorithm aka <strong class="markup--strong markup--p-strong">FTML</strong> (“<em class="markup--em markup--p-em">Follow the Moving Leader in Deep Learning</em>” by Shuai Zheng and James T. Kwok, 2017, <a href="http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf" data-href="http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PDF</a>).</p><p name="a1cb" id="a1cb" class="graf graf--p graf-after--p">I won’t go into details in this already long post, but let’s take a quick look at some of the results: learning <a href="https://www.cs.toronto.edu/~kriz/cifar.html" data-href="https://www.cs.toronto.edu/~kriz/cifar.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CIFAR-10</a> with <a href="http://yann.lecun.com/exdb/lenet/" data-href="http://yann.lecun.com/exdb/lenet/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LeNet</a> and <a href="https://arxiv.org/abs/1512.03385" data-href="https://arxiv.org/abs/1512.03385" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ResNet-110</a>.</p><figure name="e8fa" id="e8fa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-c5tiHAZJefjiV37-z_-fw.png" data-width="870" data-height="660" src="https://cdn-images-1.medium.com/max/800/1*-c5tiHAZJefjiV37-z_-fw.png"><figcaption class="imageCaption">LeNet CNN training on CIFAR-10</figcaption></figure><figure name="1cc7" id="1cc7" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*PB0WMxNjGMa3frXjEJRzDA.png" data-width="842" data-height="654" src="https://cdn-images-1.medium.com/max/800/1*PB0WMxNjGMa3frXjEJRzDA.png"><figcaption class="imageCaption">ResNet-110 training CIFAR-10</figcaption></figure><p name="977b" id="977b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Is there a new sheriff in Optimizer City?</strong> You’ll find more results in the research paper (tl;dr: FTML matches or surpasses Adam on CIFAR-100 and on LSTM tasks).</p><p name="5211" id="5211" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">FTML is available in Apache MXNet 1.1</strong>. Using it is as simple as:</p><pre name="fcb4" id="fcb4" class="graf graf--pre graf-after--p">mod.init_optimizer(optimizer=’ftml’)</pre><h3 name="ceb9" id="ceb9" class="graf graf--h3 graf-after--pre">4- Saddle points</h3><h4 name="8810" id="8810" class="graf graf--h4 graf-after--h3">Noisy SGD</h4><p name="7f13" id="7f13" class="graf graf--p graf-after--h4">In 2015, Rong Ge et al. proposed a <strong class="markup--strong markup--p-strong">noisy version of SGD</strong> (“<a href="https://arxiv.org/abs/1503.02101" data-href="https://arxiv.org/abs/1503.02101" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Escaping From Saddle Points — Online Stochastic Gradient for Tensor Decomposition</em></a>”): when updating parameters, adding a tiny amount of noise on top of the gradient is enough to nudge SGD to a slightly lower point instead of getting stuck at the saddle point. Very cool idea :)</p><blockquote name="f565" id="f565" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“<a href="http://www.offconvex.org/" data-href="http://www.offconvex.org/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Off the convex path</a>” — Rong Ge’s blog — is an excellent resource. Check it out.</blockquote><h4 name="5287" id="5287" class="graf graf--h4 graf-after--blockquote">Random initialization</h4><p name="6135" id="6135" class="graf graf--p graf-after--h4">In 2016, Lee et al. (“<a href="https://arxiv.org/abs/1602.04915" data-href="https://arxiv.org/abs/1602.04915" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Gradient Descent Converges to Minimizers</em></a>”) showed that even without adding noise, <strong class="markup--strong markup--p-strong">random initialization of parameters lets SGD evade first-order saddle points: “</strong><em class="markup--em markup--p-em">gradient descent with a random initialization and sufficiently small constant step size converges to a local minimizer or negative infinity </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">almost surely</em></strong>”.</p><p name="ded9" id="ded9" class="graf graf--p graf-after--p">All Deep Learning libraries will let you do this. For example, you could use either of these in Apache MXNet:</p><pre name="a3b5" id="a3b5" class="graf graf--pre graf-after--p">mod.init_params(initializer=mx.init.Normal())<br>mod.init_params(initializer=mx.init.Xavier())</pre><h4 name="6bfe" id="6bfe" class="graf graf--h4 graf-after--pre">Breaking out of high-order saddle points</h4><p name="f1e2" id="f1e2" class="graf graf--p graf-after--h4">Noisy SGD and random initialization save us from computing the Hessian, which is a costly operation slowing down the training process. Still, in the previous post, I gave you an example of a third-order saddle point defeating the Hessian. Is there hope for these?</p><p name="90cd" id="90cd" class="graf graf--p graf-after--p">In 2016, <a href="https://twitter.com/animaanandkumar" data-href="https://twitter.com/animaanandkumar" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Anima Anandkumar</a> (a Principal Scientist at AWS) and Rong Ge proposed <strong class="markup--strong markup--p-strong">the first method to solve third-order saddle points</strong>: “<a href="https://arxiv.org/abs/1602.05908" data-href="https://arxiv.org/abs/1602.05908" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Efficient approaches for escaping higher order saddle points in non-convex optimization</em></a>”. I’m not aware of any Deep Learning library implementing this technique, but please feel free to get in touch if there’s one :)</p><h3 name="ff67" id="ff67" class="graf graf--h3 graf-after--p">5- Gradient size &amp; distributed training</h3><p name="69ce" id="69ce" class="graf graf--p graf-after--h3">Last month, Jeremy Bernstein et al. proposed two variant of SGD called SignSGD and Signum (adding momentum) : “<a href="https://arxiv.org/abs/1802.04434" data-href="https://arxiv.org/abs/1802.04434" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">SIGNSGD: compressed optimisation for non-convex problems</em></a>”. They solve the gradient size problem by <strong class="markup--strong markup--p-strong">sending only the sign of the gradient, not its 32-bit value</strong>! Thus, this reduces the amount of data to send by a factor of 32. The network says thanks ;)</p><p name="da3d" id="da3d" class="graf graf--p graf-after--p">Amazingly, these algorithms converge at the same rate or even faster than Adam, only losing out to highly-tuned SGD (hmmm, again). Here are some results:</p><figure name="6dcf" id="6dcf" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1EkPtoC1Ko38CMPyokdCAg.png" data-width="626" data-height="266" src="https://cdn-images-1.medium.com/max/800/1*1EkPtoC1Ko38CMPyokdCAg.png"><figcaption class="imageCaption">Learning ImageNet with Signum</figcaption></figure><p name="79ab" id="79ab" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">SignSGD is available in Apache MXNet 1.1</strong>. Using it is as simple as:</p><pre name="7b86" id="7b86" class="graf graf--pre graf-after--p">mod.init_optimizer(optimizer=’signum’)</pre><blockquote name="2984" id="2984" class="graf graf--blockquote graf--hasDropCapModel graf-after--pre"><em class="markup--em markup--blockquote-em">Another interesting paper was published in late 2017 by Yujun Lin et al. : “</em><a href="https://arxiv.org/abs/1712.01887" data-href="https://arxiv.org/abs/1712.01887" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Deep Gradient Compression: reducing the communication bandwidth for distributed training</a><em class="markup--em markup--blockquote-em">”. This is based on a complete different technique: gradients are only sent when updates reach a certain threshold value. Good read.</em></blockquote><h3 name="e66e" id="e66e" class="graf graf--h3 graf-after--blockquote">Conclusion</h3><p name="5c6a" id="5c6a" class="graf graf--p graf-after--h3">We covered a lot of ground again. Let’s try to sum things up.</p><ul class="postList"><li name="9104" id="9104" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Local minima </strong>are not a problem in very large networks. A word of warning: larger networks will be more expensive to train and will overfit the training set faster.</li><li name="7d5f" id="7d5f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Adaptive optimizers</strong> solve the headache of picking a fixed learning and they converge faster.</li><li name="9d1b" id="9d1b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Adam</strong> is a popular choice but <strong class="markup--strong markup--li-strong">FTML</strong> is the new kid on the block and it means business. Add it to your lineup.</li><li name="7e25" id="7e25" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">SGD</strong> can still deliver the best accuracy, at the expense of a lot of complicated tuning. Hopefully hyper-parameter optimization (available in preview in <a href="http://aws.amazon.com/sagemaker" data-href="http://aws.amazon.com/sagemaker" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker</a>) will solve this for us.</li><li name="190b" id="190b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Saddle points</strong> is the real problem we didn’t know we had :-/</li><li name="d4c4" id="d4c4" class="graf graf--li graf-after--li">Combining <strong class="markup--strong markup--li-strong">random weight initialization</strong> and a <strong class="markup--strong markup--li-strong">very small learning rate</strong> is a proven technique to avoid first-order saddle points.</li><li name="276a" id="276a" class="graf graf--li graf-after--li">Modern optimizers should be able to break out of <strong class="markup--strong markup--li-strong">first-order and second-order saddle points</strong>. It doesn’t look like we have an off-the-shelf solution for higher-order ones at the moment.</li><li name="973d" id="973d" class="graf graf--li graf-after--li">Sharing gradients during distributed training can become a severe performance bottleneck: the <strong class="markup--strong markup--li-strong">SignSGD</strong> optimizer solves the issue with no loss of accuracy.</li></ul><p name="428e" id="428e" class="graf graf--p graf-after--li">That’s it for today. I hope that you learned a lot and that these techniques will help you build better models.</p><p name="d77a" id="d77a" class="graf graf--p graf-after--p graf--trailing">As always, thanks for reading! Please feel free to reach out on <a href="https://twitter.com/julsimon/" data-href="https://twitter.com/julsimon/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="82f7" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="8bd4" id="8bd4" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">This should be my intro tape at AWS events :D</em></p><figure name="1bca" id="1bca" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/Sp3zaeOyL7Q?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/bed3be4761d3"><time class="dt-published" datetime="2018-03-17T08:16:56.138Z">March 17, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-2-bed3be4761d3" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
