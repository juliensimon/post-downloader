<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>An introduction to the MXNet API — part 5</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">An introduction to the MXNet API — part 5</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 4, we saw how easy it was to use a pre-trained version of the Inception v3 model for object detection. In this article, we’re going…
</section>
<section data-field="body" class="e-content">
<section name="3993" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c86e" id="c86e" class="graf graf--h3 graf--leading graf--title">An introduction to the MXNet API — part 5</h3><p name="4e74" id="4e74" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" class="markup--anchor markup--p-anchor" target="_blank">part 4</a>, we saw how easy it was to use a pre-trained version of the Inception v3 model for object detection. In this article, we’re going to load two other famous <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" data-href="https://en.wikipedia.org/wiki/Convolutional_neural_network" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Convolutional Neural Networks</a> (VGG19 and ResNet-152) and we’ll compare them to Inception v3.</p><figure name="09d1" id="09d1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*V_lnH58ZtxQyxXCqHYt6DQ.png" data-width="560" data-height="253" src="https://cdn-images-1.medium.com/max/800/1*V_lnH58ZtxQyxXCqHYt6DQ.png"><figcaption class="imageCaption">Architecture of a CNN (Source: Nvidia)</figcaption></figure><h4 name="58b9" id="58b9" class="graf graf--h4 graf-after--figure">VGG16</h4><p name="599c" id="599c" class="graf graf--p graf-after--h4">Published in 2014, VGG16 is a model built from <strong class="markup--strong markup--p-strong">16 layers</strong> (<a href="https://arxiv.org/abs/1409.1556" data-href="https://arxiv.org/abs/1409.1556" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research paper</a>). It won the <a href="http://image-net.org/challenges/LSVRC/2014/" data-href="http://image-net.org/challenges/LSVRC/2014/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2014 ImageNet challenge</a> by achieving a <strong class="markup--strong markup--p-strong">7.4%</strong> error rate on object classification.</p><h4 name="1159" id="1159" class="graf graf--h4 graf-after--p">ResNet-152</h4><p name="4a3d" id="4a3d" class="graf graf--p graf-after--h4">Published in 2015, ResNet-152 is a model built from <strong class="markup--strong markup--p-strong">152 layers</strong> (<a href="https://arxiv.org/abs/1512.03385" data-href="https://arxiv.org/abs/1512.03385" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research paper</a>). It won the <a href="http://image-net.org/challenges/LSVRC/2015/" data-href="http://image-net.org/challenges/LSVRC/2015/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2015 ImageNet challenge</a> by achieving a record <strong class="markup--strong markup--p-strong">3.57%</strong> error rate on object detection. That’s much better than the typical human error rate which is usually measured at 5%.</p><h4 name="8cfe" id="8cfe" class="graf graf--h4 graf-after--p">Downloading the models</h4><p name="fd89" id="fd89" class="graf graf--p graf-after--h4">Time to visit the <a href="http://mxnet.io/model_zoo/index.html" data-href="http://mxnet.io/model_zoo/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model zoo</a> once again. Just like for Inception v3, we need to download model definitions and parameters. All three models have been trained on the same categories, so we can reuse our <em class="markup--em markup--p-em">synset.txt</em> file.</p><pre name="7c23" id="7c23" class="graf graf--pre graf-after--p">$ wget <a href="http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json" data-href="http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json" class="markup--anchor markup--pre-anchor" rel="nofollow noopener noopener" target="_blank">http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json</a></pre><pre name="622b" id="622b" class="graf graf--pre graf-after--pre">$ wget <a href="http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params" data-href="http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params" class="markup--anchor markup--pre-anchor" rel="nofollow noopener noopener" target="_blank">http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params</a></pre><pre name="2e43" id="2e43" class="graf graf--pre graf-after--pre">$ wget <a href="http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json" data-href="http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json</a></pre><pre name="e43d" id="e43d" class="graf graf--pre graf-after--pre">$ wget <a href="http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params" data-href="http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params</a></pre><h4 name="b30d" id="b30d" class="graf graf--h4 graf-after--pre">Loading the models</h4><p name="f17d" id="f17d" class="graf graf--p graf-after--h4">All three models have been trained on the <a href="http://www.image-net.org/" data-href="http://www.image-net.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> data set, with a typical image size of 224 x 224. Since data shape and categories are identical, we can <strong class="markup--strong markup--p-strong">reuse</strong> our previous <a href="https://gist.github.com/juliensimon/4a5e999d9c851f0b036ab3870eccd59d#file-mxnet_example2-py" data-href="https://gist.github.com/juliensimon/4a5e999d9c851f0b036ab3870eccd59d#file-mxnet_example2-py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">code</a> as-is.</p><p name="389e" id="389e" class="graf graf--p graf-after--p">All we have to change is the model name :) Let’s just add a parameter to our <em class="markup--em markup--p-em">loadModel</em>() and <em class="markup--em markup--p-em">init</em>() functions.</p><pre name="ca2e" id="ca2e" class="graf graf--pre graf-after--p">def loadModel(modelname):<br>        sym, arg_params, aux_params = mx.model.load_checkpoint(modelname, 0)<br>        mod = mx.mod.Module(symbol=sym)<br>        mod.bind(for_training=False, data_shapes=[(&#39;data&#39;, (1,3,224,224))])<br>        mod.set_params(arg_params, aux_params)<br>        return mod</pre><pre name="5edf" id="5edf" class="graf graf--pre graf-after--pre">def init(modelname):<br>        model = loadModel(modelname)<br>        cats = loadCategories()<br>        return model, cats</pre><h4 name="0f3d" id="0f3d" class="graf graf--h4 graf-after--pre">Comparing predictions</h4><p name="730d" id="730d" class="graf graf--p graf-after--h4">Let’s compare these models on a couple of images.</p><figure name="a980" id="a980" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KcUFMWb7KKTvOd_fgrXydA.jpeg" data-width="300" data-height="225" src="https://cdn-images-1.medium.com/max/800/1*KcUFMWb7KKTvOd_fgrXydA.jpeg"></figure><pre name="626c" id="626c" class="graf graf--pre graf-after--figure">*** VGG16<br>[(0.58786136, &#39;n03272010 electric guitar&#39;), (0.29260877, &#39;n04296562 stage&#39;), (0.013744719, &#39;n04487394 trombone&#39;), (0.013494448, &#39;n04141076 sax, saxophone&#39;), (0.00988709, &#39;n02231487 walking stick, walkingstick, stick insect&#39;)]</pre><p name="b07a" id="b07a" class="graf graf--p graf-after--pre">Good job on the top two categories, but the other three are wildly wrong. Looks like the vertical shape of the microphone stand confused the model.</p><pre name="f35b" id="f35b" class="graf graf--pre graf-after--p">*** ResNet-152<br>[(0.91063803, &#39;n04296562 stage&#39;), (0.039011702, &#39;n03272010 electric guitar&#39;), (0.031426914, &#39;n03759954 microphone, mike&#39;), (0.011822623, &#39;n04286575 spotlight, spot&#39;), (0.0020199812, &#39;n02676566 acoustic guitar&#39;)]</pre><p name="8728" id="8728" class="graf graf--p graf-after--pre">Very high on the top category. The other four are all meaningful.</p><pre name="196c" id="196c" class="graf graf--pre graf-after--p">*** Inception v3<br>[(0.58039135, &#39;n03272010 electric guitar&#39;), (0.27168664, &#39;n04296562 stage&#39;), (0.090769522, &#39;n04456115 torch&#39;), (0.023762707, &#39;n04286575 spotlight, spot&#39;), (0.0081428187, &#39;n03250847 drumstick&#39;)]</pre><p name="218a" id="218a" class="graf graf--p graf-after--pre">Very similar results to VGG16 for the top two categories. The other three are a mixed bag.</p><p name="88f0" id="88f0" class="graf graf--p graf-after--p">Let’s try another picture.</p><figure name="4a3e" id="4a3e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*03jvGuldTuyRbYCFwwRP7A.jpeg" data-width="300" data-height="169" src="https://cdn-images-1.medium.com/max/800/1*03jvGuldTuyRbYCFwwRP7A.jpeg"></figure><pre name="9542" id="9542" class="graf graf--pre graf-after--figure">*** VGG16<br>[(0.96909302, &#39;n04536866 violin, fiddle&#39;), (0.026661994, &#39;n02992211 cello, violoncello&#39;), (0.0017284016, &#39;n02879718 bow&#39;), (0.00056815811, &#39;n04517823 vacuum, vacuum cleaner&#39;), (0.00024804732, &#39;n04090263 rifle&#39;)]</pre><pre name="c581" id="c581" class="graf graf--pre graf-after--pre">*** ResNet-152<br>[(0.96826887, &#39;n04536866 violin, fiddle&#39;), (0.028052919, &#39;n02992211 cello, violoncello&#39;), (0.0008367821, &#39;n02676566 acoustic guitar&#39;), (0.00070532493, &#39;n02787622 banjo&#39;), (0.00039021231, &#39;n02879718 bow&#39;)]</pre><pre name="97b3" id="97b3" class="graf graf--pre graf-after--pre">*** Inception v3<br>[(0.82023674, &#39;n04536866 violin, fiddle&#39;), (0.15483995, &#39;n02992211 cello, violoncello&#39;), (0.0044540241, &#39;n02676566 acoustic guitar&#39;), (0.0020963412, &#39;n02879718 bow&#39;), (0.0015099624, &#39;n03447721 gong, tam-tam&#39;)]</pre><p name="52e3" id="52e3" class="graf graf--p graf-after--pre">All three models score very high on the top category. One can suppose that the shape of a violin is a very unambiguous pattern for a neural network.</p><p name="6ee4" id="6ee4" class="graf graf--p graf-after--p">Obviously, it’s impossible to draw conclusions from a couple of samples. If you’re looking for a pre-trained model, you should definitely look at the training set, run tests on your own data and make up your mind!</p><h4 name="fa03" id="fa03" class="graf graf--h4 graf-after--p">Comparing technical performance</h4><p name="0ad9" id="0ad9" class="graf graf--p graf-after--h4">You’ll find extensive model benchmarks in research papers such as <a href="https://arxiv.org/abs/1605.07678" data-href="https://arxiv.org/abs/1605.07678" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this one</a>. For developers, the two most important factors will probably be:</p><ul class="postList"><li name="7187" id="7187" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">how much memory</strong> does the model require?</li><li name="4696" id="4696" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">how fast</strong> can it predict?</li></ul><p name="17c0" id="17c0" class="graf graf--p graf-after--li">To answer the first question, we could take an educated guess by looking at the size of the parameters file:</p><ul class="postList"><li name="d71d" id="d71d" class="graf graf--li graf-after--p">VGG16: 528MB (about 140 million parameters)</li><li name="58cd" id="58cd" class="graf graf--li graf-after--li">ResNet-152: 230MB (about 60 million parameters)</li><li name="2999" id="2999" class="graf graf--li graf-after--li">Inception v3: 43MB (about 25 million parameters)</li></ul><p name="a08e" id="a08e" class="graf graf--p graf-after--li">As we can see, the current trend is to use <strong class="markup--strong markup--p-strong">deeper networks</strong> with l<strong class="markup--strong markup--p-strong">ess parameters</strong>. This has a double benefit: <strong class="markup--strong markup--p-strong">faster training time</strong> (since the network has to learn less parameters) and <strong class="markup--strong markup--p-strong">reduced memory usage</strong>.</p><p name="606e" id="606e" class="graf graf--p graf-after--p">The second question is more complex and depends on many parameters such as batch size. Let’s time the prediction call and run our examples again.</p><pre name="673e" id="673e" class="graf graf--pre graf-after--p">t1 = time.time()<br>model.forward(Batch([array]))<br>t2 = time.time()<br>t = 1000*(t2-t1)<br>print(&quot;Predicted in %2.2f millisecond&quot; % t)</pre><p name="d420" id="d420" class="graf graf--p graf-after--pre">This is what we see (values have been averaged over a few calls).</p><pre name="3ba8" id="3ba8" class="graf graf--pre graf-after--p">*** VGG16<br>Predicted in 0.30 millisecond<br>*** ResNet-152<br>Predicted in 0.90 millisecond<br>*** Inception v3<br>Predicted in 0.40 millisecond</pre><p name="d5eb" id="d5eb" class="graf graf--p graf-after--pre">To sum things up (standard disclaimer applies):</p><ul class="postList"><li name="c502" id="c502" class="graf graf--li graf-after--p">ResNet-152 has the best accuracy of all three networks (by far) but it’s also 2–3 times slower.</li><li name="c2fb" id="c2fb" class="graf graf--li graf-after--li">VGG16 is the fastest — due its small number of layers? — but it has the highest memory usage and the worst accuracy.</li><li name="3871" id="3871" class="graf graf--li graf-after--li">Inception v3 is almost as fast, while delivering better accuracy and the most conservative memory usage. This last point makes it a good candidate for constrained environments. More on this in part 6 :)</li></ul><p name="3287" id="3287" class="graf graf--p graf-after--li graf--trailing">That’s it for today! Full code below.</p></div></div></section><section name="c00a" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4b79" id="4b79" class="graf graf--p graf--leading">Next:</p><ul class="postList"><li name="2b71" id="2b71" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--li-anchor" target="_blank">Part 6</a>: Real-time object detection on a Raspberry Pi (and it speaks, too!)</li></ul><figure name="8fc9" id="8fc9" class="graf graf--figure graf--iframe graf-after--li graf--trailing"><script src="https://gist.github.com/juliensimon/ed9ef71ae35ae1d5048dd14bbefc552a.js"></script></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/9e78534096db"><time class="dt-published" datetime="2017-04-15T22:05:09.129Z">April 15, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
