<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Speeding up Apache MXNet with the NNPACK library</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Speeding up Apache MXNet with the NNPACK library</h1>
</header>
<section data-field="subtitle" class="p-summary">
Apache MXNet is an Open Source library helping developers to build, train and re-use Deep Learning networks. In this article, I’ll show you…
</section>
<section data-field="body" class="e-content">
<section name="5025" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="5d25" id="5d25" class="graf graf--h3 graf--leading graf--title">Speeding up Apache MXNet with the NNPACK library</h3><p name="ad39" id="ad39" class="graf graf--p graf-after--h3"><a href="http://mxnet.io" data-href="http://mxnet.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> is an Open Source library helping developers to build, train and re-use Deep Learning networks. In this article, I’ll show you to speed up predictions thanks to the <a href="https://github.com/Maratyszcza/NNPACK" data-href="https://github.com/Maratyszcza/NNPACK" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NNPACK</a> library. Before we dive in, let’s first discuss why we would want to do this.</p><blockquote name="11cf" id="11cf" class="graf graf--blockquote graf--hasDropCapModel graf-after--p"><strong class="markup--strong markup--blockquote-strong">The original post was based on MXNet 0.11. It was updated on April 14th, 2018 and it will now let you successfully build MXNet 1.1 with NNPACK. For more information on why this isn’t as easy as it should be: </strong><a href="https://github.com/Maratyszcza/NNPACK/issues/135" data-href="https://github.com/Maratyszcza/NNPACK/issues/135" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">https://github.com/Maratyszcza/NNPACK/issues/135</a> + <a href="https://github.com/apache/incubator-mxnet/pull/9860" data-href="https://github.com/apache/incubator-mxnet/pull/9860" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">https://github.com/apache/incubator-mxnet/pull/9860</a></blockquote><figure name="5a38" id="5a38" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*XDNyBK6Fk4Pf5Fp6RF1z0w.gif" data-width="652" data-height="267" src="https://cdn-images-1.medium.com/max/800/1*XDNyBK6Fk4Pf5Fp6RF1z0w.gif"><figcaption class="imageCaption">Speed… what else?</figcaption></figure><h4 name="6c4b" id="6c4b" class="graf graf--h4 graf-after--figure">Training</h4><p name="0833" id="0833" class="graf graf--p graf-after--h4">Training is the step where a neural network learns how to correctly predict the right output for each sample in the data set. One batch at a time (typically from 32 to 256 samples), the data set is fed into the network, which then proceeds to minimise total error by adjusting weights (and sometimes hyper parameters) thanks to the <a href="https://en.wikipedia.org/wiki/Backpropagation" data-href="https://en.wikipedia.org/wiki/Backpropagation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">backpropagation</a> algorithm.</p><p name="8df0" id="8df0" class="graf graf--p graf-after--p">Going through the full data set is called an “epoch”: large networks may be trained for hundreds of epochs to reach the highest accuracy possible. This may take days or even weeks, which is where GPUs step in: thanks to their formidable parallel processing power, training time can be significantly cut down, compared to even the most powerful of CPUs.</p><h4 name="db31" id="db31" class="graf graf--h4 graf-after--p">Inference</h4><p name="99be" id="99be" class="graf graf--p graf-after--h4">Inference is the step where you actually use the trained network to predict a result from new data samples. You could be predicting with one sample at a time, for example trying to identify objects in a single picture like <a href="https://aws.amazon.com/rekognition/" data-href="https://aws.amazon.com/rekognition/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon Rekognition</a> does, or with multiple samples, for example trying to track a moving object in a video stream.</p><p name="4f3c" id="4f3c" class="graf graf--p graf-after--p">Of course, GPUs are equally efficient at inference. However, many systems are not able to accommodate them because of cost, power consumption or form factor constraints (just think of embedded systems).</p><blockquote name="6139" id="6139" class="graf graf--blockquote graf-after--p">Being able to run fast CPU-based inference is an important topic.</blockquote><p name="e2f7" id="e2f7" class="graf graf--p graf-after--blockquote">This is where the NNPACK library comes into play, as it will help us speed up CPU inference in Apache MXNet.</p><h4 name="069f" id="069f" class="graf graf--h4 graf-after--p">The NNPACK Library</h4><p name="3a36" id="3a36" class="graf graf--p graf-after--h4">NNPACK is an Open Source library available on <a href="https://github.com/Maratyszcza/NNPACK" data-href="https://github.com/Maratyszcza/NNPACK" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a>. It implements operations like matrix multiplication, convolution and pooling in a highly optimised fashion.</p><blockquote name="2212" id="2212" class="graf graf--blockquote graf-after--p">These operations are at the core of neural networks, in particular of <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" data-href="https://en.wikipedia.org/wiki/Convolutional_neural_network" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Convolution Neural Networks</a> which are predominantly used to detect features in an image.</blockquote><p name="2f25" id="2f25" class="graf graf--p graf-after--blockquote">If you’re curious about the theory and math that help make these operations very fast, please refer to the research papers mentioned by the author in this <a href="https://www.reddit.com/r/MachineLearning/comments/4bswi6/nnpack_acceleration_package_for_neural_networks/" data-href="https://www.reddit.com/r/MachineLearning/comments/4bswi6/nnpack_acceleration_package_for_neural_networks/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Reddit post</a>.</p><p name="9ad8" id="9ad8" class="graf graf--p graf-after--p">NNPACK is available for Linux and MacOS X platforms. It’s optimised for the Intel x64 processor with the AVX2 instruction set, as well as the ARMv7 processor with the NEON instruction set and the ARM v8.</p><p name="05d2" id="05d2" class="graf graf--p graf-after--p">In this post, I will use a c4.8xlarge instance running the <a href="https://aws.amazon.com/marketplace/pp/B06VSPXKDX" data-href="https://aws.amazon.com/marketplace/pp/B06VSPXKDX" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Deep Learning AMI</a>, which already includes some of the dependencies we need. Here’s what we going to do:</p><ul class="postList"><li name="3772" id="3772" class="graf graf--li graf-after--p">Build the NNPACK library from source,</li><li name="d7ba" id="d7ba" class="graf graf--li graf-after--li">Build the cpuinfo library from source,</li><li name="46a6" id="46a6" class="graf graf--li graf-after--li">Build Apache MXNet from source with NNPACK and cpuinfo,</li><li name="242f" id="242f" class="graf graf--li graf-after--li">Run some image classification benchmarks using a variety of networks.</li></ul><p name="14dc" id="14dc" class="graf graf--p graf-after--li">Let’s get to work!</p><h4 name="0430" id="0430" class="graf graf--h4 graf-after--p">Building NNPACK</h4><p name="626c" id="626c" class="graf graf--p graf-after--h4">NNPACK uses the <a href="http://ninja-build.org/" data-href="http://ninja-build.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ninja</a> build tool. Unfortunately, the Ubuntu repository does not host the latest version, so we need to build it from source as well.</p><figure name="4fae" id="4fae" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/767105398d5665ff361197ee6dbdc3c3.js"></script></figure><p name="711d" id="711d" class="graf graf--p graf-after--figure">Now let’s prepare the NNPACK build, as per <a href="https://github.com/Maratyszcza/NNPACK" data-href="https://github.com/Maratyszcza/NNPACK" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">instructions</a>.</p><figure name="f98f" id="f98f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/b048e1058042787ec726f929bf6dd2ea.js"></script></figure><p name="91f7" id="91f7" class="graf graf--p graf-after--figure">Before we actually build, we need to tweak the configuration file a bit. The reason for this is that NNPACK only builds as a static library whereas MXNET builds as a dynamic library: thus, they won’t link properly. The MXNet documentation suggests to use an older version of NNPACK, but there’s another way ;)</p><p name="bc09" id="bc09" class="graf graf--p graf-after--p">We need to edit the build.ninja file and the ‘-fPIC’ flag, in order to build C and C++ files as position-independent code, which is really all we need to link with the MXNet shared library.</p><figure name="d6ac" id="d6ac" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/f790f6de6bc70243e37d5950d2036d9b.js"></script></figure><p name="9224" id="9224" class="graf graf--p graf-after--figure">Now, let’s build NNPACK and run some basic tests.</p><figure name="fc2b" id="fc2b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/73b93853a364528d3ef4747e572b2085.js"></script></figure><p name="64e3" id="64e3" class="graf graf--p graf-after--figure">We’re done with NNPACK: you should see the library in <strong class="markup--strong markup--p-strong">~/NNPACK/lib</strong>.</p><h4 name="e6de" id="e6de" class="graf graf--h4 graf-after--p">Building cpuinfo</h4><p name="6dd1" id="6dd1" class="graf graf--p graf-after--h4">NNPACK relies on this library to accurately detect CPU information. Build instructions are very similar.</p><figure name="82ab" id="82ab" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/634678be884528588895f82c74545570.js"></script></figure><p name="9443" id="9443" class="graf graf--p graf-after--figure">Then, we need to edit build.ninja and update <em class="markup--em markup--p-em">cflags</em> and <em class="markup--em markup--p-em">cxxflags</em> again.</p><figure name="1f68" id="1f68" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/f790f6de6bc70243e37d5950d2036d9b.js"></script></figure><p name="d87a" id="d87a" class="graf graf--p graf-after--figure">Now, let’s build cpuinfo and run some basic tests.</p><figure name="4171" id="4171" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/73b93853a364528d3ef4747e572b2085.js"></script></figure><p name="d615" id="d615" class="graf graf--p graf-after--figure">We’re also done with cpuinfo: you should see the library in <strong class="markup--strong markup--p-strong">~/cpuinfo/lib</strong>.</p><h4 name="ef30" id="ef30" class="graf graf--h4 graf-after--p">Building Apache MXNet with NNPACK and cpuinfo</h4><p name="c1e9" id="c1e9" class="graf graf--p graf-after--h4">First, let’s install dependencies as well as the latest MXNet sources (1.1 at the time of writing). Detailed build instructions are available on the <a href="https://mxnet.incubator.apache.org/get_started/install.html" data-href="https://mxnet.incubator.apache.org/get_started/install.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MXNet website</a>.</p><figure name="64ed" id="64ed" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7722360217dd7afecd9e1db1e09d6a2e.js"></script></figure><p name="a30b" id="a30b" class="graf graf--p graf-after--figure">Now, we need to configure the MXNet build. You should edit the make/config.mk file and set the variables below in order to include NNPACK in the build, as well as the dependencies we installed earlier.</p><figure name="b67a" id="b67a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/a0902ab0babefe598c7a57ad8a1baaf5.js"></script></figure><p name="5068" id="5068" class="graf graf--p graf-after--figure">Now, we’re ready to build MXNet. Our instance has 36 vCPUs, so let’s put them to good use.</p><figure name="90dc" id="90dc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/9a70d08b26b766c3a4217dbbdad14228.js"></script></figure><p name="220e" id="220e" class="graf graf--p graf-after--figure">A few minutes later, the build is complete. Let’s install our new MXNet library and its Python bindings.</p><figure name="60ce" id="60ce" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/4042f8836af3780f11a2794f396b55d2.js"></script></figure><p name="5019" id="5019" class="graf graf--p graf-after--figure">We can quickly check that we have the proper version by importing MXNet in Python.</p><figure name="78c2" id="78c2" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/1be7ffb73e9aebf192df3a244a2b4a18.js"></script></figure><p name="e758" id="e758" class="graf graf--p graf-after--figure">We’re all set. Time to run some benchmarks.</p><h3 name="fe10" id="fe10" class="graf graf--h3 graf-after--p">Benchmarks</h3><p name="6566" id="6566" class="graf graf--p graf-after--h3">Benchmarking with a couple of images isn’t going to give us a reliable view on whether NNPACK makes a difference. Fortunately, the MXNet sources include a benchmarking script which feeds randomly generated images in a variety of batch sizes through the following models: <strong class="markup--strong markup--p-strong">Alexnet, VGG16, Inception-BN, Inception v3, Resnet-50 and Resnet-152</strong>.</p><blockquote name="9764" id="9764" class="graf graf--blockquote graf-after--p">April 14th, 2018: these numbers have not been updated. They’re still the old MXNet 0.11 numbers.</blockquote><p name="1224" id="1224" class="graf graf--p graf-after--blockquote">As nothing is ever simple, we need to fix a line of code in the script. C4 instances don’t have any GPU installed (which is the whole point here) and the script is unable to properly detect that fact. Here’s the modification you need to apply to<em class="markup--em markup--p-em">~/incubator-mxnet/example/image-classification/benchmark_score.py</em>. While we’re at it, let’s add additional batch sizes.</p><figure name="e84c" id="e84c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/1f95775151558a7592cf58d84f9e599b.js"></script></figure><p name="dd4e" id="dd4e" class="graf graf--p graf-after--figure">Time to run some benchmarks. Let’s give 8 threads to NNPACK, which is the largest recommended value.</p><figure name="a181" id="a181" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/3e2c3c54c5763b9b0fcc52e9ee8ad575.js"></script></figure><p name="b89f" id="b89f" class="graf graf--p graf-after--figure">Full results are available <a href="https://gist.github.com/juliensimon/9aa7868e8f0fb98ef5e4a05ea03d1b06" data-href="https://gist.github.com/juliensimon/9aa7868e8f0fb98ef5e4a05ea03d1b06" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>. As a reference, I also ran the same script on an identical instance running the vanilla 0.11.0-rc3 MXNet (full results are available <a href="https://gist.github.com/juliensimon/c3394ca697e5b8692a1956027afd34d1" data-href="https://gist.github.com/juliensimon/c3394ca697e5b8692a1956027afd34d1" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>).</p><figure name="82c8" id="82c8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EFQd3uDy3ZkVDxxFSeZVUQ.png" data-width="1633" data-height="970" src="https://cdn-images-1.medium.com/max/800/1*EFQd3uDy3ZkVDxxFSeZVUQ.png"><figcaption class="imageCaption">Number of images per second vs. batch size</figcaption></figure><p name="0e85" id="0e85" class="graf graf--p graf-after--figure">As we can see on the graph above, <strong class="markup--strong markup--p-strong">NNPACK delivers a significant speedup for Alexnet (up to 2x), VGG (up to 3x) and Inception-BN (almost 2x).</strong></p><p name="16c3" id="16c3" class="graf graf--p graf-after--p">For reasons beyond the scope of this article, NNPACK doesn’t (yet?) deliver any speedup for Inception v3 and Resnet.</p><h4 name="bc4c" id="bc4c" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="580f" id="580f" class="graf graf--p graf-after--h4">When GPU inference is not available, adding NNPACK to Apache MXNet may be an easy option to extract more performance from your network.</p><blockquote name="f973" id="f973" class="graf graf--blockquote graf-after--p">As always, your mileage may vary and you should always run your own tests.</blockquote><p name="de18" id="de18" class="graf graf--p graf-after--blockquote">It would definitely be interesting to run the same benchmark on the upcoming <a href="https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/" data-href="https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">c5 instances</a> (based on the latest Intel Skylake architecture) and on a Raspberry Pi. More articles to be written :)</p><p name="50a6" id="50a6" class="graf graf--p graf-after--p graf--trailing">Thanks for reading!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/7427f367490f"><time class="dt-published" datetime="2017-09-09T10:34:56.700Z">September 9, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-7427f367490f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>