<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>An introduction to the MXNet API — part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">An introduction to the MXNet API — part 2</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 1, we covered some MXNet basics and then discussed the NDArray API (tldr: NDArrays is where we’re going to store data, parameters…
</section>
<section data-field="body" class="e-content">
<section name="03c4" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2fff" id="2fff" class="graf graf--h3 graf--leading graf--title">An introduction to the MXNet API — part 2</h3><p name="7ea0" id="7ea0" class="graf graf--p graf-after--h3"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" class="markup--anchor markup--p-anchor" target="_blank">In part 1</a>, we covered some MXNet basics and then discussed the <em class="markup--em markup--p-em">NDArray</em> API (tldr: <em class="markup--em markup--p-em">NDArrays</em> is where we’re going to store data, parameters, etc).</p><p name="a26c" id="a26c" class="graf graf--p graf-after--p">Now that we’ve got data covered, it’s time to look at how MXNet defines computation steps.</p><h4 name="8a36" id="8a36" class="graf graf--h4 graf-after--p">Computation steps? You mean code, right?</h4><p name="d696" id="d696" class="graf graf--p graf-after--h4">That’s a fair question! Haven’t we all learned that “program = data structures + code”? <em class="markup--em markup--p-em">NDArrays</em> are our data structures, let’s just add code!</p><p name="89e4" id="89e4" class="graf graf--p graf-after--p">Well yes, we could to that. We’d have to define all the steps explicitly and run them sequentially on our data. This is called “<a href="https://en.wikipedia.org/wiki/Imperative_programming" data-href="https://en.wikipedia.org/wiki/Imperative_programming" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">imperative programming</strong></a>” and it’s how Fortran, Pascal, C, C++ and so on work. Nothing wrong with that.</p><p name="eb1c" id="eb1c" class="graf graf--p graf-after--p">However, neural networks are intrinsically <strong class="markup--strong markup--p-strong">parallel</strong> beasts: inside a given layer, all outputs can be computed <strong class="markup--strong markup--p-strong">simultaneously</strong>. Independent layers could also run in parallel. So, in order to get good performance, we’d have to implement parallel processing ourselves using multithreading or something similar. We know how that usually works out. And even if we got the code right, how <strong class="markup--strong markup--p-strong">reusable</strong> would it be if data size or network layout kept changing?</p><p name="6721" id="6721" class="graf graf--p graf-after--p">Fortunately, there is an alternative.</p><figure name="a3bb" id="a3bb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*V0ykrxGre3rGMet1I-eLVA.jpeg" data-width="640" data-height="360" src="https://cdn-images-1.medium.com/max/800/1*V0ykrxGre3rGMet1I-eLVA.jpeg"></figure><h4 name="8a82" id="8a82" class="graf graf--h4 graf-after--figure">Dataflow programming</h4><p name="051b" id="051b" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4">“D<a href="https://en.wikipedia.org/wiki/Dataflow_programming" data-href="https://en.wikipedia.org/wiki/Dataflow_programming" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">ataflow programming</strong></a>” is a flexible way of defining parallel computation, where data flows through a <strong class="markup--strong markup--p-strong">graph</strong>. The graph defines the order of operations, i.e. whether they need to be run sequentially or whether they may be run in parallel. Each operation is a <strong class="markup--strong markup--p-strong">black box</strong>: we only define its input and output, without specifying its actual behaviour.</p><p name="50c0" id="50c0" class="graf graf--p graf-after--p">This might sound like Computer Science mumbo jumbo, but this model is exactly what we need to define neural networks : let input data flow through an ordered sequence of operations called “layers”, with each layer running many instructions in parallel.</p><p name="c687" id="c687" class="graf graf--p graf-after--p">Enough talk. Let’s look at an example. This is how we would define E as (A*B) + (C*D).</p><figure name="96ec" id="96ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*h0M4n_9FPyriCwT-LjE0HQ.png" data-width="300" data-height="284" src="https://cdn-images-1.medium.com/max/800/1*h0M4n_9FPyriCwT-LjE0HQ.png"><figcaption class="imageCaption">E = (A*B) + (C*D)</figcaption></figure><blockquote name="0ffd" id="0ffd" class="graf graf--blockquote graf-after--figure">What A,B,C and D are is irrelevant at this point. They are <strong class="markup--strong markup--blockquote-strong">symbols.</strong></blockquote><p name="37a7" id="37a7" class="graf graf--p graf-after--blockquote">No matter what the inputs are (integers, vectors, matrices, etc.), this graph tells us how to compute the output value — provided that operations “+” and “*” are defined.</p><blockquote name="32a7" id="32a7" class="graf graf--blockquote graf-after--p">This graph also tells us that (A*B) and (C*D) can be computed in <strong class="markup--strong markup--blockquote-strong">parallel</strong>.</blockquote><p name="c0d5" id="c0d5" class="graf graf--p graf-after--blockquote">Of course, MXNet will use this information for optimisation purposes.</p><h4 name="64d1" id="64d1" class="graf graf--h4 graf-after--p">The Symbol API</h4><p name="77a9" id="77a9" class="graf graf--p graf-after--h4">So now we know why these things are called <strong class="markup--strong markup--p-strong">symbols</strong> (not a minor victory!). Let’s see if we can code the example above.</p><pre name="16ce" id="16ce" class="graf graf--pre graf-after--p">&gt;&gt;&gt; import mxnet as mx<br>&gt;&gt;&gt; a = mx.symbol.Variable(&#39;A&#39;)<br>&gt;&gt;&gt; b = mx.symbol.Variable(&#39;B&#39;)<br>&gt;&gt;&gt; c = mx.symbol.Variable(&#39;C&#39;)<br>&gt;&gt;&gt; d = mx.symbol.Variable(&#39;D&#39;)<br>&gt;&gt;&gt; e = (a*b)+(c*d)</pre><p name="98c1" id="98c1" class="graf graf--p graf-after--pre">See? This is perfectly valid code. We can assign a result to e without knowing what a, b, c and d are. Let’s keep going.</p><pre name="1cdf" id="1cdf" class="graf graf--pre graf-after--p">&gt;&gt;&gt; (a,b,c,d)<br>(&lt;Symbol A&gt;, &lt;Symbol B&gt;, &lt;Symbol C&gt;, &lt;Symbol D&gt;)<br>&gt;&gt;&gt; e<br>&lt;Symbol _plus1&gt;<br>&gt;&gt;&gt; type(e)<br>&lt;class &#39;mxnet.symbol.Symbol&#39;&gt;</pre><p name="b3d3" id="b3d3" class="graf graf--p graf-after--pre">a, b, c and d are symbols which we explicitly declared. e is different: it is a symbol as well, but one that is the result of a ‘+’ operation. Let’s try to learn more about e.</p><pre name="d1be" id="d1be" class="graf graf--pre graf-after--p">&gt;&gt;&gt; e.list_arguments()<br>[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]<br>&gt;&gt;&gt; e.list_outputs()<br>[&#39;_plus1_output&#39;]<br>&gt;&gt;&gt; e.get_internals().list_outputs()<br>[&#39;A&#39;, &#39;B&#39;, &#39;_mul0_output&#39;, &#39;C&#39;, &#39;D&#39;, &#39;_mul1_output&#39;, &#39;_plus1_output&#39;]</pre><p name="b606" id="b606" class="graf graf--p graf-after--pre">What this tells us is that:</p><ul class="postList"><li name="acb2" id="acb2" class="graf graf--li graf-after--p">e depends on variables a, b, c and d,</li><li name="eb4f" id="eb4f" class="graf graf--li graf-after--li">the operation that computes e is a sum,</li><li name="4470" id="4470" class="graf graf--li graf-after--li">e is indeed (a*b)+(c*d).</li></ul><p name="2cd8" id="2cd8" class="graf graf--p graf-after--li">Of course, we can do much more with symbols than ‘+’ and ‘*’. Just like for <em class="markup--em markup--p-em">NDArrays</em>, a lot of operations are defined (math, formatting, etc.). You should take some time to explore the <a href="http://mxnet.io/api/python/symbol.html" data-href="http://mxnet.io/api/python/symbol.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">API</a>.</p><p name="b0b8" id="b0b8" class="graf graf--p graf-after--p">So now we know how do define our computing steps. Let’s see how we can apply them to actual data.</p><h4 name="6a7e" id="6a7e" class="graf graf--h4 graf-after--p">Binding <em class="markup--em markup--h4-em">NDArrays</em> and <em class="markup--em markup--h4-em">Symbols</em></h4><blockquote name="8b50" id="8b50" class="graf graf--blockquote graf-after--h4">Applying computing steps defined with Symbols to data stored in NDArrays requires an operation called ‘<strong class="markup--strong markup--blockquote-strong">binding</strong>’, i.e. assigning an NDArray to each input variable of the graph.</blockquote><p name="e465" id="e465" class="graf graf--p graf-after--blockquote">Let’s continue with the example above. Here, I’d like to set ‘A’ to 1, ‘B’ to 2, C to ‘3’ and ‘D’ to 4, which is why I’m creating 4 <em class="markup--em markup--p-em">NDArrays</em> containing a single integer.</p><pre name="edd7" id="edd7" class="graf graf--pre graf-after--p">&gt;&gt;&gt; import numpy as np<br>&gt;&gt;&gt; a_data = mx.nd.array([1], dtype=np.int32)<br>&gt;&gt;&gt; b_data = mx.nd.array([2], dtype=np.int32)<br>&gt;&gt;&gt; c_data = mx.nd.array([3], dtype=np.int32)<br>&gt;&gt;&gt; d_data = mx.nd.array([4], dtype=np.int32)</pre><p name="f7ea" id="f7ea" class="graf graf--p graf-after--pre">Next, I’m binding each <em class="markup--em markup--p-em">NDArray</em> to its corresponding <em class="markup--em markup--p-em">Symbol</em>. Please note that I have to select the <strong class="markup--strong markup--p-strong">context</strong> (CPU or GPU) where execution will take place.</p><pre name="f457" id="f457" class="graf graf--pre graf-after--p">&gt;&gt;&gt; executor=e.bind(mx.cpu(), {&#39;A&#39;:a_data, &#39;B&#39;:b_data, &#39;C&#39;:c_data, &#39;D&#39;:d_data})<br>&gt;&gt;&gt; executor<br>&lt;mxnet.executor.Executor object at 0x10da6ec90&gt;</pre><p name="7522" id="7522" class="graf graf--p graf-after--pre">Now, it’s time to let our input data flow through the graph in order to get a result: the <em class="markup--em markup--p-em">forward</em>() function will get things going. It returns an array of <em class="markup--em markup--p-em">NDArrays</em>, because a graph could have multiple outputs. Here, we have a single output, holding the value ‘14’ — which is reassuringly equal to (1*2)+(3*4).</p><pre name="bf01" id="bf01" class="graf graf--pre graf-after--p">&gt;&gt;&gt; e_data = executor.forward()<br>&gt;&gt;&gt; e_data<br>[&lt;NDArray 1 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>&gt;&gt;&gt; e_data[0]<br>&lt;NDArray 1 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;<br>&gt;&gt;&gt; e_data[0].asnumpy()<br>array([14], dtype=int32)</pre><p name="b4e3" id="b4e3" class="graf graf--p graf-after--pre">Let’s apply the same graph to four 1000 x 1000 matrices filled with random floats between 0 and 1. All we have to do is define new input data: binding and computing are <strong class="markup--strong markup--p-strong">identical</strong>.</p><pre name="9e05" id="9e05" class="graf graf--pre graf-after--p">&gt;&gt;&gt; a_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))<br>&gt;&gt;&gt; b_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))<br>&gt;&gt;&gt; c_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))<br>&gt;&gt;&gt; d_data = mx.nd.uniform(low=0, high=1, shape=(1000,1000))</pre><pre name="3156" id="3156" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; executor=e.bind(mx.cpu(), {&#39;A&#39;:a_data, &#39;B&#39;:b_data, &#39;C&#39;:c_data, &#39;D&#39;:d_data})<br>&gt;&gt;&gt; e_data = executor.forward()</pre><pre name="02e0" id="02e0" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; e_data<br>[&lt;NDArray 1000x1000 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>&gt;&gt;&gt; e_data[0]<br>&lt;NDArray 1000x1000 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;<br>&gt;&gt;&gt; e_data[0].asnumpy()<br>array([[ 0.89252722,  0.46442914,  0.44864511, ...,  0.08874825,<br>         0.83029556,  1.15613985],<br>       [ 0.10265817,  0.22077513,  0.36850023, ...,  0.36564362,<br>         0.98767519,  0.57575727],<br>       [ 0.24852338,  0.6468209 ,  0.25207704, ...,  1.48333383,<br>         0.1183901 ,  0.70523977],<br>       ...,<br>       [ 0.85037285,  0.21420079,  1.21267629, ...,  0.35427764,<br>         0.43418071,  1.12958288],<br>       [ 0.14908466,  0.03095067,  0.19960476, ...,  1.13549757,<br>         0.22000578,  0.16202438],<br>       [ 0.47174677,  0.19318949,  0.05837669, ...,  0.06060726,<br>         1.01848066,  0.48173574]], dtype=float32)</pre><p name="a8e2" id="a8e2" class="graf graf--p graf-after--pre">Pretty cool, isn’t it? This clean separation between data and computation aims at giving us <strong class="markup--strong markup--p-strong">the best of both worlds</strong>:</p><ul class="postList"><li name="6f51" id="6f51" class="graf graf--li graf-after--p">data is loaded and prepared using the <strong class="markup--strong markup--li-strong">imperative</strong> programming model that we’re all very familiar with. We can even use any external library in the process (it’s just good old code!).</li><li name="4fcf" id="4fcf" class="graf graf--li graf-after--li">computation is performed using the <strong class="markup--strong markup--li-strong">symbolic</strong> programming model, which allows MXNet not only to decouple code and data but also to perform parallel execution as well as graph optimisation.</li></ul><p name="0a25" id="0a25" class="graf graf--p graf-after--li graf--trailing">That’s it for today. In the next article, we’ll look at the Module API, the last one we need to cover before we can start training and using neural networks!</p></div></div></section><section name="2f38" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="460b" id="460b" class="graf graf--p graf--leading">Next :</p><ul class="postList"><li name="de6c" id="de6c" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" class="markup--anchor markup--li-anchor" target="_blank">Part 3</a>: the Module API</li><li name="8171" id="8171" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" class="markup--anchor markup--li-anchor" target="_blank">Part 4</a>: Using a pre-trained model for image classification (Inception v3)</li><li name="99aa" id="99aa" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" class="markup--anchor markup--li-anchor" target="_blank">Part 5</a>: More pre-trained models (VGG16 and ResNet-152)</li><li name="39a8" id="39a8" class="graf graf--li graf-after--li graf--trailing"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--li-anchor" target="_blank">Part 6</a>: Real-time object detection on a Raspberry Pi (and it speaks, too!)</li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/ce761513124e"><time class="dt-published" datetime="2017-04-10T22:26:56.239Z">April 10, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-2-ce761513124e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>