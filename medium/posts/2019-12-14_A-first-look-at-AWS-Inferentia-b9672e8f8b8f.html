<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A first look at AWS Inferentia</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A first look at AWS Inferentia</h1>
</header>
<section data-field="subtitle" class="p-summary">
Launched at AWS re:Invent 2019, AWS Inferentia is a high performance machine learning inference chip, custom designed by AWS: its purpose…
</section>
<section data-field="body" class="e-content">
<section name="7819" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="35c5" id="35c5" class="graf graf--h3 graf--leading graf--title">A first look at AWS Inferentia</h3><p name="e762" id="e762" class="graf graf--p graf-after--h3">Launched at AWS re:Invent 2019, <a href="https://aws.amazon.com/machine-learning/inferentia/" data-href="https://aws.amazon.com/machine-learning/inferentia/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS Inferentia</a> is a high performance machine learning inference chip, custom designed by AWS: its purpose is to deliver cost effective, low latency predictions at scale. Inferentia is present in <a href="https://aws.amazon.com/ec2/instance-types/inf1/" data-href="https://aws.amazon.com/ec2/instance-types/inf1/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon EC2 inf1</a> instances, a new family of instances also <a href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/" data-href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">launched</a> at re:Invent.</p><p name="80b4" id="80b4" class="graf graf--p graf-after--p">In this post, I’d like to show you how to get started with Inferentia and <a href="https://www.tensorflow.org" data-href="https://www.tensorflow.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a>. Please note that <a href="https://mxnet.apache.org" data-href="https://mxnet.apache.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a>, <a href="https://pytorch.org" data-href="https://pytorch.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch</a> and <a href="https://onnx.ai" data-href="https://onnx.ai" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ONNX</a> are also supported.</p><h4 name="c835" id="c835" class="graf graf--h4 graf-after--p">A primer on Inferentia</h4><p name="959b" id="959b" class="graf graf--p graf-after--h4">The <a href="https://www.youtube.com/watch?v=17r1EapAxpk" data-href="https://www.youtube.com/watch?v=17r1EapAxpk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CMP324</a> breakout session is a great introduction to Inferentia, and the Alexa use case is a rare look under the hood. It’s well worth your time.</p><figure name="e7e5" id="e7e5" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/17r1EapAxpk?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="3b69" id="3b69" class="graf graf--p graf-after--figure">In a nutshell, each Inferentia chip hosts 4 NeuronCores. Each one of these implements a “high performance <a href="https://en.wikipedia.org/wiki/Systolic_array" data-href="https://en.wikipedia.org/wiki/Systolic_array" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">systolic array</a> matrix multipy engine” (nicely put, Gadi), and is also equipped with a large on-chip cache.</p><figure name="459a" id="459a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IQmXRvF9uIyNlzb-voaIUQ.png" data-width="1306" data-height="662" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*IQmXRvF9uIyNlzb-voaIUQ.png"></figure><p name="ed55" id="ed55" class="graf graf--p graf-after--figure">Chips are interconnected, which makes it possible to:</p><ul class="postList"><li name="c682" id="c682" class="graf graf--li graf-after--p">Partition a model across multiple cores (and Inferentia chips, if several are available), storing it 100% in on-cache memory.</li><li name="97c4" id="97c4" class="graf graf--li graf-after--li">Stream data at full speed through the pipeline of cores, without having to deal with latency caused by external memory access.</li></ul><figure name="6381" id="6381" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*xSIJzmWfkef7cxnBSMpdyQ.png" data-width="1314" data-height="608" src="https://cdn-images-1.medium.com/max/800/1*xSIJzmWfkef7cxnBSMpdyQ.png"></figure><p name="719c" id="719c" class="graf graf--p graf-after--figure">Alternatively, you can run inference with different models on the same Inferentia chip. This is achieved by partitioning NeuronCores into NeuronCore Groups, and by loading different models on different groups.</p><h4 name="dd95" id="dd95" class="graf graf--h4 graf-after--p">The Neuron SDK</h4><p name="03f4" id="03f4" class="graf graf--p graf-after--h4">In order to run on Inferentia, models first need to be compiled to a hardware-optimized representation. Then, they may be loaded, executed and profiled using a specific runtime. These operations can be performed through command-line tools available in the <a href="https://github.com/aws/aws-neuron-sdk" data-href="https://github.com/aws/aws-neuron-sdk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS Neuron SDK</a>, or through framework APIs.</p><div name="8013" id="8013" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/aws/aws-neuron-sdk" data-href="https://github.com/aws/aws-neuron-sdk" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/aws/aws-neuron-sdk"><strong class="markup--strong markup--mixtapeEmbed-strong">aws/aws-neuron-sdk</strong><br><em class="markup--em markup--mixtapeEmbed-em">AWS Neuron Overview Getting started AWS Neuron is a software development kit (SDK) enabling high-performance deep…</em>github.com</a><a href="https://github.com/aws/aws-neuron-sdk" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1c2339412aac7cbb1d63f50156d6a0e8" data-thumbnail-img-id="0*ovQiaC2pBNJgP-t_" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ovQiaC2pBNJgP-t_);"></a></div><p name="4b4b" id="4b4b" class="graf graf--p graf-after--mixtapeEmbed">Let’s get started!</p><h4 name="8182" id="8182" class="graf graf--h4 graf-after--p">Launching an EC2 instance for model compilation</h4><p name="b9cc" id="b9cc" class="graf graf--p graf-after--h4">This first step doesn’t require an <em class="markup--em markup--p-em">inf1</em> instance. In fact, you should use a <strong class="markup--strong markup--p-strong">compute-optimized instance</strong> for fast and cost effective compilation. In order to avoid any software configuration, you should also use the <a href="https://aws.amazon.com/machine-learning/amis/" data-href="https://aws.amazon.com/machine-learning/amis/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Deep Learning AMI</strong></a>, which comes preinstalled with the Neuron SDK and with up to date frameworks.</p><p name="e627" id="e627" class="graf graf--p graf-after--p">At the time of writing, the most recent Deep Learning AMI for Amazon Linux 2 is version 26.0, and it’s identifier is <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=ami-08e68326c36bf3710" data-href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=ami-08e68326c36bf3710" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">ami-08e68326c36bf3710</em></a>.</p><figure name="6324" id="6324" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AWzrZcLnyTz5vCtiJAezJw.png" data-width="2778" data-height="222" src="https://cdn-images-1.medium.com/max/800/1*AWzrZcLnyTz5vCtiJAezJw.png"></figure><p name="9734" id="9734" class="graf graf--p graf-after--figure">Using this AMI, I fire up a <em class="markup--em markup--p-em">c5d.4xlarge</em> instance. No special settings are required: just make sure you allow SSH access in the Security Group.</p><figure name="d9eb" id="d9eb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*94uEjdipHKN2SPasjdDmdQ.png" data-width="2074" data-height="672" src="https://cdn-images-1.medium.com/max/800/1*94uEjdipHKN2SPasjdDmdQ.png"><figcaption class="imageCaption">family, instance name, vCPUs, RAM, storage</figcaption></figure><p name="8c60" id="8c60" class="graf graf--p graf-after--figure">Once the instance is up, I ssh to it, and I’m greeted by the familiar Deep Learning AMI banner, tellling me that <a href="https://docs.conda.io/en/latest/" data-href="https://docs.conda.io/en/latest/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Conda</a> environments are available for TensorFlow and Apache MXNet.</p><pre name="0889" id="0889" class="graf graf--pre graf-after--p">====================================================================<br> __|  __|_  )<br> _|  (     / Deep Learning AMI (Amazon Linux 2) Version 26.0<br>___|\___|___|<br>====================================================================</pre><pre name="0c85" id="0c85" class="graf graf--pre graf-after--pre">Please use one of the following commands to start the required environment with the framework of your choice:<br>for MXNet(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) <br>source activate mxnet_p36<br>for MXNet(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) <br>source activate mxnet_p27<br><strong class="markup--strong markup--pre-strong">for MXNet(+AWS Neuron) with Python3 <br>source activate aws_neuron_mxnet_p36</strong><br>for TensorFlow(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) source activate tensorflow_p36<br>for TensorFlow(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) source activate tensorflow_p27<br><strong class="markup--strong markup--pre-strong">for TensorFlow(+AWS Neuron) with Python3<br>source activate aws_neuron_tensorflow_p36</strong><br>for TensorFlow 2(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) ssource activate tensorflow2_p36<br>for TensorFlow 2(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) ssource activate tensorflow2_p27<br>for PyTorch with Python3 (CUDA 10.1 and Intel MKL) <br>source activate pytorch_p36<br>for PyTorch with Python2 (CUDA 10.1 and Intel MKL)<br>source activate pytorch_p27<br>for Chainer with Python2 (CUDA 10.0 and Intel iDeep)<br>source activate chainer_p27<br>for Chainer with Python3 (CUDA 10.0 and Intel iDeep)<br>source activate chainer_p36<br>for base Python2 (CUDA 10.0)<br>source activate python2<br>for base Python3 (CUDA 10.0) <br>source activate python3</pre><pre name="17d0" id="17d0" class="graf graf--pre graf-after--pre">Official Conda User Guide: <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/" data-href="https://docs.conda.io/projects/conda/en/latest/user-guide/" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://docs.conda.io/projects/conda/en/latest/user-guide/</a><br>AWS Deep Learning AMI Homepage: <a href="https://aws.amazon.com/machine-learning/amis/" data-href="https://aws.amazon.com/machine-learning/amis/" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://aws.amazon.com/machine-learning/amis/</a><br>Developer Guide and Release Notes: <a href="https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html" data-href="https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html</a><br>Support: <a href="https://forums.aws.amazon.com/forum.jspa?forumID=263" data-href="https://forums.aws.amazon.com/forum.jspa?forumID=263" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://forums.aws.amazon.com/forum.jspa?forumID=263</a><br>For a fully managed experience, check out Amazon SageMaker at <a href="https://aws.amazon.com/sagemaker" data-href="https://aws.amazon.com/sagemaker" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://aws.amazon.com/sagemaker</a><br><strong class="markup--strong markup--pre-strong">When using INF1 type instances, please update regularly using the instructions at: </strong><a href="https://github.com/aws/aws-neuron-sdk/tree/master/release-notes" data-href="https://github.com/aws/aws-neuron-sdk/tree/master/release-notes" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--pre-strong">https://github.com/aws/aws-neuron-sdk/tree/master/release-notes</strong></a><br>====================================================================</pre><p name="cffb" id="cffb" class="graf graf--p graf-after--pre">I activate the appropriate environment, which provides all required dependencies.</p><blockquote name="f482" id="f482" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">For the rest of this post, any shell command prefixed by <em class="markup--em markup--blockquote-em">(aws_neuron_tensorflow_p36)</em> should be run inside that Conda environment.</blockquote><pre name="c28e" id="c28e" class="graf graf--pre graf-after--blockquote">$ source activate aws_neuron_tensorflow_p36<br>(aws_neuron_tensorflow_p36) $</pre><p name="8d77" id="8d77" class="graf graf--p graf-after--pre">Next, I upgrade the <em class="markup--em markup--p-em">tensorflow-neuron</em> package.</p><pre name="33af" id="33af" class="graf graf--pre graf-after--p">(aws_neuron_tensorflow_p36) $ conda install numpy=1.17.2 --yes<br>(aws_neuron_tensorflow_p36) $ conda update tensorflow-neuron</pre><p name="3011" id="3011" class="graf graf--p graf-after--pre">We’re now ready to fetch a model and compile it.</p><h4 name="f30f" id="f30f" class="graf graf--h4 graf-after--p">Compiling a model</h4><p name="30d5" id="30d5" class="graf graf--p graf-after--h4">The code below grabs a <a href="https://arxiv.org/abs/1512.03385" data-href="https://arxiv.org/abs/1512.03385" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ResNet50</a> image classification model pretrained on the <a href="http://image-net.org" data-href="http://image-net.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> dataset, and stores it in the <em class="markup--em markup--p-em">resnet50</em> directory.</p><p name="c41b" id="c41b" class="graf graf--p graf-after--p">Then, it compiles it for Inferentia. I highlighted the single line of code required: everything else is vanilla TensorFlow. Then, the compiled model is saved in the <em class="markup--em markup--p-em">ws_resnet50</em> directory, and in a ZIP file for easy copy to an <em class="markup--em markup--p-em">inf1</em> instance.</p><pre name="785e" id="785e" class="graf graf--pre graf-after--p">import os<br>import time<br>import shutil<br>import tensorflow as tf<br><strong class="markup--strong markup--pre-strong">import tensorflow.neuron as tfn</strong><br>import tensorflow.compat.v1.keras as keras<br>from tensorflow.keras.applications.resnet50 import ResNet50<br>from tensorflow.keras.applications.resnet50 import preprocess_input<br><br># Create a workspace<br>WORKSPACE = &#39;./ws_resnet50&#39;<br>os.makedirs(WORKSPACE, exist_ok=True)<br><br># Prepare export directory (old one removed)<br>model_dir = os.path.join(WORKSPACE, &#39;resnet50&#39;)<br>compiled_model_dir = os.path.join(WORKSPACE, &#39;resnet50_neuron&#39;)<br>shutil.rmtree(model_dir, ignore_errors=True)<br>shutil.rmtree(compiled_model_dir, ignore_errors=True)<br><br># Instantiate Keras ResNet50 model<br>keras.backend.set_learning_phase(0)<br>keras.backend.set_image_data_format(&#39;channels_last&#39;)<br><br>model = ResNet50(weights=&#39;imagenet&#39;)<br><br># Export SavedModel<br>tf.saved_model.simple_save(<br>    session            = keras.backend.get_session(),<br>    export_dir         = model_dir,<br>    inputs             = {&#39;input&#39;: model.inputs[0]},<br>    outputs            = {&#39;output&#39;: model.outputs[0]})<br><br><strong class="markup--strong markup--pre-strong"># Compile using Neuron<br>tfn.saved_model.compile(model_dir, compiled_model_dir)    </strong><br><br># Prepare SavedModel for uploading to Inf1 instance<br>shutil.make_archive(&#39;./resnet50_neuron&#39;, &#39;zip&#39;, WORKSPACE, &#39;resnet50_neuron&#39;)</pre><p name="a3d7" id="a3d7" class="graf graf--p graf-after--pre">That one <a href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/tensorflow-neuron/api-compilation-python-api.md" data-href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/tensorflow-neuron/api-compilation-python-api.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">API</a> is all it takes! Impressive.</p><blockquote name="1379" id="1379" class="graf graf--blockquote graf-after--p">Power users will enjoy reading about the CLI compiler, <a href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-cc" data-href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-cc" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">neuron-cc</a>.</blockquote><p name="bc75" id="bc75" class="graf graf--p graf-after--blockquote">Running this code produces the expected output.</p><pre name="f0cb" id="f0cb" class="graf graf--pre graf-after--p">(aws_neuron_tensorflow_p36) $ python compile_resnet.py</pre><pre name="4999" id="4999" class="graf graf--pre graf-after--pre">&lt;output removed&gt;<br>Downloading data from <a href="https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5" data-href="https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5</a><br>102973440/102967424 [==============================] - 2s 0us/step<br>&lt;output removed&gt;<br>INFO:tensorflow:fusing subgraph neuron_op_d6f098c01c780733 with neuron-cc<br>INFO:tensorflow:Number of operations in TensorFlow session: 4638<br>INFO:tensorflow:Number of operations after tf.neuron optimizations: 556<br>INFO:tensorflow:Number of operations placed on Neuron runtime: 554<br>INFO:tensorflow:Successfully converted ./ws_resnet50/resnet50 to ./ws_resnet50/resnet50_neuron</pre><p name="054b" id="054b" class="graf graf--p graf-after--pre">Then, I simply copy the ZIP file to an Amazon S3 bucket, probably the easiest way to share it with <em class="markup--em markup--p-em">inf1</em> instances used for inference.</p><pre name="aba2" id="aba2" class="graf graf--pre graf-after--p">$ ls *.zip<br>resnet50_neuron.zip<br>$ aws s3 mb s3://jsimon-inf1-useast1 <br>$ aws s3 cp resnet50_neuron.zip s3://jsimon-inf1-useast1<br>upload: ./resnet50_neuron.zip to s3://jsimon-inf1-useast1/resnet50_neuron.zip</pre><p name="f164" id="f164" class="graf graf--p graf-after--pre">Alright, let’s fire up one of these babies.</p><h4 name="1187" id="1187" class="graf graf--h4 graf-after--p">Predicting on Inferentia with TensorFlow</h4><p name="d44b" id="d44b" class="graf graf--p graf-after--h4">Using the same AMI as above, I launch an <em class="markup--em markup--p-em">inf1.xlarge</em> instance.</p><figure name="cb7b" id="cb7b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hMip9-P6o7RaPt46Qk9csg.png" data-width="2072" data-height="236" src="https://cdn-images-1.medium.com/max/800/1*hMip9-P6o7RaPt46Qk9csg.png"><figcaption class="imageCaption">family, instance name, vCPUs, RAM, storage</figcaption></figure><p name="19de" id="19de" class="graf graf--p graf-after--figure">Once this instance is up, I ssh to it, and I update the Neuron CLI tools.</p><pre name="94d3" id="94d3" class="graf graf--pre graf-after--p">$ sudo yum install aws-neuron-tools aws-neuron-runtime-base aws-neuron-runtime -y</pre><p name="ba56" id="ba56" class="graf graf--p graf-after--pre">For example, I can view hardware properties using <a href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-tools" data-href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-tools" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">neuron-ls</em></a>.</p><figure name="70bc" id="70bc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nDcXAWoafSbF6Mcc0mi-DQ.png" data-width="1180" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*nDcXAWoafSbF6Mcc0mi-DQ.png"></figure><p name="262c" id="262c" class="graf graf--p graf-after--figure">4 NeuronCores, as expected. The ‘east’ and ‘west’ columns show connections to other Inferentia chips: as this instance only has one, they’re empty.</p><p name="45ba" id="45ba" class="graf graf--p graf-after--p">Next, I retrieve the compiled model from my S3 bucket, and extract it. I also download a test image.</p><pre name="f987" id="f987" class="graf graf--pre graf-after--p">$ aws s3 cp s3://jsimon-inf1-useast1/resnet50_neuron.zip .<br>download: s3://jsimon-inf1-useast1/resnet50_neuron.zip to resnet50_neuron.zip<br>$ unzip resnet50_neuron.zip<br>Archive: resnet50_neuron.zip<br> creating: resnet50_neuron/<br> creating: resnet50_neuron/variables/<br> inflating: resnet50_neuron/saved_model.pb<br>$ curl -O <a href="https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg" data-href="https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg</a></pre><p name="ba0d" id="ba0d" class="graf graf--p graf-after--pre">Using the code below, I load and transform the test image. I then load the compiled model, and use it to classify the image.</p><pre name="aaf7" id="aaf7" class="graf graf--pre graf-after--p">import os<br>import time<br>import numpy as np<br>import tensorflow as tf<br>from tensorflow.keras.preprocessing import image<br>from tensorflow.keras.applications import resnet50<br><br>tf.keras.backend.set_image_data_format(&#39;channels_last&#39;)<br><br># Create input from image<br>img_sgl = image.load_img(&#39;kitten_small.jpg&#39;, target_size=(224, 224))<br>img_arr = image.img_to_array(img_sgl)<br>img_arr2 = np.expand_dims(img_arr, axis=0)<br>img_arr3 = resnet50.preprocess_input(img_arr2)<br><br># Load model<br>COMPILED_MODEL_DIR = &#39;./resnet50_neuron/&#39;<br>predictor_inferentia = tf.contrib.predictor.from_saved_model(COMPILED_MODEL_DIR)<br><br># Run inference<br>model_feed_dict={&#39;input&#39;: img_arr3}<br>infa_rslts = predictor_inferentia(model_feed_dict);<br><br># Display results<br>print(resnet50.decode_predictions(infa_rslts[&quot;output&quot;], top=5)[0])</pre><p name="17e8" id="17e8" class="graf graf--p graf-after--pre">Can you guess how many lines of Inferentia specific code are present here? The answer is <strong class="markup--strong markup--p-strong">zero</strong>. We seamlessly use the <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/predictor" data-href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/predictor" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">tf.contrib.predictor</em></a> API. Woohoo!</p><p name="ebad" id="ebad" class="graf graf--p graf-after--p">Running this code produces the expected output, and we see the top 5 classes for the image.</p><pre name="5a24" id="5a24" class="graf graf--pre graf-after--p">(aws_neuron_tensorflow_p36) $ python infer_resnet50.py</pre><pre name="d7ef" id="d7ef" class="graf graf--pre graf-after--pre">&lt;output removed&gt;<br>[(&#39;n02123045&#39;, &#39;tabby&#39;, 0.6918919), (&#39;n02127052&#39;, &#39;lynx&#39;, 0.12770271), (&#39;n02123159&#39;, &#39;tiger_cat&#39;, 0.08277027), (&#39;n02124075&#39;, &#39;Egyptian_cat&#39;, 0.06418919), (&#39;n02128757&#39;, &#39;snow_leopard&#39;, 0.009290541)]</pre><p name="1454" id="1454" class="graf graf--p graf-after--pre">Now let’s see how we can load a compiled model using <a href="https://www.tensorflow.org/tfx/guide/serving" data-href="https://www.tensorflow.org/tfx/guide/serving" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow Serving,</a> which is a very good option for production deployments.</p><h4 name="6ac4" id="6ac4" class="graf graf--h4 graf-after--p">Predicting on Inferentia with TensorFlow Serving</h4><p name="4a5f" id="4a5f" class="graf graf--p graf-after--h4">First, we need to package the model properly, and move it to a directory reflecting its version. We have only one here, so let’s move the saved model to a directory named ‘1’.</p><pre name="507e" id="507e" class="graf graf--pre graf-after--p">$ pwd<br>/home/ec2-user/resnet50_neuron<br>$ mkdir 1<br>$ mv * 1</pre><p name="e024" id="e024" class="graf graf--p graf-after--pre">First-time users of TensorFlow Serving are often confused by the file layout, so here’s what it should look like. This directory is the one you should pass to TensorFlow Serving using the <em class="markup--em markup--p-em">model_base_path</em> argument.</p><figure name="8b31" id="8b31" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2BvyHsk-yVimJFYgKumYEQ.png" data-width="320" data-height="140" src="https://cdn-images-1.medium.com/max/800/1*2BvyHsk-yVimJFYgKumYEQ.png"></figure><p name="e6dd" id="e6dd" class="graf graf--p graf-after--figure">Now, we can launch TensorFlow Serving, and load the compiled model. Once again, this is vanilla TensorFlow.</p><pre name="384f" id="384f" class="graf graf--pre graf-after--p">(aws_neuron_tensorflow_p36) $ tensorflow_model_server_neuron <br>--model_name=resnet50 <br>--model_base_path=/home/ec2-user/resnet50_neuron <br>--port=8500</pre><pre name="1bdd" id="1bdd" class="graf graf--pre graf-after--pre">2019–12–13 16:16:27.704882: I tensorflow_serving/core/loader_harness.cc:87] <strong class="markup--strong markup--pre-strong">Successfully loaded servable version {name: resnet50 version: 1}</strong><br>2019–12–13 16:16:27.706241: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500</pre><p name="949c" id="949c" class="graf graf--p graf-after--pre">Once TensorFlow Serving is up and running, we can use the script below to load a test image, and send it for prediction. At the risk of repeating myself… this is vanilla TensorFlow :)</p><pre name="72b2" id="72b2" class="graf graf--pre graf-after--p">import numpy as np<br>import grpc<br>import tensorflow as tf<br>from tensorflow.keras.preprocessing import image<br>from tensorflow.keras.applications.resnet50 import preprocess_input<br>from tensorflow.keras.applications.resnet50 import decode_predictions<br>from tensorflow_serving.apis import predict_pb2<br>from tensorflow_serving.apis import prediction_service_pb2_grpc<br><br>if __name__ == &#39;__main__&#39;:<br>    chan = grpc.insecure_channel(&#39;localhost:8500&#39;)<br>    stub = prediction_service_pb2_grpc.PredictionServiceStub(chan)</pre><pre name="d7ad" id="d7ad" class="graf graf--pre graf-after--pre">    img_file=&quot;kitten_small.jpg&quot;      <br>    img = image.load_img(img_file, target_size=(224, 224))<br>    img_array = preprocess_input(image.img_to_array(img)[None, ...])</pre><pre name="7973" id="7973" class="graf graf--pre graf-after--pre">    request = predict_pb2.PredictRequest()<br>    request.model_spec.name = &#39;resnet50&#39;<br>    request.inputs[&#39;input&#39;].CopyFrom(<br>        tf.contrib.util.make_tensor_proto(<br>            img_array, shape=img_array.shape)<br>    )</pre><pre name="0d07" id="0d07" class="graf graf--pre graf-after--pre">    result = stub.Predict(request)<br>    prediction = tf.make_ndarray(result.outputs[&#39;output&#39;])<br>    print(decode_predictions(prediction))</pre><p name="4a1f" id="4a1f" class="graf graf--p graf-after--pre">Running this code produces the expected output, and we see the top 5 classes for the image.</p><pre name="42e6" id="42e6" class="graf graf--pre graf-after--p">(aws_neuron_tensorflow_p36) $ python tfserving_resnet50.py</pre><pre name="f27a" id="f27a" class="graf graf--pre graf-after--pre">&lt;output removed&gt;<br>[[(‘n02123045’, ‘tabby’, 0.6918919), (‘n02127052’, ‘lynx’, 0.12770271), (‘n02123159’, ‘tiger_cat’, 0.08277027), (‘n02124075’, ‘Egyptian_cat’, 0.06418919), (‘n02128757’, ‘snow_leopard’, 0.009290541)]]</pre><p name="9cd2" id="9cd2" class="graf graf--p graf-after--pre">I recorded this demo, and it’s available on YouTube.</p><figure name="b370" id="b370" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/ifOR6CEINLo?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><h4 name="ecb6" id="ecb6" class="graf graf--h4 graf-after--figure">Diving deeper</h4><p name="9a67" id="9a67" class="graf graf--p graf-after--h4">That’s it for today. I hope I gave you a clear introduction to AWS Inferentia, and how easy it is to use it! All it took is <strong class="markup--strong markup--p-strong">one line of code</strong> to compile our model.</p><p name="4182" id="4182" class="graf graf--p graf-after--p">If you’d like to dive deeper, I highly recommend the excellent <a href="https://github.com/awshlabs/reinvent19Inf1Lab" data-href="https://github.com/awshlabs/reinvent19Inf1Lab" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">workshop</a> delivered at re:Invent by my colleague Wenming Ye. One of the labs shows you how to compile a 32-bit floating point (FP32) ResNet50 model to 16-bit floating point (FP16). By reducing arithmetic complexity, this technique is known to improve performance while preserving accuracy. Indeed, on an <em class="markup--em markup--p-em">inf1.2xlarge</em> instance, the FP16 model delivers an impressive <strong class="markup--strong markup--p-strong">1,500 image classifications per second</strong>!</p><p name="8a3c" id="8a3c" class="graf graf--p graf-after--p graf--trailing">As always, thank you for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="3567" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="0042" id="0042" class="graf graf--p graf--leading">Inferentia NeuronCores are totally hardcore. They destroy everything \m/</p><figure name="fc46" id="fc46" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/DBwgX8yBqsw?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/b9672e8f8b8f"><time class="dt-published" datetime="2019-12-14T20:25:08.323Z">December 14, 2019</time></a>.</p><p><a href="https://medium.com/@julsimon/a-first-look-at-aws-inferentia-b9672e8f8b8f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
