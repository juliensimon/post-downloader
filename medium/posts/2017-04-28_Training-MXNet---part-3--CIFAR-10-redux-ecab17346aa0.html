<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Training MXNet — part 3: CIFAR-10 redux</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Training MXNet — part 3: CIFAR-10 redux</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 2, we learned about the CIFAR-10 data set and we saw how to easily load it using a RecordIO object. Using this data set, we both…
</section>
<section data-field="body" class="e-content">
<section name="149a" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0b8e" id="0b8e" class="graf graf--h3 graf--leading graf--title">Training MXNet — part 3: CIFAR-10 redux</h3><p name="bec4" id="bec4" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" data-href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" class="markup--anchor markup--p-anchor" target="_blank">part 2</a>, we learned about the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" data-href="https://www.cs.toronto.edu/~kriz/cifar.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">CIFAR-10</a> data set and we saw how to easily load it using a <a href="http://mxnet.io/api/python/io.html#module-mxnet.recordio" data-href="http://mxnet.io/api/python/io.html#module-mxnet.recordio" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RecordIO</a> object. Using this data set, we both trained a network from scratch and fine-tuned a network trained on ImageNet. In both cases, we used a fixed learning rate and we got to a point where validation accuracy <strong class="markup--strong markup--p-strong">plateaued</strong> at about 85%.</p><p name="c48b" id="c48b" class="graf graf--p graf-after--p">In this article, we’re going to focus on improving validation accuracy.</p><h4 name="5df0" id="5df0" class="graf graf--h4 graf-after--p">Using a variable learning rate</h4><p name="15b1" id="15b1" class="graf graf--p graf-after--h4">Let’s fine-tune our pre-trained network again. This time, we’re going to reduce the learning rate gradually, which should help the model converge to a “lower” minimum: we’ll start at 0.05 and multiply by 0.9 each time 25 epochs have gone by, until we reach 300 epochs.</p><blockquote name="799f" id="799f" class="graf graf--blockquote graf-after--p">Gradually reducing the learning rate is a key technique in improving validation accuracy.</blockquote><p name="dc94" id="dc94" class="graf graf--p graf-after--blockquote">We need 2 extra parameters to do this:</p><ul class="postList"><li name="1f07" id="1f07" class="graf graf--li graf-after--p"><em class="markup--em markup--li-em">lr-step-epochs</em>: the epoch steps when the learning rate will be updated.</li><li name="3d9b" id="3d9b" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">lr-factor</em>: the factor by which the learning rate will be updated.</li></ul><p name="d19a" id="d19a" class="graf graf--p graf-after--li">The new fine-tuning command becomes:</p><pre name="27f5" id="27f5" class="graf graf--pre graf-after--p">$ python fine-tune.py <br>--pretrained-model resnext-101 --load-epoch 0000 <br>--gpus 0,1,2,3 — batch-size 128<br>--data-train cifar10_train.rec --data-val cifar10_val.rec <br>--num-examples 50000 --num-classes 10 --image-shape 3,28,28 <br>--num-epoch 300 --lr 0.05 --lr-factor 0.9<br>--lr-step-epochs 25,50,75,100,125,150,175,200,225,250,275,300</pre><p name="b7fb" id="b7fb" class="graf graf--p graf-after--pre">A while later, here’s the result.</p><figure name="e476" id="e476" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D5hose4dujfVKUhu4bSx_Q.png" data-width="1086" data-height="852" src="https://cdn-images-1.medium.com/max/800/1*D5hose4dujfVKUhu4bSx_Q.png"><figcaption class="imageCaption">Fine-tuning a pre-trained R<em class="markup--em markup--figure-em">esNext-101 model on CIFAR-10, variable learning rate</em></figcaption></figure><p name="35f2" id="35f2" class="graf graf--p graf-after--figure">As we can see, reducing the learning rate definitely helps reaching higher validation accuracy. Previously, it plateaued at 85% after 25 epochs or so. Here, it’s climbing steadily up to <strong class="markup--strong markup--p-strong">89%</strong> and would probably keep increasing beyond 300 epochs. 4% is a huge difference: on a 10,000 image validation set, it means that an extra 400 images are correctly labelled.</p><h4 name="485e" id="485e" class="graf graf--h4 graf-after--p">AdaDelta</h4><p name="4ee9" id="4ee9" class="graf graf--p graf-after--h4">Surely, we could keep tweaking and find an even better combination for the initial learning rate, the factor and the steps. But we could also do without all these parameters, thanks to the AdaDelta optimizer.</p><p name="5b52" id="5b52" class="graf graf--p graf-after--p">AdaDelta is a evolution of the SGD algorithm we’ve been using so far for optimization (research paper). It doesn’t need to be given a learning rate. In fact, it will automatically select and adapt a learning rate for each dimension.</p><h4 name="7083" id="7083" class="graf graf--h4 graf-after--p">Training ResNext-101 from scratch with AdaDelta</h4><p name="edd9" id="edd9" class="graf graf--p graf-after--h4">Let’s try to train ResNext-101 from scratch, using AdaDelta. Instead of loading the network, we’re going to build one using <em class="markup--em markup--p-em">resnext.get_symbol()</em>, a <a href="https://github.com/dmlc/mxnet/tree/master/example/image-classification/symbols" data-href="https://github.com/dmlc/mxnet/tree/master/example/image-classification/symbols" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Python function</a> available in MXnet. There’s a whole set of functions to build different networks: I suggest that you take some time to look at them, as they will help you understand how these networks are structured.</p><p name="ff9d" id="ff9d" class="graf graf--p graf-after--p">You should now be familiar with the rest of the code :)</p><figure name="9c61" id="9c61" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/e99c7842bb61926282cd1bb03dc4d2e5.js"></script></figure><p name="b854" id="b854" class="graf graf--p graf-after--figure">These are the results after 300 epochs.</p><figure name="8f5e" id="8f5e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*t4mBFlioQavz4dCkC79W0g.png" data-width="1080" data-height="844" src="https://cdn-images-1.medium.com/max/800/1*t4mBFlioQavz4dCkC79W0g.png"><figcaption class="imageCaption">Training a R<em class="markup--em markup--figure-em">esNext-101 model from scratch on CIFAR-10, AdaDelta optimizer</em></figcaption></figure><p name="4cdb" id="4cdb" class="graf graf--p graf-after--figure">In our previous article, training the same model from scratch only yielded 80% validation accuracy. Here, not only does training accuracy increase extremely fast, we also reach <strong class="markup--strong markup--p-strong">86%</strong> validation accuracy without having to guess about learning rate or when to decrease it.</p><p name="0e22" id="0e22" class="graf graf--p graf-after--p">It’s very likely that an expert would achieve better results by tweaking optimization parameters, but for the rest of us, AdaDelta is an interesting option.</p><p name="2328" id="2328" class="graf graf--p graf-after--p">MXNet supports a whole set of <a href="http://mxnet.io/api/python/model.html#optimizer-api-reference" data-href="http://mxnet.io/api/python/model.html#optimizer-api-reference" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">optimization algorithms</a>. If you’d like to learn more on how they work and how they differ, here’s an <a href="http://sebastianruder.com/optimizing-gradient-descent/" data-href="http://sebastianruder.com/optimizing-gradient-descent/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">excellent article</a> by Sebastian Ruder.</p><h4 name="1576" id="1576" class="graf graf--h4 graf-after--p">Fast and Furious</h4><p name="a9c4" id="a9c4" class="graf graf--p graf-after--h4">One last thing. I mentioned earlier that training took ‘a while’. More precisely, it took 12+ hours using all 4 GPUs of a <a href="https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" data-href="https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">g2.8xlarg</a>e instance.</p><p name="d0b3" id="d0b3" class="graf graf--p graf-after--p">Could we go faster? Sure, I could use a <a href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" data-href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">p2.16xlarge</a> instance. That’s as large as GPU servers get.</p><p name="7f3a" id="7f3a" class="graf graf--p graf-after--p">Even faster? We need distributed training, which we’ll cover in part 4.</p><p name="e59d" id="e59d" class="graf graf--p graf-after--p graf--trailing">Thanks for reading :)</p></div></div></section><section name="a6b7" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a8dd" id="a8dd" class="graf graf--p graf--leading">Next:</p><p name="ad65" id="ad65" class="graf graf--p graf-after--p"><a href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" data-href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" class="markup--anchor markup--p-anchor" target="_blank">Part 4</a> — Distributed training</p><p name="a519" id="a519" class="graf graf--p graf-after--p graf--trailing"><a href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" data-href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" class="markup--anchor markup--p-anchor" target="_blank">Part 5</a> — Distributed training, EFS edition</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/ecab17346aa0"><time class="dt-published" datetime="2017-04-28T13:01:53.461Z">April 28, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>