<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Predicting world temperature with time series and DeepAR on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Predicting world temperature with time series and DeepAR on Amazon SageMaker</h1>
</header>
<section data-field="subtitle" class="p-summary">
Predicting time-based values is a popular use case for Machine Learning. Indeed, a lot of phenomena — from rainfall to fast-food queues to…
</section>
<section data-field="body" class="e-content">
<section name="d1e9" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="49f0" id="49f0" class="graf graf--h3 graf--leading graf--title">Predicting world temperature with time series and DeepAR on Amazon SageMaker</h3><p name="c909" id="c909" class="graf graf--p graf-after--h3">Predicting time-based values is a popular use case for Machine Learning. Indeed, a lot of phenomena — from rainfall to fast-food queues to stock prices — exhibit <strong class="markup--strong markup--p-strong">time-based patterns</strong> that can be successfully captured by a Machine Learning model.</p><p name="17ce" id="17ce" class="graf graf--p graf-after--p">In this post, you will learn how to predict <strong class="markup--strong markup--p-strong">temperature time-series</strong> using <strong class="markup--strong markup--p-strong">DeepAR</strong> — one of the latest <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">built-in algorithms</a> added to <a href="https://aws.amazon.com/sagemaker" data-href="https://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">Amazon SageMaker</a>. As usual, a <a href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/04-DeepAR-temperatures.ipynb" data-href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/04-DeepAR-temperatures.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Jupyter notebook</a> is available on Github.</p><blockquote name="7050" id="7050" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">The very nice collection of <a href="https://github.com/awslabs/amazon-sagemaker-examples" data-href="https://github.com/awslabs/amazon-sagemaker-examples" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">SageMaker sample notebooks</a> includes another <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb" data-href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">DeepAR example</a> and I strongly encourage you to check it out. Mine differs in two ways: it uses a real-life data set (not a synthetic one) and it doesn’t use the <em class="markup--em markup--blockquote-em">pandas</em> library, which I believe makes it a little easier to understand :)</blockquote><h4 name="8025" id="8025" class="graf graf--h4 graf-after--blockquote">A word about DeepAR</h4><p name="0bc4" id="0bc4" class="graf graf--p graf-after--h4">DeepAR is an algorithm introduced in 2017. It’s quite complex and I won’t go into details here. Let’s just say that unlike other techniques that train a different model for each time-series, DeepAR builds a <strong class="markup--strong markup--p-strong">single model for all time-series </strong>and tries to identify similarities across them. Intuitively, this sounds like a good idea for temperature time-series as we could expect them to exhibit similar patterns year after year.</p><p name="ee91" id="ee91" class="graf graf--p graf-after--p">If you’d like to know more about DeepAR, please refer to the original <a href="https://arxiv.org/abs/1704.04110" data-href="https://arxiv.org/abs/1704.04110" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research article</a> as well as the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker documentation</a>.</p><h4 name="6a2f" id="6a2f" class="graf graf--h4 graf-after--p">The Berkeyley Earth dataset</h4><p name="3fc2" id="3fc2" class="graf graf--p graf-after--h4">This <a href="http://berkeleyearth.org/data/" data-href="http://berkeleyearth.org/data/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dataset</a> contains a very large number of temperatures recorded across the globe since the 18th century. Here, I will use the <a href="http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt" data-href="http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Daily Land</a> dataset, which holds a <strong class="markup--strong markup--p-strong">daily temperature measure from 1880 to 2014</strong>. Temperatures are reported as a variation (aka anomaly) from the 1951–1980 average (8.68°C), as visible in the sixth column.</p><figure name="64b0" id="64b0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/2264e3e8261673a259518fd1ce9d15ed.js"></script></figure><p name="151f" id="151f" class="graf graf--p graf-after--figure">Before we go any further, we have to decide how to build our time-series. Given data resolution (one data point per day), we should probably build <strong class="markup--strong markup--p-strong">yearly</strong> series: having long enough series (hundreds of samples at least) is one of the requirements for a successful model. Thus, we’re going to build <strong class="markup--strong markup--p-strong">135 series of 365 samples </strong>(or 366 for leap years) using the second and sixth columns in our file.</p><p name="b0c6" id="b0c6" class="graf graf--p graf-after--p">In addition, we should decide how many samples we’d like to predict: let’s go for 30, i.e. <strong class="markup--strong markup--p-strong">predicting a month’s worth of temperatures</strong>.</p><h4 name="e8aa" id="e8aa" class="graf graf--h4 graf-after--p">Loading the dataset</h4><p name="8582" id="8582" class="graf graf--p graf-after--h4">Very little dataset preparation is required (woohoo!). Once we’ve downloaded and cleaned up the file (remove header and empty lines), we can directly load it into a list (for plotting) and a dictionary (for training). Note that we’re also adding the average temperature to all samples in order to work with actual temperatures, not variations.</p><figure name="d9f2" id="d9f2" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/bbd6696397491e0d73dd8b45b496db86.js"></script></figure><h4 name="7ca5" id="7ca5" class="graf graf--h4 graf-after--figure">Plotting the dataset</h4><p name="4f19" id="4f19" class="graf graf--p graf-after--h4">Let’s take a quick look at our dataset.</p><figure name="e5d0" id="e5d0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/588480b77e1e9a27b67c10d2486c6ac3.js"></script></figure><p name="94cc" id="94cc" class="graf graf--p graf-after--figure">I see an upward trend, but I’ll let each of you come to their own conclusions.</p><figure name="110d" id="110d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DR85XG5Tie8zv2WhcqHi-g.png" data-width="3608" data-height="904" src="https://cdn-images-1.medium.com/max/800/1*DR85XG5Tie8zv2WhcqHi-g.png"><figcaption class="imageCaption">World temperature from 1880 to 2014.</figcaption></figure><h4 name="6243" id="6243" class="graf graf--h4 graf-after--figure">Preparing the training and test sets</h4><p name="6024" id="6024" class="graf graf--p graf-after--h4">We’re not going to split 80/20 like we usually would. Things are a bit different when working with time series:</p><ul class="postList"><li name="5d5c" id="5d5c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Training set</strong>: we need to <strong class="markup--strong markup--li-strong">remove the last 30 sample points</strong> from each time series. Time series should also be shuffled, although it is unnecessary here because Python dictionaries are not ordered ;)</li><li name="88d9" id="88d9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Test set</strong>: we can use the <strong class="markup--strong markup--li-strong">full dataset</strong>.</li></ul><figure name="eb4b" id="eb4b" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/4040fce372c1b57fe2c7fd9e1f3196f3.js"></script></figure><h4 name="1fcc" id="1fcc" class="graf graf--h4 graf-after--figure">Writing the datasets to JSON format</h4><p name="d0f8" id="d0f8" class="graf graf--p graf-after--h4">The <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">input format</a> for DeepAR is <a href="http://jsonlines.org/" data-href="http://jsonlines.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">JSON Lines</a>: <strong class="markup--strong markup--p-strong">one sample per line</strong>, such as this one.</p><pre name="f8ff" id="f8ff" class="graf graf--pre graf-after--p">{&quot;start&quot;:&quot;2009-11-01 00:00:00&quot;, &quot;target&quot;: [4.3, 10.3, ...]}</pre><p name="0577" id="0577" class="graf graf--p graf-after--pre">Easy enough, let’s take care of it.</p><figure name="3672" id="3672" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7e77430ecbc63ec364ed490e79babb31.js"></script></figure><h4 name="2d0a" id="2d0a" class="graf graf--h4 graf-after--figure">Uploading to S3</h4><p name="34e2" id="34e2" class="graf graf--p graf-after--h4">Next, let’s upload our data to S3. The <a href="https://github.com/aws/sagemaker-python-sdk" data-href="https://github.com/aws/sagemaker-python-sdk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker SDK</a> has a nice little function to do this.</p><figure name="9cba" id="9cba" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/819c360dc958d16b29d7efbc7d32bf7f.js"></script></figure><h4 name="ac0e" id="ac0e" class="graf graf--h4 graf-after--figure">Configuring the training job</h4><p name="2553" id="2553" class="graf graf--p graf-after--h4">As usual with built-in algorithms, we need to select the <strong class="markup--strong markup--p-strong">container</strong> corresponding to the region we run in and then create an <a href="http://sagemaker.readthedocs.io/en/latest/estimators.html" data-href="http://sagemaker.readthedocs.io/en/latest/estimators.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Estimator</em></a>. Nothing unusual here.</p><figure name="8f85" id="8f85" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/aaa889936f96dec8fc34047f913bd983.js"></script></figure><p name="bf22" id="bf22" class="graf graf--p graf-after--figure">Now let’s look at hyper parameters.</p><h4 name="b029" id="b029" class="graf graf--h4 graf-after--p">Defining hyper parameters</h4><p name="5c03" id="5c03" class="graf graf--p graf-after--h4">Hyper parameters for DeepAR are detailed in the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentation</a>. Let’s focus on the required ones:</p><ul class="postList"><li name="a7ca" id="a7ca" class="graf graf--li graf-after--p"><em class="markup--em markup--li-em">time_freq</em>: the <strong class="markup--strong markup--li-strong">resolution of time series</strong> (from minutes to months). Ours has daily resolution.</li><li name="5780" id="5780" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">prediction_length</em>: <strong class="markup--strong markup--li-strong">how many data samples we’re going to predict</strong> (30, remember?).</li><li name="5170" id="5170" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">context_length</em>: <strong class="markup--strong markup--li-strong">how many data points we’re going to look at before predicting</strong>. We’ll use 30 too, it should be enough to figure out temperatures.</li><li name="28c0" id="28c0" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">epochs</em>: I wonder what this does ;)</li></ul><p name="6d4a" id="6d4a" class="graf graf--p graf-after--li">In addition, after a unreasonable number of different tests, I ended up getting better results with only <strong class="markup--strong markup--p-strong">two layers</strong> (default is three), a <strong class="markup--strong markup--p-strong">smaller learning rate</strong> and a <strong class="markup--strong markup--p-strong">large number of epochs</strong>.</p><blockquote name="1800" id="1800" class="graf graf--blockquote graf-after--p">My intuition is that the data set being pretty small with rather short time-series, three layers tend to overfit more. Two layers don’t learn as well, but letting them learn longer with a smaller learning rate makes up for it. Or something. Curious to here your own opinion.</blockquote><figure name="72e7" id="72e7" class="graf graf--figure graf--iframe graf-after--blockquote"><script src="https://gist.github.com/juliensimon/eada2a7d9e26244946d248220e766fd0.js"></script></figure><h4 name="4558" id="4558" class="graf graf--h4 graf-after--figure">Training the model</h4><p name="5348" id="5348" class="graf graf--p graf-after--h4">Super simple :)</p><figure name="d579" id="d579" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8afeef113853f75b5254b257da34fc12.js"></script></figure><p name="06c8" id="06c8" class="graf graf--p graf-after--figure">Training stops early and the best epoch is selected (#206). Here are my three metrics: <strong class="markup--strong markup--p-strong">loss for p50 and p90</strong> (which tell us how accurate the predicted distribution is), as well as <strong class="markup--strong markup--p-strong">Root Mean Square Error</strong>.</p><pre name="8952" id="8952" class="graf graf--pre graf-after--p">[01/31/2018 22:19:52 INFO 140078416930624] #test_score (algo-1, <strong class="markup--strong markup--pre-strong">wQuantileLoss[0.5]</strong>): <strong class="markup--strong markup--pre-strong">0.0584739</strong><br>[01/31/2018 22:19:52 INFO 140078416930624] #test_score (algo-1, <strong class="markup--strong markup--pre-strong">wQuantileLoss[0.9]</strong>): <strong class="markup--strong markup--pre-strong">0.0294685</strong><br>[01/31/2018 22:19:52 INFO 140078416930624] #test_score (algo-1, <strong class="markup--strong markup--pre-strong">RMSE</strong>): <strong class="markup--strong markup--pre-strong">0.62858571005</strong></pre><p name="5fa4" id="5fa4" class="graf graf--p graf-after--pre">OK, now let’s deploy this model and use it.</p><h4 name="6054" id="6054" class="graf graf--h4 graf-after--p">Deploying the model</h4><p name="ee25" id="ee25" class="graf graf--p graf-after--h4">Nothing complicated here: create an <strong class="markup--strong markup--p-strong">endpoint</strong> hosting our model and create a <em class="markup--em markup--p-em">RealTimePredictor</em> to send requests to.</p><figure name="14b3" id="14b3" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/64869098ce9bc2edea8c67c35264b728.js"></script></figure><h4 name="bfa7" id="bfa7" class="graf graf--h4 graf-after--figure">Building a prediction request</h4><p name="850b" id="850b" class="graf graf--p graf-after--h4">According to the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">inference format</a> for DeepAR, here’s what we should send to the endpoint:</p><ul class="postList"><li name="3b85" id="3b85" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">JSON-formatted samples</strong> (we’ll use only one at a time)</li><li name="8bb1" id="8bb1" class="graf graf--li graf-after--li">an <strong class="markup--strong markup--li-strong">optional configuration</strong> listing the <strong class="markup--strong markup--li-strong">series</strong> we’d like to receive (mean values, quantiles and raw samples: we’ll take all of them, thank you) as well as the <strong class="markup--strong markup--li-strong">number of samples</strong> from which they’re built.</li></ul><figure name="179f" id="179f" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/35bcc0fedcbceeafa7889d2b7ed3d8d7.js"></script></figure><p name="4fde" id="4fde" class="graf graf--p graf-after--figure">Here’s an example request, where we provide the first 30 data points.</p><pre name="fe75" id="fe75" class="graf graf--pre graf-after--p">{&quot;<strong class="markup--strong markup--pre-strong">instances</strong>&quot;: [ {&quot;<strong class="markup--strong markup--pre-strong">start</strong>&quot;: &quot;2018-01-01 00:00:00&quot;, &quot;<strong class="markup--strong markup--pre-strong">target</strong>&quot;: [8.371208085491508, 8.38437885371535, 8.860699073980985, 8.047195011672134, 9.42771383264719, 8.02120332304575, 9.839234913116105, 9.237618947392374, 8.214949470821212, 9.814497679561292, 9.052164695305954, 8.102437854966766, 8.928941871965348, 9.844116398312188, 9.221646100693144, 8.853571486995326, 8.560903044968434, 8.240263518568812, 9.221323908588538, 9.448381346299827, 9.996678314417732, 8.520757726306975, 9.978841260562627, 9.196420806291513, 9.587904493744922, 9.367880938747199, 9.606228859687628, 9.277298500001638, 8.694011829622228, 8.264125277439893]}], <br>&quot;<strong class="markup--strong markup--pre-strong">configuration</strong>&quot;: {&quot;<strong class="markup--strong markup--pre-strong">output_types</strong>&quot;: [&quot;mean&quot;, &quot;quantiles&quot;, &quot;samples&quot;], &quot;<strong class="markup--strong markup--pre-strong">quantiles</strong>&quot;: [&quot;0.1&quot;, &quot;0.9&quot;], &quot;<strong class="markup--strong markup--pre-strong">num_samples</strong>&quot;: 100}}</pre><h4 name="5d5b" id="5d5b" class="graf graf--h4 graf-after--pre">Extracting prediction results</h4><p name="eb86" id="eb86" class="graf graf--p graf-after--h4">Once we get prediction results, we need to extract each time series for plotting. Obviously, we’re not interested in the 100 raw sample series, let’s just pick one at random.</p><figure name="5217" id="5217" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8e328a3859bae2aacc4c3f402c997c87.js"></script></figure><h4 name="d1be" id="d1be" class="graf graf--h4 graf-after--figure">Plotting prediction results</h4><p name="7d8f" id="7d8f" class="graf graf--p graf-after--h4">Pretty graphs: everyone loves them. They’ll also help us get a sense of how well we’re predicting. We’ll throw in ground truth for good measure.</p><figure name="bc53" id="bc53" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/44863b030d62fc3baf52ee5b0c704595.js"></script></figure><h4 name="29d6" id="29d6" class="graf graf--h4 graf-after--figure">Predicting some samples</h4><p name="efef" id="efef" class="graf graf--p graf-after--h4">All right, time to put all of this to work!</p><p name="c6f6" id="c6f6" class="graf graf--p graf-after--p">First, let’s predict the <strong class="markup--strong markup--p-strong">last 30 days of 1984</strong> and compare to <strong class="markup--strong markup--p-strong">ground truth</strong>.</p><figure name="bf60" id="bf60" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/711c03d03f436ec27304ddcc6145c3ed.js"></script></figure><figure name="6ad4" id="6ad4" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*Y9JN97itj-NVuKmr_2Bozw.png" data-width="764" data-height="504" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*Y9JN97itj-NVuKmr_2Bozw.png"><figcaption class="imageCaption">Purple vs blue: not too bad!</figcaption></figure><p name="fe56" id="fe56" class="graf graf--p graf-after--figure">Let’s try another example. This time, suppose that we have <strong class="markup--strong markup--p-strong">data samples for the first 90 days of 2018 </strong>(we’ll just use random values here) and that we want to <strong class="markup--strong markup--p-strong">predict the next 30 days</strong>. Here’s how you would do it.</p><figure name="cf6b" id="cf6b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/3be1369fd8113bf0f54ee23eb08c2e4e.js"></script></figure><figure name="7b66" id="7b66" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*eQSdBMJSrdT0MUkhL1hnWw.png" data-width="764" data-height="504" src="https://cdn-images-1.medium.com/max/800/1*eQSdBMJSrdT0MUkhL1hnWw.png"></figure><h4 name="45f5" id="45f5" class="graf graf--h4 graf-after--figure">Conclusion</h4><p name="54fb" id="54fb" class="graf graf--p graf-after--h4">As you can see, built-in algorithms like DeepAR are a great way to get the job done quickly: <strong class="markup--strong markup--p-strong">no training code to write, no infrastructure drama to endure</strong>. We can thus focus on experimenting with our time series and hyper-parameters to get the best result possible.</p><p name="de7d" id="de7d" class="graf graf--p graf-after--p">If you’re curious about other<strong class="markup--strong markup--p-strong"> SageMaker built-in algorithms</strong>, here are some previous posts on:</p><ul class="postList"><li name="9d00" id="9d00" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" data-href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" class="markup--anchor markup--li-anchor" target="_blank">spam classification with XGBoost</a>,</li><li name="69fa" id="69fa" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" data-href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" class="markup--anchor markup--li-anchor" target="_blank">image classification with Deep Learning</a>,</li><li name="a8fc" id="a8fc" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8" data-href="https://medium.com/@julsimon/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8" class="markup--anchor markup--li-anchor" target="_blank">movie recommendation with Factorization Machines</a>.</li></ul><p name="ca29" id="ca29" class="graf graf--p graf-after--li graf--trailing">As always, thank you for reading. Happy to answer questions on <a href="https://twitter.com/julsimon/" data-href="https://twitter.com/julsimon/" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="f9b8" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c91e" id="c91e" class="graf graf--p graf--startsWithDoubleQuote graf--leading"><em class="markup--em markup--p-em">“Do you wanna know the truth, son? Lord, I’ll tell you the truth. Your soul’s gonna burn in a lake of fire”.</em></p><figure name="aa91" id="aa91" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/k2rR5HaXnZQ?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/e371cf94ddb5"><time class="dt-published" datetime="2018-02-01T19:07:33.611Z">February 1, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/predicting-world-temperature-with-time-series-and-deepar-on-amazon-sagemaker-e371cf94ddb5" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>