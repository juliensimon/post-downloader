<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Fascinating Tales of a Strange Tomorrow</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Fascinating Tales of a Strange Tomorrow</h1>
</header>
<section data-field="subtitle" class="p-summary">
AI: Science vs. Fiction
</section>
<section data-field="body" class="e-content">
<section name="ae9a" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f0d0" id="f0d0" class="graf graf--h3 graf--leading graf--title">Fascinating Tales of a Strange Tomorrow</h3><h3 name="a4da" id="a4da" class="graf graf--h3 graf-after--h3">AI: Science vs. Fiction</h3><p name="0d92" id="0d92" class="graf graf--p graf-after--h3">Our trip begins in March 1956 with the release of the “Forbidden Planet” movie, which featured <strong class="markup--strong markup--p-strong">Robbie the Robot</strong>, commonly acknowledged as the first Science Fiction robot on screen. A few months later, a small group of Computer Scientists led by <strong class="markup--strong markup--p-strong">John McCarthy</strong><a href="https://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29" data-href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[1]</a> held a 6-week workshop<a href="https://en.wikipedia.org/wiki/Dartmouth_workshop" data-href="https://en.wikipedia.org/wiki/Dartmouth_workshop" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[2]</a> at Dartmouth College in New Hampshire.</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="618a" id="618a" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 58.207%;"><img class="graf-image" data-image-id="1*BP0H48KcXBO52m912cV_mw.png" data-width="480" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*BP0H48KcXBO52m912cV_mw.png"></figure><figure name="0bcc" id="0bcc" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 41.793%;"><img class="graf-image" data-image-id="1*g_mqan44TfqMfks65-MMmw.png" data-width="344" data-height="479" src="https://cdn-images-1.medium.com/max/600/1*g_mqan44TfqMfks65-MMmw.png"><figcaption class="imageCaption" style="width: 239.275%; left: -139.275%;">John McCarthy (Turing Award 1971) &amp; Robbie the Robot</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="a9ab" id="a9ab" class="graf graf--p graf-after--figure">The topic of this workshop was “<strong class="markup--strong markup--p-strong">Artificial Intelligence</strong>”, a term coined by McCarthy himself, which he defined this way:</p><blockquote name="d475" id="d475" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p"><em class="markup--em markup--blockquote-em">“Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it”</em>.</blockquote><p name="9750" id="9750" class="graf graf--p graf-after--blockquote">Wouldn’t it be great if the Dartmouth workshop had actually be triggered by John McCarthy seeing the “Forbidden Planet” and then going home thinking: “Let’s build Robbie”? That’s probably not true at all, though. Oh well.</p><p name="34b1" id="34b1" class="graf graf--p graf-after--p">Anyway, the group got to work and laid the foundations of Artificial Intelligence as we know it. In fact, most of the participants devoted their entire career to furthering the state of the art on AI, receiving no less than four Turing Awards in the process: <strong class="markup--strong markup--p-strong">Marvin Minsky</strong><a href="https://en.wikipedia.org/wiki/Marvin_Minsky" data-href="https://en.wikipedia.org/wiki/Marvin_Minsky" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[3]</a> in 1969, John McCarthy in 1971, <strong class="markup--strong markup--p-strong">Herbert Simon</strong><a href="https://en.wikipedia.org/wiki/Herbert_A._Simon" data-href="https://en.wikipedia.org/wiki/Herbert_A._Simon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[4]</a> &amp; <strong class="markup--strong markup--p-strong">Allen Newell</strong><a href="https://en.wikipedia.org/wiki/Allen_Newell" data-href="https://en.wikipedia.org/wiki/Allen_Newell" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[5]</a> in 1975.</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="724c" id="724c" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 40.579%;"><img class="graf-image" data-image-id="1*EPw7CI9Pecfhde4FwDq_Mw.png" data-width="307" data-height="300" src="https://cdn-images-1.medium.com/max/600/1*EPw7CI9Pecfhde4FwDq_Mw.png"></figure><figure name="6441" id="6441" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 59.421%;"><img class="graf-image" data-image-id="1*LBdjlgihtm_Pu5x2BXZWJg.png" data-width="352" data-height="235" src="https://cdn-images-1.medium.com/max/800/1*LBdjlgihtm_Pu5x2BXZWJg.png"><figcaption class="imageCaption" style="width: 168.291%; left: -68.291%;">Herbert Simon (Turing Award 1975, Nobel Prize in Economics 1978) ad Allen Newell (Turing Award 1975)</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="43a8" id="43a8" class="graf graf--p graf-after--figure">During the early years of AI, these bright scientists made predictions, such as:</p><p name="27c1" id="27c1" class="graf graf--p graf-after--p">· 1958, Herbert Simon and Allen Newell: “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Within 10 years a digital computer will be the world’s chess champion.</em></strong>”</p><p name="1b0c" id="1b0c" class="graf graf--p graf-after--p">· 1965, Herbert Simon: “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Machines will be capable, within 20 years, of doing any work a man can do.</em></strong>”</p><p name="08df" id="08df" class="graf graf--p graf-after--p">· 1967 Marvin Minsky: “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Within a generation, the problem of creating ‘artificial intelligence’ will substantially be solved</em>.</strong>”</p><p name="7d37" id="7d37" class="graf graf--p graf-after--p">· 1970 Marvin Minsky: “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">In from 3 to 8 years, we will have a machine with the general intelligence of an average human being.</em></strong>”</p><p name="6c94" id="6c94" class="graf graf--p graf-after--p">Oops.</p><h3 name="c139" id="c139" class="graf graf--h3 graf-after--p">The AI Winter is Coming</h3><p name="5cfa" id="5cfa" class="graf graf--p graf-after--h3">Predicting the future is always risky business, but still… <strong class="markup--strong markup--p-strong">This raises an daunting question: how could such brilliant minds be so awfully wrong about what AI would (or wouldn’t) achieve in a reasonable time frame? </strong>Don’t worry, we’ll answer this question later on.</p><blockquote name="25d9" id="25d9" class="graf graf--blockquote graf-after--p">Unfortunately, repeated failures to achieve significant progress became a trademark of Artificial Intelligence.</blockquote><p name="68e3" id="68e3" class="graf graf--p graf-after--blockquote">Expectations were high, little or no results were delivered, funds were cut and projects were abandoned. Unsurprisingly, these multiple “<strong class="markup--strong markup--p-strong">AI winters</strong>” discouraged all but the most hardcore supporters.</p><p name="1c86" id="1c86" class="graf graf--p graf-after--p">The most glaring symbol of this disillusion came from Marvin Minsky himself. In 2001, he gave a talk named “<strong class="markup--strong markup--p-strong">It’s 2001: Where is HAL?</strong>” referring of course to the HAL computer in Stanley Kubrick’s movie “2001: A Space Odyssey”. This is all the more significant that back in 1968, Minsky actually advised Kubrick during the making of the movie. In this talk, he notably addresses the “<strong class="markup--strong markup--p-strong">Common Sense issue</strong>” in non-ambiguous terms: <em class="markup--em markup--p-em">“No program today can distinguish a dog from a cat, or recognize objects in typical rooms, or answer questions that 4-year-olds can!”</em></p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="d8b5" id="d8b5" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 59.219%;"><img class="graf-image" data-image-id="1*l9D6KMIc_WX9kvoxlp5VaQ.png" data-width="624" data-height="429" src="https://cdn-images-1.medium.com/max/800/1*l9D6KMIc_WX9kvoxlp5VaQ.png"></figure><figure name="1feb" id="1feb" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 40.781%;"><img class="graf-image" data-image-id="1*u2hrDMKr1k__X6fSgmKzqQ.png" data-width="429" data-height="428" src="https://cdn-images-1.medium.com/max/600/1*u2hrDMKr1k__X6fSgmKzqQ.png"><figcaption class="imageCaption" style="width: 245.212%; left: -145.212%;">Marvin Minsky (Turing Award 1969) &amp; HAL 9000</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="10c1" id="10c1" class="graf graf--p graf-after--figure">Bottom line: AI is cool to play with in a lab environment, but it will never achieve anything in the real world. Case closed.</p><h3 name="175c" id="175c" class="graf graf--h3 graf-after--p">Meanwhile, on the US West Coast…</h3><p name="9ebc" id="9ebc" class="graf graf--p graf-after--h3">While AI researchers despaired in their labs, a number of startups were reinventing the world: Amazon, Google, Yahoo, later joined by Facebook and a few others were growing their web platforms at a frantic pace. In the process, they were acquiring users by the millions and piling up mountains of data. It soon became clear that this data was a goldmine, <strong class="markup--strong markup--p-strong">if it could actually be mined</strong>!</p><p name="52d9" id="52d9" class="graf graf--p graf-after--p">Using commodity hardware, these companies’ engineers set on a quest to design and build data processing platforms that would allow them to <strong class="markup--strong markup--p-strong">crunch raw data and extract business value that could turn into revenue</strong>… always a key goal for fast-growing startups!</p><p name="f747" id="f747" class="graf graf--p graf-after--p">A major milestone was reached in December 2004, when Google released the famous <strong class="markup--strong markup--p-strong">Map Reduce</strong> paper<a href="https://research.google.com/archive/mapreduce.html" data-href="https://research.google.com/archive/mapreduce.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[6]</a>, where they described « <em class="markup--em markup--p-em">a programming model and an associated implementation for processing and generating large data sets </em>». Not to be outdone, Yahoo implemented the ideas described in this paper and released a first version of their project in April 2006: <strong class="markup--strong markup--p-strong">Hadoop</strong><a href="https://hadoop.apache.org/" data-href="https://hadoop.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[7]</a> was born.</p><blockquote name="a3d3" id="a3d3" class="graf graf--blockquote graf-after--p">Gasoline waiting for a match: the Machine Learning explosion happened and the rest, as they say, is <strong class="markup--strong markup--blockquote-strong">history</strong>.</blockquote><h3 name="9973" id="9973" class="graf graf--h3 graf-after--blockquote">Fast-forward a few years</h3><p name="5f32" id="5f32" class="graf graf--p graf-after--h3">2010 or so: <strong class="markup--strong markup--p-strong">Machine Learning is now a commodity</strong>. Customers have a wide range of options, from DIY to Machine Learning as a Service. Everything is great in Data World. But is it really? Yes, Machine Learning helped us make a lot of applications “smarter” but <strong class="markup--strong markup--p-strong">did we make significant progress on Artificial Intelligence?</strong> In other words, are we any closer to “building HAL”? Well… no. Let’s try to understand why.</p><p name="a57a" id="a57a" class="graf graf--p graf-after--p">One of the first steps in building a Machine Learning application is called “<strong class="markup--strong markup--p-strong">feature extraction</strong>”. In a nutshell, this is a step where Data Scientists explore the data set to figure out which variables are meaningful in predicting or classifying data and which aren’t. Although this is still mostly a lengthy manual process, it’s now well understood and works nicely on structured or semi-structured data such as web logs or sales data.</p><p name="d23a" id="d23a" class="graf graf--p graf-after--p">However, <strong class="markup--strong markup--p-strong">it doesn’t work for complex AI problems</strong> such as computer vision or computer speech, simply because it’s quite impossible to define <strong class="markup--strong markup--p-strong">formally</strong> what the features are: for example, what makes a cat a cat? And how is a cat different from a dog? Or from a lion?</p><blockquote name="67ca" id="67ca" class="graf graf--blockquote graf-after--p">To put it simply, traditional Machine Learning doesn’t solve this kind of problem, which is why new tools are needed. Enter neural networks!</blockquote><h3 name="99fb" id="99fb" class="graf graf--h3 graf-after--blockquote">Back To The Future</h3><p name="6d0c" id="6d0c" class="graf graf--p graf-after--h3">New tools? Hardly! In 1957, Frank Rosenblatt designed an electro-mechanical neural network, the Perceptron<a href="https://en.wikipedia.org/wiki/Perceptron" data-href="https://en.wikipedia.org/wiki/Perceptron" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[8]</a>, which he trained to recognize images (20x20 “pixels”). In 1975, Paul Werbos published a article describing “backpropagation”<a href="https://en.wikipedia.org/wiki/Backpropagation" data-href="https://en.wikipedia.org/wiki/Backpropagation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[9]</a>, an algorithm allowing better and faster training of neural networks.</p><p name="488b" id="488b" class="graf graf--p graf-after--p">So, <strong class="markup--strong markup--p-strong">if neural networks have been around for so long, surely they must be partly responsible for failed AI attempts, right?</strong> Should they really be resurrected? Why would they suddenly be successful?</p><p name="001a" id="001a" class="graf graf--p graf-after--p">Very valid questions indeed. Let’s first take a quick look at how neural networks work. A <strong class="markup--strong markup--p-strong">neuron</strong> is a simple construct, which sums multiple <strong class="markup--strong markup--p-strong">weighted inputs</strong> to produce an <strong class="markup--strong markup--p-strong">output</strong>. Neurons are organized in <strong class="markup--strong markup--p-strong">layers</strong>, where the output of each neuron in layer ’n’ serves as an input to each neuron in layer ‘n+1’. The first layer is called the <strong class="markup--strong markup--p-strong">input layer</strong> and is fed with the input data, say the pixel values of an image. The last layer is called the <strong class="markup--strong markup--p-strong">output layer</strong> and produces the result, say a category number for the image (“this is a dog”).</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="3c2b" id="3c2b" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 64.244%;"><img class="graf-image" data-image-id="1*s3q_ob5bY1V-xB25zJE4iA.png" data-width="477" data-height="287" src="https://cdn-images-1.medium.com/max/800/1*s3q_ob5bY1V-xB25zJE4iA.png"></figure><figure name="303c" id="303c" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 35.756%;"><img class="graf-image" data-image-id="1*qloAHmuLvAUbLsDYobaGfQ.png" data-width="332" data-height="359" src="https://cdn-images-1.medium.com/max/600/1*qloAHmuLvAUbLsDYobaGfQ.png"><figcaption class="imageCaption" style="width: 279.673%; left: -179.673%;">The basic structure of a neural network (Source: “Deep Learning”, Goodfellow &amp; Bengio, 2016)</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><blockquote name="2d18" id="2d18" class="graf graf--blockquote graf-after--figure">The beauty of neural networks is that they’re able to <strong class="markup--strong markup--blockquote-strong">self-organize</strong>: given a large enough data set (say, images as inputs and category labels as outputs), a neural network is able to learn <strong class="markup--strong markup--blockquote-strong">automatically</strong> how to produce correct answers</blockquote><p name="7b70" id="7b70" class="graf graf--p graf-after--blockquote">Thanks to an iterative training process, it’s able to <strong class="markup--strong markup--p-strong">discover the features</strong> which allow images to be categorized, and adjusts weights repeatedly to reach the best result, i.e. the one with the smallest error rate.</p><p name="f0e5" id="f0e5" class="graf graf--p graf-after--p">The training phase and its automatic feature discovery are well adapted to solving informal problems, but here’s the catch: they involve <strong class="markup--strong markup--p-strong">a lot</strong> of math operations which tend to grow exponentially as <strong class="markup--strong markup--p-strong">data size</strong> increases (think high-resolution pictures) and as the <strong class="markup--strong markup--p-strong">number of layers</strong> increases. This problem is called the “<strong class="markup--strong markup--p-strong">Curse of Dimensionality</strong>” and it’s one of the major reasons why neural networks stagnated for decades: there was simply not enough <strong class="markup--strong markup--p-strong">computing power</strong> available to run them at scale.</p><p name="a6ca" id="a6ca" class="graf graf--p graf-after--p">Nor was enough <strong class="markup--strong markup--p-strong">data</strong> available. Neural networks need a lot of data to learn properly. The more data, the better! Until recently, it was simply not possible to gather and store vast amounts of digital data. Do you remember punch cards or floppy disks?</p><p name="f2df" id="f2df" class="graf graf--p graf-after--p">A significant breakthrough happened in 1998 when Yann Le Cun invented <strong class="markup--strong markup--p-strong">Convolutional Neural Networks</strong><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" data-href="https://en.wikipedia.org/wiki/Convolutional_neural_network" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[10]</a>, a new breed of multi-layered networks (hence the term “Deep Learning”).</p><blockquote name="fc4f" id="fc4f" class="graf graf--blockquote graf-after--p">In a nutshell, CNNs are able to extract features efficiently while reducing the size of input data: this allows smaller networks to be used for classification, which dramatically reduces the computing cost of network training.</blockquote><p name="da83" id="da83" class="graf graf--p graf-after--blockquote">This approach was so successful that banks adopted CNN-driven systems to automate handwriting recognition for checks. This was an encouraging accomplishment for neural networks… but the best was still to come!</p><figure name="7d8d" id="7d8d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qGWVCW3scw3g6EEF8Pscog.png" data-width="1061" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*qGWVCW3scw3g6EEF8Pscog.png"><figcaption class="imageCaption">Architecture of a Convolutional Neural Network (Source: NVIDIA blog)</figcaption></figure><h3 name="b95c" id="b95c" class="graf graf--h3 graf-after--figure">The Neural Empire Strikes Back</h3><p name="1dd1" id="1dd1" class="graf graf--p graf-after--h3">By the late 2000s, three quasi-simultaneous events made large-scale neural networks possible.</p><p name="65da" id="65da" class="graf graf--p graf-after--p">First, <strong class="markup--strong markup--p-strong">large data sets</strong> became widely available. Text, pictures, movies, music: everything was suddenly digital and could be used to train neural networks. Today, the ImageNet<a href="http://image-net.org" data-href="http://image-net.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[11]</a> database holds over 14 million labeled images and researchers worldwide use it to compete every year<a href="http://image-net.org/challenges/LSVRC/" data-href="http://image-net.org/challenges/LSVRC/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[12]</a> in build the most successful image detection and classification network (more on this later).</p><p name="57d2" id="57d2" class="graf graf--p graf-after--p">Then, researchers were able to leverage the <strong class="markup--strong markup--p-strong">spectacular parallel processing power</strong> of <strong class="markup--strong markup--p-strong">Graphics Processing Units</strong> (GPUs) to train large neural networks. Can you believe that the ones that won the 2015 and 2016 ImageNet competition have respectively 152 and 269 layers?</p><p name="704e" id="704e" class="graf graf--p graf-after--p">Last but not least, <strong class="markup--strong markup--p-strong">Cloud computing</strong> brought <strong class="markup--strong markup--p-strong">elasticity</strong> and <strong class="markup--strong markup--p-strong">scalability</strong> to developers and researchers, allowing them to use as much infrastructure as needed for training… without having to build, run or pay for it long term.</p><blockquote name="4451" id="4451" class="graf graf--blockquote graf-after--p">The combination of these three factors helped neural networks deliver on their 60-year old promise.</blockquote><p name="88c2" id="88c2" class="graf graf--p graf-after--blockquote">State of the art networks are now able to <strong class="markup--strong markup--p-strong">classify images</strong> <strong class="markup--strong markup--p-strong">faster</strong> and <strong class="markup--strong markup--p-strong">more accurately</strong> than any human (less than 3% error vs. 5% for humans). Devices like the Amazon Echo understand <strong class="markup--strong markup--p-strong">natural language</strong> and speak back at us. Autonomous cars are becoming a reality. And the list of AI applications grows every day.</p><p name="942e" id="942e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Wouldn’t you like to add yours?</strong></p><figure name="0eff" id="0eff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*V_FsNpscIuseTUAB3cy_AA.png" data-width="1469" data-height="787" src="https://cdn-images-1.medium.com/max/800/1*V_FsNpscIuseTUAB3cy_AA.png"><figcaption class="imageCaption">Number of layers and error rate of ILSVRC winners</figcaption></figure><h3 name="2bcb" id="2bcb" class="graf graf--h3 graf-after--figure">How AWS can help you build Deep Learning applications</h3><p name="0461" id="0461" class="graf graf--p graf-after--h3">AWS provides everything you need to start building Deep Learning applications:</p><p name="ba63" id="ba63" class="graf graf--p graf-after--p">· A wide range of <strong class="markup--strong markup--p-strong">Amazon EC2 instances</strong> to build and train your models, with your choice of <strong class="markup--strong markup--p-strong">CPU</strong><a href="https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/" data-href="https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[13]</a>, <strong class="markup--strong markup--p-strong">GPU</strong><a href="https://aws.amazon.com/blogs/aws/in-the-work-amazon-ec2-elastic-gpus/" data-href="https://aws.amazon.com/blogs/aws/in-the-work-amazon-ec2-elastic-gpus/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[14]</a> <a href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" data-href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[15]</a> or even <strong class="markup--strong markup--p-strong">FPGA</strong><a href="https://aws.amazon.com/blogs/aws/developer-preview-ec2-instances-f1-with-programmable-hardware/" data-href="https://aws.amazon.com/blogs/aws/developer-preview-ec2-instances-f1-with-programmable-hardware/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[16]</a>.</p><p name="8b7f" id="8b7f" class="graf graf--p graf-after--p">· The <strong class="markup--strong markup--p-strong">Deep Learning Amazon Machine Image</strong><a href="https://aws.amazon.com/marketplace/pp/B01M0AXXQB" data-href="https://aws.amazon.com/marketplace/pp/B01M0AXXQB" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[17]</a>, a collection of pre-installed tools and libraries: mxnet<a href="http://mxnet.io/" data-href="http://mxnet.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[18]</a> (which AWS officially supports), Theano, Caffe, TensorFlow, Torch, Anaconda and more.</p><p name="cfe7" id="cfe7" class="graf graf--p graf-after--p">· High-level <strong class="markup--strong markup--p-strong">AI services</strong><a href="https://aws.amazon.com/amazon-ai/" data-href="https://aws.amazon.com/amazon-ai/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[19]</a> for image recognition (<strong class="markup--strong markup--p-strong">Amazon Rekognition</strong>), speech to text (<strong class="markup--strong markup--p-strong">Amazon Polly</strong>) and chatbots (<strong class="markup--strong markup--p-strong">Amazon Lex</strong>).</p><blockquote name="7809" id="7809" class="graf graf--blockquote graf-after--p">The choice is yours, <strong class="markup--strong markup--blockquote-strong">just get started</strong> and help Science catch up with Fiction!</blockquote><h3 name="b112" id="b112" class="graf graf--h3 graf-after--blockquote">A New Hope?</h3><figure name="1b54" id="1b54" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*j206w6dxISxonkrDVhlxIg.png" data-width="294" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*j206w6dxISxonkrDVhlxIg.png"></figure><p name="d628" id="d628" class="graf graf--p graf-after--figure">Artificial Intelligence is making progress every day. One can only wonder what is coming next!</p><p name="013a" id="013a" class="graf graf--p graf-after--p">Will machines learn how to understand humans — not the other way around?</p><p name="b3e3" id="b3e3" class="graf graf--p graf-after--p">Will they help humans understand each other?</p><p name="aa85" id="aa85" class="graf graf--p graf-after--p">Will they end up ruling the world?</p><p name="2cc2" id="2cc2" class="graf graf--p graf-after--p">Who knows?</p><p name="3c5a" id="3c5a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Whatever happens, these will be fascinating tales of a strange tomorrow.</strong></p><p name="ff4c" id="ff4c" class="graf graf--p graf-after--p graf--trailing"><em class="markup--em markup--p-em">Note: this is an edited transcript of one of my current keynote talks. Original slides are available </em><a href="https://www.slideshare.net/JulienSIMON5/fascinating-tales-of-a-strange-tomorrow-74449554" data-href="https://www.slideshare.net/JulienSIMON5/fascinating-tales-of-a-strange-tomorrow-74449554" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">here</em></a><em class="markup--em markup--p-em">.</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/72048639e754"><time class="dt-published" datetime="2017-04-05T13:30:38.358Z">April 5, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/fascinating-tales-of-a-strange-tomorrow-72048639e754" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
