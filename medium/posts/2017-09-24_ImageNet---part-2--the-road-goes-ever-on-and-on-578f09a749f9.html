<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>ImageNet — part 2: the road goes ever on and on</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">ImageNet — part 2: the road goes ever on and on</h1>
</header>
<section data-field="subtitle" class="p-summary">
In a previous post, we looked at what it took to download and prepare the ImageNet dataset. Now it’s time to train!
</section>
<section data-field="body" class="e-content">
<section name="3c2b" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fdfa" id="fdfa" class="graf graf--h3 graf--leading graf--title">ImageNet — part 2: the road goes ever on and on</h3><p name="1f24" id="1f24" class="graf graf--p graf-after--h3"><a href="https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure-c0a62976dc72" data-href="https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure-c0a62976dc72" class="markup--anchor markup--p-anchor" target="_blank">In a previous post</a>, we looked at what it took to download and prepare the <a href="http://www.image-net.org/" data-href="http://www.image-net.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> dataset. Now it’s time to train!</p><figure name="c3d8" id="c3d8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UofOMkW6S9tcOBme77EeXQ.jpeg" data-width="750" data-height="410" src="https://cdn-images-1.medium.com/max/800/1*UofOMkW6S9tcOBme77EeXQ.jpeg"><figcaption class="imageCaption">Can you see it, Mister Frodo? Our first ImageNet model! Oh wait, we have to cross Mordor first…</figcaption></figure><p name="5be2" id="5be2" class="graf graf--p graf-after--figure">The MXNet repository has a nice <a href="https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/train_imagenet.py" data-href="https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/train_imagenet.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">script</a>, let’s use it right away.</p><pre name="94ba" id="94ba" class="graf graf--pre graf-after--p">python train_imagenet.py --network resnet --num-layers 50 \<br>--gpus 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \<br>--data-train /data/im2rec/imagenet_training.rec \<br>--data-val /data/im2rec/imagenet_validation.rec</pre><p name="4e3d" id="4e3d" class="graf graf--p graf-after--pre">Easy enough. How fast is this running?</p><figure name="37a5" id="37a5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d6icdQ6QOk6x5iXsyKyTDA.png" data-width="814" data-height="172" src="https://cdn-images-1.medium.com/max/800/1*d6icdQ6QOk6x5iXsyKyTDA.png"></figure><p name="7620" id="7620" class="graf graf--p graf-after--figure">About <strong class="markup--strong markup--p-strong">400 images per second</strong>, which means about <strong class="markup--strong markup--p-strong">53 minutes per epoch</strong>. Over <strong class="markup--strong markup--p-strong">three and a half days for 100 epochs</strong>. Come on, we don’t want to wait this long. Think, think… Didn’t we read somewhere that a <strong class="markup--strong markup--p-strong">larger batch size</strong> will speed up training and help the model generalize better? Let’s figure this out :)</p><h4 name="ab1e" id="ab1e" class="graf graf--h4 graf-after--p">Picking the largest batch size</h4><p name="0023" id="0023" class="graf graf--p graf-after--h4">In this context, the largest batch size means the <strong class="markup--strong markup--p-strong">largest that will fit on one of our GPUs</strong>: each of them has <strong class="markup--strong markup--p-strong">11439MB</strong> of RAM.</p><p name="809a" id="809a" class="graf graf--p graf-after--p">Using the <em class="markup--em markup--p-em">nvidia-smi</em> command, we can see that the current training only uses about <strong class="markup--strong markup--p-strong">1500MB</strong>. As we didn’t pass a batch size parameter to our script, it’s using the default value of 128. That’s not efficient at all.</p><figure name="2b3f" id="2b3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pp8tJrL_-HSQ4VqDP7R4dQ.png" data-width="1128" data-height="714" src="https://cdn-images-1.medium.com/max/800/1*pp8tJrL_-HSQ4VqDP7R4dQ.png"></figure><p name="7f70" id="7f70" class="graf graf--p graf-after--figure">By trial and error, we can quickly figure out that the largest possible batch size is <strong class="markup--strong markup--p-strong">1408</strong>. Let’s give it a try.</p><pre name="2a87" id="2a87" class="graf graf--pre graf-after--p">python train_imagenet.py --network resnet --num-layers 50 \<br>--gpus 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \<br>--data-train /data/im2rec/imagenet_training.rec \<br>--data-val /data/im2rec/imagenet_validation.rec \<br>--batch-size 1408</pre><figure name="8327" id="8327" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*RN2OL90GbUnyL33Szj3NmQ.png" data-width="1132" data-height="712" src="https://cdn-images-1.medium.com/max/800/1*RN2OL90GbUnyL33Szj3NmQ.png"></figure><p name="8c39" id="8c39" class="graf graf--p graf-after--figure">That’s more like it: the GPU RAM is maxed out. Training speed should be much higher… right?</p><figure name="d423" id="d423" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jqMF6N7ZaiSK5UPIKYtQ7Q.png" data-width="816" data-height="172" src="https://cdn-images-1.medium.com/max/800/1*jqMF6N7ZaiSK5UPIKYtQ7Q.png"></figure><p name="d1c4" id="d1c4" class="graf graf--p graf-after--figure">Nope. Something is definitely not right. Let’s pop the hood.</p><h4 name="12b8" id="12b8" class="graf graf--h4 graf-after--p">Detecting stalled GPUs</h4><p name="213d" id="213d" class="graf graf--p graf-after--h4">GPU RAM is fully used, but what about the actual GPU cores? As it turns out, there’s an easy way to find out. Let’s look at the “volatile GPU information” returned by <em class="markup--em markup--p-em">nvidia-smi, </em>it’ll give us an idea on how hard GPUs actually work.</p><figure name="fc2e" id="fc2e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YmOv_Z-mIPmTWGFfDsxuCg.png" data-width="1124" data-height="1120" src="https://cdn-images-1.medium.com/max/800/1*YmOv_Z-mIPmTWGFfDsxuCg.png"></figure><p name="d2ca" id="d2ca" class="graf graf--p graf-after--figure">That’s not good. One second, our GPUs are running at 100% and the next they’re idle.</p><p name="ad10" id="ad10" class="graf graf--p graf-after--p">It looks like they’re stalling over and over, which probably means that we can’t maintain a fast enough stream of data to keep them busy all the time. Let’s take a look at our Python process…</p><h4 name="a0e7" id="a0e7" class="graf graf--h4 graf-after--p">Scaling the Python process</h4><p name="1ddc" id="1ddc" class="graf graf--p graf-after--h4">The RecordIO files storing the training set are hosted on an EBS volume (built from a snapshot, as explained before). Our Python script reads the images using a default value of 4 threads. Then, it performs data augmentation on them(resizing, changing aspect ratio, etc.) before feeding them to the GPUs. The bottleneck is probably in there.</p><figure name="429a" id="429a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5cYTS_KkanzQ4C2wGsuv_Q.png" data-width="1128" data-height="272" src="https://cdn-images-1.medium.com/max/800/1*5cYTS_KkanzQ4C2wGsuv_Q.png"></figure><p name="5999" id="5999" class="graf graf--p graf-after--figure">Idle time is extremely high (id=<strong class="markup--strong markup--p-strong">80.5%</strong>), but there are no I/O waits (wa=0%). It looks like this system is simply not working hard enough. The p2.16xlarge has 64 vCPUs, so let’s add more <strong class="markup--strong markup--p-strong">decoding threads</strong>.</p><pre name="9c91" id="9c91" class="graf graf--pre graf-after--p">python train_imagenet.py --network resnet --num-layers 50 \<br>--gpus 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \<br>--data-train /data/im2rec/imagenet_training.rec \<br>--data-val /data/im2rec/imagenet_validation.rec \<br>--batch-size 1408 --data-nthreads=32</pre><h4 name="781a" id="781a" class="graf graf--h4 graf-after--pre">Firing on all sixteen</h4><p name="33ba" id="33ba" class="graf graf--p graf-after--h4">Our GPUs look pretty busy now.</p><figure name="caa4" id="caa4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*bG8FI9uAsko5WRIfeXqYPQ.png" data-width="1114" data-height="1018" src="https://cdn-images-1.medium.com/max/800/1*bG8FI9uAsko5WRIfeXqYPQ.png"></figure><p name="c728" id="c728" class="graf graf--p graf-after--figure">What about our python process?</p><figure name="a912" id="a912" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jHTEWgToiEEz24yGzHRirA.png" data-width="1326" data-height="274" src="https://cdn-images-1.medium.com/max/800/1*jHTEWgToiEEz24yGzHRirA.png"></figure><p name="eaed" id="eaed" class="graf graf--p graf-after--figure">It’s working harder as well, with <strong class="markup--strong markup--p-strong">all 32 threads running in parallel</strong>. Still no I/O wait in sight, though (thank you EBS). In fact, we seem to have a nice safety margin when it comes to I/O: we could certainly add more threads to support a larger batch size or faster GPUs if we had them.</p><p name="4eb4" id="4eb4" class="graf graf--p graf-after--p">What about training speed? It’s nicely crusing at a stable <strong class="markup--strong markup--p-strong">700+ images per second</strong>. That’s a <strong class="markup--strong markup--p-strong">75%</strong> increase from where we started, so it sure was worth tweaking.</p><p name="b25d" id="b25d" class="graf graf--p graf-after--p">An epoch will complete in <strong class="markup--strong markup--p-strong">30 minutes</strong>, which gives us just a little over <strong class="markup--strong markup--p-strong">2 days for 100 epochs</strong>. Not too bad.</p><figure name="3d1e" id="3d1e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jA6ePyofWNBkyNB2KUjhVg.png" data-width="1148" data-height="336" src="https://cdn-images-1.medium.com/max/800/1*jA6ePyofWNBkyNB2KUjhVg.png"></figure><h4 name="e96c" id="e96c" class="graf graf--h4 graf-after--figure">Optimizing cost</h4><p name="5d9c" id="5d9c" class="graf graf--p graf-after--h4">At on-demand price in us-east-1, 50 hours of training would cost us <strong class="markup--strong markup--p-strong">$720</strong>. Ouch. Surely we can optimize this as well?</p><p name="2d38" id="2d38" class="graf graf--p graf-after--p">Let’s look at <a href="https://aws.amazon.com/ec2/spot/" data-href="https://aws.amazon.com/ec2/spot/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">spot prices</a> for the p2.16xlarge. They vary a lot from region to region, but here’s what I found in us-west-2 (hint: using the <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-price-history.html" data-href="https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-price-history.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">describe-spot-price</a> API should help you find good deals really quick).</p><figure name="3a98" id="3a98" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*meQBA-2CiIp8z8dvnwW-NA.png" data-width="1696" data-height="838" src="https://cdn-images-1.medium.com/max/800/1*meQBA-2CiIp8z8dvnwW-NA.png"></figure><p name="92ae" id="92ae" class="graf graf--p graf-after--figure">Yes, ladies and gentlemen. That’s an <strong class="markup--strong markup--p-strong">89% discount</strong> right there. Training would now cost something like <strong class="markup--strong markup--p-strong">$80</strong>.</p><h4 name="c93c" id="c93c" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="8be9" id="8be9" class="graf graf--p graf-after--h4">As you can see, there is much more to Deep Learning than data sets and models. Making sure that your infrastructure runs at <strong class="markup--strong markup--p-strong">full capacity</strong> and at the <strong class="markup--strong markup--p-strong">best possible price</strong> also requires some attention. Hopefully, this post will help you do both.</p><p name="de8f" id="de8f" class="graf graf--p graf-after--p">In the next post, I think we’ll look at training ImageNet with Keras, but I’m not quite sure yet :D</p><p name="b732" id="b732" class="graf graf--p graf-after--p graf--trailing">As always, thank you for reading.</p></div></div></section><section name="d9b7" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e6cc" id="e6cc" class="graf graf--p graf--leading graf--trailing"><em class="markup--em markup--p-em">Congratulations if you caught the Bilbo </em><a href="https://en.wikipedia.org/wiki/The_Road_Goes_Ever_On_%28song%29" data-href="https://en.wikipedia.org/wiki/The_Road_Goes_Ever_On_(song)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">reference</em></a><em class="markup--em markup--p-em"> in the title. You’re a proper Tolkien nerd ;)</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/578f09a749f9"><time class="dt-published" datetime="2017-09-24T18:11:09.854Z">September 24, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/imagenet-part-2-the-road-goes-ever-on-and-on-578f09a749f9" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
