<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Optimize the prediction latency of Transformers with a single Docker command!</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Optimize the prediction latency of Transformers with a single Docker command!</h1>
</header>
<section data-field="subtitle" class="p-summary">
Transformer models are great. Still, they’re large models, and prediction latency can be a problem. This is the problem that Hugging Face…
</section>
<section data-field="body" class="e-content">
<section name="764d" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6403" id="6403" class="graf graf--h3 graf--leading graf--title">Optimize the prediction latency of Transformers with a single Docker command!</h3><p name="d3ac" id="d3ac" class="graf graf--p graf-after--h3">Transformer models are great. Still, they’re large models, and prediction latency can be a problem. This is the problem that <a href="https://huggingface.co/infinity" data-href="https://huggingface.co/infinity" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Infinity</a> solves with a single Docker command.</p><p name="76c5" id="76c5" class="graf graf--p graf-after--p">In this video, I start from a pre-trained model hosted on the Hugging Face hub. Using an AWS CPU instance based on the Intel Ice Lake architecture (c6i.xlarge), I optimize my model using the Infinity Multiverse Docker container.</p><p name="cc13" id="cc13" class="graf graf--p graf-after--p">Then, I push the model back to the Hugging Face hub, and I deploy it on a prediction API running in an Infinity container on my AWS instance.</p><p name="4032" id="4032" class="graf graf--p graf-after--p">Finally, I predict with the optimized model and get a 5x speedup compared to the original model.</p><figure name="22ea" id="22ea" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/ngB8VBbVRd4?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="a946" id="a946" class="graf graf--p graf-after--figure">Original model: <a href="https://huggingface.co/juliensimon/autonlp-imdb-demo-hf-16622767" data-href="https://huggingface.co/juliensimon/autonlp-imdb-demo-hf-16622767" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://huggingface.co/juliensimon/autonlp-imdb-demo-hf-16622767</a></p><p name="953d" id="953d" class="graf graf--p graf-after--p">Code: <a href="https://huggingface.co/juliensimon/imdb-demo-infinity/tree/main/code" data-href="https://huggingface.co/juliensimon/imdb-demo-infinity/tree/main/code" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://huggingface.co/juliensimon/imdb-demo-infinity/tree/main/code</a></p><p name="625d" id="625d" class="graf graf--p graf-after--p graf--trailing">New to Transformers? Check out the Hugging Face course at <a href="https://huggingface.co/course" data-href="https://huggingface.co/course" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://huggingface.co/course</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/cebac9b9c91b"><time class="dt-published" datetime="2021-11-05T14:07:37.954Z">November 5, 2021</time></a>.</p><p><a href="https://medium.com/@julsimon/optimize-the-prediction-latency-of-transformers-with-a-single-docker-command-cebac9b9c91b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
