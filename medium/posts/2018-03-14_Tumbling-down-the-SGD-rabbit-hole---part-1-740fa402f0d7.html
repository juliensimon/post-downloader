<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Tumbling down the SGD rabbit hole — part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Tumbling down the SGD rabbit hole — part 1</h1>
</header>
<section data-field="subtitle" class="p-summary">
As could be feared, my Deep Learning excursions have led me in increasingly strange places. Lately, I’ve become quite obsessed with…
</section>
<section data-field="body" class="e-content">
<section name="3e3e" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="16c8" id="16c8" class="graf graf--h3 graf--leading graf--title">Tumbling down the SGD rabbit hole — part 1</h3><p name="7934" id="7934" class="graf graf--p graf-after--h3">As could be feared, my Deep Learning excursions have led me in increasingly strange places. Lately, I’ve become quite obsessed with optimizers, maybe the single most important factor in successfully training a Deep Learning model.</p><p name="c189" id="c189" class="graf graf--p graf-after--p">This post assumes a basic knowledge of Deep Learning (if you’re new to the topic, <a href="https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459" data-href="https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459" class="markup--anchor markup--p-anchor" target="_blank">this previous post</a> is probably a better one to read).</p><p name="ce17" id="ce17" class="graf graf--p graf-after--p">We shall start with a basic explanation of <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" data-href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Stochatic Gradient Descent</a>, the most commonly used optimization function used in Deep Learning. And then, we’ll fall into darkness :D</p><figure name="0532" id="0532" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*M6qZCkH6Y2BJg2trmnSflg.png" data-width="720" data-height="576" src="https://cdn-images-1.medium.com/max/800/1*M6qZCkH6Y2BJg2trmnSflg.png"><figcaption class="imageCaption">It’s a long way down</figcaption></figure><h3 name="073a" id="073a" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What Deep Learning really is</strong></h3><p name="2e6d" id="2e6d" class="graf graf--p graf-after--h3">Deep Learning boils down to:</p><ul class="postList"><li name="4820" id="4820" class="graf graf--li graf-after--p">building a <strong class="markup--strong markup--li-strong">data set</strong>,</li><li name="130b" id="130b" class="graf graf--li graf-after--li">designing a complicated <strong class="markup--strong markup--li-strong">function</strong> with a l<strong class="markup--strong markup--li-strong">arge number of parameters</strong> (the neural network),</li><li name="59a3" id="59a3" class="graf graf--li graf-after--li">using the data set, trying to <strong class="markup--strong markup--li-strong">find a set of parameters</strong> yielding a level of <strong class="markup--strong markup--li-strong">accuracy</strong> in line with our expectations.</li></ul><p name="b2bf" id="b2bf" class="graf graf--p graf-after--li">That’s it. Zooming in on the last item, what we’re really trying to achieve is find a set of parameters for which the <strong class="markup--strong markup--p-strong">loss function</strong> (which computes the difference between ground truth and predicted values) reaches <strong class="markup--strong markup--p-strong">a small enough value</strong>.</p><p name="24ee" id="24ee" class="graf graf--p graf-after--p">This set of parameters cannot be computed algebraically: it has to be <strong class="markup--strong markup--p-strong">approximated</strong> iteratively using an <strong class="markup--strong markup--p-strong">optimization function</strong>.</p><blockquote name="c47d" id="c47d" class="graf graf--blockquote graf-after--p">As <a href="http://ruder.io" data-href="http://ruder.io" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Sebastian Ruder</a> puts it: “« Deep Learning ultimately is about finding a minimum that generalizes well, with bonus points for finding one fast and reliably ». Indeed, we’re not necessarily looking for the absolute minimum, just for one that would generalize “well enough” to solve our business problem.</blockquote><h4 name="01d8" id="01d8" class="graf graf--h4 graf-after--blockquote">The SGD optimizer</h4><p name="aa0c" id="aa0c" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Stochastic Gradient Descent</strong> was invented in 1951 by Robbins and Monro. It’s still very widely used today. All it requires is the ability to <strong class="markup--strong markup--p-strong">derive</strong> a function, plus a single <strong class="markup--strong markup--p-strong">hyper-parameter</strong> called the <strong class="markup--strong markup--p-strong">learning rate</strong> (noted ‘lr’ below: 0.1 is a typical value).</p><p name="7959" id="7959" class="graf graf--p graf-after--p">Here’s the SGD update rule when the function has a single parameter called ‘w’: <strong class="markup--strong markup--p-strong">w = w-(lr*f’(w))</strong></p><p name="429f" id="429f" class="graf graf--p graf-after--p">In plain-speak: we compute the <strong class="markup--strong markup--p-strong">function derivative at our present location</strong>. This tells us what the <strong class="markup--strong markup--p-strong">slope</strong> is and which way is <strong class="markup--strong markup--p-strong">down</strong>. Accordingly, we then take <strong class="markup--strong markup--p-strong">a small step down</strong> and <strong class="markup--strong markup--p-strong">repeat</strong> until we get to the <strong class="markup--strong markup--p-strong">minimum</strong> value for f().</p><p name="f3cb" id="f3cb" class="graf graf--p graf-after--p">An example is the best way to explain it :)</p><h4 name="ee6f" id="ee6f" class="graf graf--h4 graf-after--p">SGD with a single parameter</h4><p name="07f5" id="07f5" class="graf graf--p graf-after--h4">Let’s apply this to a very simple function : <strong class="markup--strong markup--p-strong">f(x) = x²</strong>. I hope that you’ll remember from high-school that its derivative is <strong class="markup--strong markup--p-strong">2x</strong> ;)</p><p name="9cfa" id="9cfa" class="graf graf--p graf-after--p">At x=1 (our starting point for this example), the derivative is equal to 2. The slope is positive, which means that if we have to decrease x to get closer to the minimum (If the slope was negative, we’d increase x to get closer to the minimum)</p><blockquote name="0668" id="0668" class="graf graf--blockquote graf-after--p">Reember, SGD updates parameters in the <strong class="markup--strong markup--blockquote-strong">reverse direction of the slope.</strong></blockquote><p name="5de0" id="5de0" class="graf graf--p graf-after--blockquote">Let’s apply SGD with a learning rate of 0.25: x = x-(0.25*2) = 0.5.</p><p name="e6e7" id="e6e7" class="graf graf--p graf-after--p">For x=0.5, the derivative is equal to 1. Let’s run another round of SGD:<br>x = x-(0.25*1) =0.25.</p><p name="d152" id="d152" class="graf graf--p graf-after--p">After a few iterations, this is what we get.</p><figure name="bd2e" id="bd2e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zbh5tD6IArRFTjFcxjZVlQ.png" data-width="420" data-height="528" src="https://cdn-images-1.medium.com/max/800/1*zbh5tD6IArRFTjFcxjZVlQ.png"></figure><p name="e04c" id="e04c" class="graf graf--p graf-after--figure">As you can see, we gradually get closer and closer to the function’s minimum, i.e. x=0. SGD works, woohoo.</p><blockquote name="efc9" id="efc9" class="graf graf--blockquote graf-after--p">…unless we pick a very large value — causing divergence — or a very small value — causing intolerably slow convergence. “Neither too small nor too large” seems to be the mantra of Deep Learning practitioners :)</blockquote><h4 name="7367" id="7367" class="graf graf--h4 graf-after--blockquote">SGD with many parameters</h4><p name="529e" id="529e" class="graf graf--p graf-after--h4">In a deep neural networks, <strong class="markup--strong markup--p-strong">each weight is a parameter</strong>. As a consequence, the network’s function has as many dimensions as the network has weights… which could be <strong class="markup--strong markup--p-strong">millions</strong> and even <strong class="markup--strong markup--p-strong">tens of millions</strong>. Mind-boggling.</p><p name="1455" id="1455" class="graf graf--p graf-after--p">Still, SGD works the same. However, for additional performance, we don’t want to update weights one at a time: during the training phase, <a href="https://en.wikipedia.org/wiki/Backpropagation" data-href="https://en.wikipedia.org/wiki/Backpropagation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">backpropagation</a> should instead update all the weights in a given layer at once (starting from the last layer and moving on to the previous one until it reaches the input layer).</p><p name="5296" id="5296" class="graf graf--p graf-after--p">To do so, let’s define a <strong class="markup--strong markup--p-strong">vector storing the weights</strong> for this layer: <br><strong class="markup--strong markup--p-strong">L = [w1, w2, …]</strong></p><p name="1b0c" id="1b0c" class="graf graf--p graf-after--p">Similarly, let’s define a <strong class="markup--strong markup--p-strong">vector storing the partial derivatives</strong> of our function. This vector is called the <strong class="markup--strong markup--p-strong">gradient</strong> (yes, that’s the ‘G’ in ‘SGD): it informs us about the slope in each dimension.</p><p name="8780" id="8780" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">grad(f)(L) = [(df/dW1)(w1), (df/dW2)(w2), … ]</strong></p><p name="2feb" id="2feb" class="graf graf--p graf-after--p">Fortunately, Deep Learning libraries like Apache MXNet or TensorFlow know how to compute partial derivatives automatically, so <strong class="markup--strong markup--p-strong">we’ll never have to do that ourselves</strong>. Pfeew.</p><p name="20ff" id="20ff" class="graf graf--p graf-after--p">The SGD update now becomes: <strong class="markup--strong markup--p-strong">L = L-(lr*grad(f)(L))</strong></p><p name="31d0" id="31d0" class="graf graf--p graf-after--p">Here’s a simulated example with two parameters, where <strong class="markup--strong markup--p-strong">we literally “walk down the mountain” step by step to reach a low point in the “valley”</strong>.</p><figure name="342a" id="342a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*i_CKvkSzVvjWuX6M6zPxYw.png" data-width="936" data-height="748" src="https://cdn-images-1.medium.com/max/800/1*i_CKvkSzVvjWuX6M6zPxYw.png"><figcaption class="imageCaption">z=x²- xy. Generated with love at academo.org.</figcaption></figure><p name="986d" id="986d" class="graf graf--p graf-after--figure">See? It’s not that complicated: you’ve now conquered SGD. No matter the number of dimensions, we have a <strong class="markup--strong markup--p-strong">simple iterative algorithm that lets us find the smallest value possible for our function</strong>.</p><p name="d660" id="d660" class="graf graf--p graf-after--p">Does this <strong class="markup--strong markup--p-strong">really</strong> work every single time? Well… no :)</p><h3 name="7b51" id="7b51" class="graf graf--h3 graf-after--p">Problem #1: local minima</h3><p name="3440" id="3440" class="graf graf--p graf-after--h3">A function may exhibit many <strong class="markup--strong markup--p-strong">local minima</strong>. Look at the weird beast below.</p><figure name="e5bb" id="e5bb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*v4qVjlBngKhd1BHGcNa6wA.png" data-width="1248" data-height="588" src="https://cdn-images-1.medium.com/max/800/1*v4qVjlBngKhd1BHGcNa6wA.png"><figcaption class="imageCaption">z=(cos(x)/x)*(sin(y)/y). Generated with love at academo.org.</figcaption></figure><p name="00b0" id="00b0" class="graf graf--p graf-after--figure">Lots of <strong class="markup--strong markup--p-strong">very shallow local minima</strong>, some <strong class="markup--strong markup--p-strong">deeper</strong> ones and a <strong class="markup--strong markup--p-strong">global</strong> one. As we iterate on SGD, we’d definitely want to avoid getting stuck in any of the shallow ones — this could happen if we used a really small learning rate, preventing us from crawling out. Ideally, we’d like to end up that global minimum, but we could end up falling into a deep local one.</p><p name="799d" id="799d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Is this a theoretical problem? Do these situations actually take place with deep neural networks? If so, would there be any way to catapult ourselves out of these local minima?</strong></p><h3 name="00e2" id="00e2" class="graf graf--h3 graf-after--p">Problem #2: slow convergence</h3><p name="e426" id="e426" class="graf graf--p graf-after--h3">Imagine a part of the parameter space where the slope would be <strong class="markup--strong markup--p-strong">close to zero in every dimension</strong>. Let’s call this a plateau, even if the word doesn’t really make sense in high-dimension spaces.</p><p name="a76d" id="a76d" class="graf graf--p graf-after--p">There, all components of the <strong class="markup--strong markup--p-strong">gradient</strong> would be close to <strong class="markup--strong markup--p-strong">zero</strong>, right? Hardly any slope. The consequence would be <strong class="markup--strong markup--p-strong">near-zero updates for all weights</strong>, which in turn would mean that we would <strong class="markup--strong markup--p-strong">hardly</strong> move towards the minimum. We’d be <strong class="markup--strong markup--p-strong">stuck</strong> on that plateau for a long time and training would be extremely <strong class="markup--strong markup--p-strong">slow</strong>, no matter how much hardware we’d throw at it. Definitely an undesirable situation (unless we’d reached a nice minimum, of course).</p><p name="a4bd" id="a4bd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Could we speed up, i.e. increase the learning rate when slope is minimal?</strong></p><h3 name="89e4" id="89e4" class="graf graf--h3 graf-after--p">Problem #3: different slopes</h3><p name="b846" id="b846" class="graf graf--p graf-after--h3">Should we really expect all dimensions to have <strong class="markup--strong markup--p-strong">roughly the same slope</strong>? Or could there be <strong class="markup--strong markup--p-strong">steep dimensions</strong> — where we’d make good progress quickly — and <strong class="markup--strong markup--p-strong">flatter dimension</strong>s — where we’d move much slower?</p><p name="0b47" id="0b47" class="graf graf--p graf-after--p">Surely that’s a reasonable hypothesis. As SGD uses the same learning rate for all parameters, this would surely cause <strong class="markup--strong markup--p-strong">uneven progress</strong> and slow down the training process.</p><p name="7877" id="7877" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Could we have adapt the learning rate to the slope of each dimension?</strong></p><h3 name="d0d5" id="d0d5" class="graf graf--h3 graf-after--p">Problem #4: saddle points</h3><p name="122a" id="122a" class="graf graf--p graf-after--h3">Now, imagine we’d reach a specific point in the parameter space where <strong class="markup--strong markup--p-strong">all components of the gradient are actually equal to zero </strong>(yes, these points actually exist, more on this in a minute). What would happen then? <strong class="markup--strong markup--p-strong">No more weight updates</strong>! We’d be stuck there for all eternity. Doom on us.</p><blockquote name="c812" id="c812" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--pullquote-strong">“There are more things</strong> in heaven and earth, Horatio, than are dreamt of in your philosophy”</blockquote><h4 name="d466" id="d466" class="graf graf--h4 graf-after--pullquote">Defeating the gradient</h4><p name="61af" id="61af" class="graf graf--p graf-after--h4">Let’s look at an example: <strong class="markup--strong markup--p-strong">f(x,y) = x²- y²</strong>. This does look like a <strong class="markup--strong markup--p-strong">horse saddle</strong>, doesn’t it?</p><figure name="368e" id="368e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dRMHui7T9r_pbzCUZT6Apg.png" data-width="1144" data-height="710" src="https://cdn-images-1.medium.com/max/800/1*dRMHui7T9r_pbzCUZT6Apg.png"><figcaption class="imageCaption">The horse saddle. Generated with love at academo.org.</figcaption></figure><p name="9607" id="9607" class="graf graf--p graf-after--figure">Now, let’s compute the <strong class="markup--strong markup--p-strong">partial derivatives</strong>: df/dx = 2x, df/dy = -2y. Hence, the <strong class="markup--strong markup--p-strong">gradient</strong> of this function is <strong class="markup--strong markup--p-strong">[2x, -2y]</strong>. Obviously, at the point (0,0), both components of the gradient are equal to zero.</p><p name="afc2" id="afc2" class="graf graf--p graf-after--p">Looking at the graph above, indeed we see that this point is a <strong class="markup--strong markup--p-strong">minimum along the x axis</strong> and a <strong class="markup--strong markup--p-strong">maximum along the y axis</strong>. Such points are called <strong class="markup--strong markup--p-strong">saddle points</strong>: a minimum or a maximum for every dimension.</p><p name="63b7" id="63b7" class="graf graf--p graf-after--p">Should SGD lead us to this exact place, our training process would be toast as <strong class="markup--strong markup--p-strong">weights would not be updated any longer</strong>. Our model would probably be no good either since we’d definitely not have reached a minimum value.</p><p name="1756" id="1756" class="graf graf--p graf-after--p">Saddle points have been proven to be very common in Deep Learning optimization problems (we’ll see in the next post how to try and limit their occurence).For now, let’s see if we can break out!</p><h4 name="9275" id="9275" class="graf graf--h4 graf-after--p">Enter the Hessian</h4><p name="7f2b" id="7f2b" class="graf graf--p graf-after--h4">Our problem here is that we’re only looking at <strong class="markup--strong markup--p-strong">unidirectional slopes</strong>. Instead, if we looked at the <strong class="markup--strong markup--p-strong">curvature</strong> around the saddle point (i.e. how much the surface deviates from a plane), then maybe we’d figure out that there’s actually a way down along th y axis.</p><p name="4f34" id="4f34" class="graf graf--p graf-after--p">Studying the curvature of a function requires the computation of <strong class="markup--strong markup--p-strong">second-order partial derivatives</strong> (ah come on, don’t quit on me now. Hang on!). They’re stored in a square matrix called the <strong class="markup--strong markup--p-strong">Hessian</strong>.</p><p name="b3cb" id="b3cb" class="graf graf--p graf-after--p">Let’s do this for our previous example:</p><ul class="postList"><li name="d14b" id="d14b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">d²f/dx² = 2, d²f/dxdy = 0</strong></li><li name="563f" id="563f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">d²f/dydx = 0, d²f/dy²= -2</strong></li></ul><p name="8093" id="8093" class="graf graf--p graf-after--li">Thus, the Hessian for our function is:</p><figure name="1465" id="1465" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nUR3lIOwL8YA6uKOC5qcPQ.png" data-width="462" data-height="164" src="https://cdn-images-1.medium.com/max/800/1*nUR3lIOwL8YA6uKOC5qcPQ.png"></figure><p name="effb" id="effb" class="graf graf--p graf-after--figure">By multiplying this matrix with <strong class="markup--strong markup--p-strong">unit vectors along the x and y axes</strong>, we’re going to find out what the <strong class="markup--strong markup--p-strong">curvature</strong> looks like:</p><ul class="postList"><li name="8d7d" id="8d7d" class="graf graf--li graf-after--p">H * [0, 1] = [0 -2] = <strong class="markup--strong markup--li-strong">-2</strong>*[0, 1]</li><li name="d947" id="d947" class="graf graf--li graf-after--li">H * [0, -1] = [0 2] = <strong class="markup--strong markup--li-strong">-2</strong>*[0, -1]</li><li name="5189" id="5189" class="graf graf--li graf-after--li">H * [1, 0] = [2 0] = <strong class="markup--strong markup--li-strong">2</strong>*[1, 0]</li><li name="0bb4" id="0bb4" class="graf graf--li graf-after--li">H * [ -1, 0] = [-2 0] = <strong class="markup--strong markup--li-strong">2</strong>*[-1, 0]</li></ul><p name="9bde" id="9bde" class="graf graf--p graf-after--li">What do we see here? Multiplying H by a unit vector along the x axis yields a <strong class="markup--strong markup--p-strong">positive multiple of the vector</strong>: curvature is <strong class="markup--strong markup--p-strong">positive</strong>, we can only go <strong class="markup--strong markup--p-strong">up</strong>. Indeed, (0,0) is a minimum along the x axis.</p><p name="4f77" id="4f77" class="graf graf--p graf-after--p">On the contrary, multiplying H by a unit vector along the y axis yields a <strong class="markup--strong markup--p-strong">negative multiple of the vector</strong>. This indicates a <strong class="markup--strong markup--p-strong">negative</strong> curvature, which means that there is a way <strong class="markup--strong markup--p-strong">down</strong>. Victory!</p><blockquote name="10ba" id="10ba" class="graf graf--blockquote graf-after--p">A little more algebra (hate me, I don’t care): when a non-zero vector v and a square matrix M are such that M*v is a multiple of v, the vector v is called an <strong class="markup--strong markup--blockquote-strong">eigenvector</strong> of M and the multiple is called an <strong class="markup--strong markup--blockquote-strong">eigenvalue</strong>. So, whenever we encounter a saddle point, <strong class="markup--strong markup--blockquote-strong">computing the Hessian and looking for a negative eigenvalue is how we can figure out which way is down</strong>. Ain’t life grand?</blockquote><p name="4230" id="4230" class="graf graf--p graf-after--blockquote">Now we know how to break out of saddle points. The only problem is that <strong class="markup--strong markup--p-strong">computing the Hessian is quite expensive</strong>. There are other ways to avoid saddle points, albeit less rigorous ones (more on this in the next post).</p><p name="dacf" id="dacf" class="graf graf--p graf-after--p">Are we out of the woods when it comes to saddle points? Far from it, I’m afraid.</p><blockquote name="1768" id="1768" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p">“Hell is empty and all the devils are here.”</blockquote><h4 name="dc69" id="dc69" class="graf graf--h4 graf-after--pullquote">Defeating the Hessian</h4><p name="0f59" id="0f59" class="graf graf--p graf-after--h4">Here’s an example, called the monkey saddle: <strong class="markup--strong markup--p-strong">f(x,y) = x³- 3xy².</strong></p><figure name="a780" id="a780" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sTVNBIoLkaWoYxIVWTj2EQ.png" data-width="1158" data-height="776" src="https://cdn-images-1.medium.com/max/800/1*sTVNBIoLkaWoYxIVWTj2EQ.png"><figcaption class="imageCaption">The monkey saddle. Generated with love at academo.org.</figcaption></figure><p name="6575" id="6575" class="graf graf--p graf-after--figure">Let’s compute the <strong class="markup--strong markup--p-strong">gradient</strong> and the <strong class="markup--strong markup--p-strong">Hessian </strong>again<strong class="markup--strong markup--p-strong">.</strong></p><p name="f584" id="f584" class="graf graf--p graf-after--p">The gradient is <strong class="markup--strong markup--p-strong">[3x²- 3y², -6xy]</strong>, which is equal to [0, 0] at point (0,0). This is a saddle point again.</p><p name="4758" id="4758" class="graf graf--p graf-after--p">Accordingly, the Hessian is:</p><figure name="c502" id="c502" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lltWEcReoEICt6tMHmzBqw.png" data-width="802" data-height="194" src="https://cdn-images-1.medium.com/max/800/1*lltWEcReoEICt6tMHmzBqw.png"></figure><p name="16fa" id="16fa" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">H(0,0) is a zero matrix!</strong> Multiplying it by a unit vector (or any vector, for that matter) will result in a zero vector: <strong class="markup--strong markup--p-strong">we’re unable to figure out curvature</strong>. Neither the gradient nor the Hessian provide any information on which way is down: such points are <strong class="markup--strong markup--p-strong">degenerate saddle points</strong>. Oh, the agony…</p><p name="4464" id="4464" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Should we lose all hope? Are there more advanced techniques to detect these conditions?</strong></p><h3 name="f517" id="f517" class="graf graf--h3 graf-after--p">Problem #5: gradient size &amp; distributed training</h3><p name="e616" id="e616" class="graf graf--p graf-after--h3">This last problem is different from the previous ones. Imagine that we’re working with a <strong class="markup--strong markup--p-strong">very large data set</strong>: <strong class="markup--strong markup--p-strong">distributing training</strong> — splitting computation across multiple instances — would surely deliver a nice speed up.</p><p name="d5ed" id="d5ed" class="graf graf--p graf-after--p">The way this typically works is quite straightforward. Each instance in the training cluster grabs a <strong class="markup--strong markup--p-strong">batch</strong> of samples, processes it (forward propagation to compute the loss for the batch, then backprogation to compute gradients for all layers). Then, each instance would <strong class="markup--strong markup--p-strong">send the computed gradients for this batch</strong> to all other instances (or to a central location in charge of propagating them). Each instance would then <strong class="markup--strong markup--p-strong">update their weights</strong> accordingly.</p><p name="c0a8" id="c0a8" class="graf graf--p graf-after--p">All good, right? Not quite. For large models, gradients are actually very heavy: <strong class="markup--strong markup--p-strong">close to 100MB for ResNet-50</strong>, a popular image classification model. This means that <strong class="markup--strong markup--p-strong">each instance would have to send 100MB to all other instances after each round of backpropagation!</strong> Even with a limited number of instances, this could quickly become a <strong class="markup--strong markup--p-strong">performance bottleneck</strong>, stalling our computing efforts and slowing down the training process.</p><p name="19c3" id="19c3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Could we reduce the gradient size? Compress this data, maybe?</strong></p><h3 name="8fa8" id="8fa8" class="graf graf--h3 graf-after--p">Now what?</h3><p name="1721" id="1721" class="graf graf--p graf-after--h3">In this post, we looked at the <strong class="markup--strong markup--p-strong">mathematical foundation of SGD</strong>. We learned about <strong class="markup--strong markup--p-strong">mathematical tools</strong> that help us find which way is down in the quest for a nice minimum. This was a bit math-heavy, but hopefully you made it to the end. Remember, <strong class="markup--strong markup--p-strong">you can do this</strong>! Stephen Hawking passed away yesterday and he once said:</p><blockquote name="17b5" id="17b5" class="graf graf--pullquote graf-after--p">Equations are just the boring part of mathematics. I attempt to see things in terms of geometry.</blockquote><p name="2773" id="2773" class="graf graf--p graf-after--pullquote">I feel that this is great advice when trying to understand Deep Learning.</p><p name="c4b8" id="c4b8" class="graf graf--p graf-after--p"><a href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-2-bed3be4761d3" data-href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-2-bed3be4761d3" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">In the next post</strong></a><strong class="markup--strong markup--p-strong">, we’ll look at solutions for the issues discussed above…and how to apply them in practice.</strong></p><p name="8492" id="8492" class="graf graf--p graf-after--p graf--trailing">Thank you for reading.</p></div></div></section><section name="0f7c" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="0b63" id="0b63" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">In search of the global minimum, we might have to break out from the bottom of the well indeed \m/</em></p><figure name="3bdc" id="3bdc" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/2zxsAVCGOZE?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><figure name="c0a7" id="c0a7" class="graf graf--figure graf--iframe graf-after--figure"><iframe src="https://upscri.be/8f5f8b?as_embed=true" width="700" height="350" frameborder="0" scrolling="no"></iframe></figure></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="3"><figure name="02dc" id="02dc" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 33.333%;"><a href="https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c" data-href="https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*2f7OqE2AJK1KSrhkmD9ZMw.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*2f7OqE2AJK1KSrhkmD9ZMw.png"></a></figure><figure name="775a" id="775a" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.333%;"><a href="https://upscri.be/8f5f8b" data-href="https://upscri.be/8f5f8b" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*v-PpfkSWHbvlWWamSVHHWg.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*v-PpfkSWHbvlWWamSVHHWg.png"></a></figure><figure name="54ed" id="54ed" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure graf--trailing" style="width: 33.333%;"><a href="https://becominghuman.ai/write-for-us-48270209de63" data-href="https://becominghuman.ai/write-for-us-48270209de63" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="1*Wt2auqISiEAOZxJ-I7brDQ.png" data-width="255" data-height="170" src="https://cdn-images-1.medium.com/max/400/1*Wt2auqISiEAOZxJ-I7brDQ.png"></a></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/740fa402f0d7"><time class="dt-published" datetime="2018-03-14T10:04:30.308Z">March 14, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>