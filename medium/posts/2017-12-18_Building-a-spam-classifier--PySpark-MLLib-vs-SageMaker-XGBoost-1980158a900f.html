<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Building a spam classifier: PySpark+MLLib vs SageMaker+XGBoost</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Building a spam classifier: PySpark+MLLib vs SageMaker+XGBoost</h1>
</header>
<section data-field="subtitle" class="p-summary">
In this article, I will first show you how to build a spam classifier using Apache Spark, its Python API (aka PySpark) and a variety of…
</section>
<section data-field="body" class="e-content">
<section name="c54a" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9afb" id="9afb" class="graf graf--h3 graf--leading graf--title">Building a spam classifier: PySpark+MLLib vs SageMaker+XGBoost</h3><p name="2b08" id="2b08" class="graf graf--p graf-after--h3">In this article, I will first show you how to build a <strong class="markup--strong markup--p-strong">spam classifier</strong> using <a href="https://spark.apache.org/" data-href="https://spark.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Apache Spark</strong></a>, its Python API (aka PySpark) and a variety of Machine Learning algorithms implemented in <a href="https://spark.apache.org/mllib/" data-href="https://spark.apache.org/mllib/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Spark MLLib</strong></a>.</p><p name="bee9" id="bee9" class="graf graf--p graf-after--p">Then, we will use the new <a href="https://aws.amazon.com/blogs/aws/sagemaker/" data-href="https://aws.amazon.com/blogs/aws/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon Sagemaker</strong></a> service to train, save and deploy an <strong class="markup--strong markup--p-strong">XGBoost</strong> model trained on the same data set.</p><figure name="6348" id="6348" class="graf graf--figure graf--startsWithDoubleQuote graf-after--p"><img class="graf-image" data-image-id="1*uATxLSXT1WoAyqEp0uX-Wg.jpeg" data-width="1920" data-height="1080" src="https://cdn-images-1.medium.com/max/800/1*uATxLSXT1WoAyqEp0uX-Wg.jpeg"><figcaption class="imageCaption">“I must break you”</figcaption></figure><p name="60da" id="60da" class="graf graf--p graf-after--figure graf--trailing">All code runs in a Jupyter notebook, available on <a href="https://github.com/juliensimon/dlnotebooks/tree/master/spark" data-href="https://github.com/juliensimon/dlnotebooks/tree/master/spark" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a> :)</p></div></div></section><section name="5fb0" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="bf39" id="bf39" class="graf graf--h3 graf--leading">PySpark + MLLib</h3><h4 name="bca4" id="bca4" class="graf graf--h4 graf-after--h3">The big picture</h4><p name="849e" id="849e" class="graf graf--p graf-after--h4">Our raw data set is composed of <strong class="markup--strong markup--p-strong">1-line messages</strong> stored in <strong class="markup--strong markup--p-strong">two files</strong>:</p><ul class="postList"><li name="39c2" id="39c2" class="graf graf--li graf-after--p">the ‘<a href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/ham" data-href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/ham" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">ham</a>’ file: 4827 valid messages,</li><li name="023b" id="023b" class="graf graf--li graf-after--li">the ‘<a href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/spam" data-href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/spam" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">spam</a>’ file: 747 messages.</li></ul><p name="cbd6" id="cbd6" class="graf graf--p graf-after--li">In order to classify these messages, we need to build an intermediate data set with <strong class="markup--strong markup--p-strong">two classes</strong>. For this purpose, we’re going to use a simple but efficient technique called <a href="https://en.wikipedia.org/wiki/Feature_hashing" data-href="https://en.wikipedia.org/wiki/Feature_hashing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Feature Hashing</strong></a>:</p><ul class="postList"><li name="b7b7" id="b7b7" class="graf graf--li graf-after--p">For each message in the data set, we first <strong class="markup--strong markup--li-strong">hash</strong> its words into a <strong class="markup--strong markup--li-strong">fixed</strong> number of buckets (say, 1000).</li><li name="b08b" id="b08b" class="graf graf--li graf-after--li">Then, we build a <strong class="markup--strong markup--li-strong">vector</strong> indicating non-zero occurrences for each word: these are the <strong class="markup--strong markup--li-strong">features</strong> that will be used to decide whether a message is spam or not.</li><li name="6b8e" id="6b8e" class="graf graf--li graf-after--li">For a valid message, the corresponding <strong class="markup--strong markup--li-strong">label</strong> will be zero, i.e. the message is not spam. Accordingly, for a spam message, the label will be one.</li></ul><p name="32ce" id="32ce" class="graf graf--p graf-after--li">Once we’re done, our intermediate data set will be:</p><ul class="postList"><li name="0985" id="0985" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">4827 word vectors labeled with a zero</strong>,</li><li name="ebeb" id="ebeb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">747 word vectors labeled with a one</strong>.</li></ul><p name="ed68" id="ed68" class="graf graf--p graf-after--li">We’ll split it <strong class="markup--strong markup--p-strong">80/20</strong> for training and validation and run in through a number of <strong class="markup--strong markup--p-strong">classification</strong> algorithms.</p><p name="e0cb" id="e0cb" class="graf graf--p graf-after--p">For prediction, the process will be similar: hash the message, send the word vector to the model and get the predicted result.</p><p name="d1d6" id="d1d6" class="graf graf--p graf-after--p">Not that difficult, hey? Let’s get to work!</p><h4 name="ed4c" id="ed4c" class="graf graf--h4 graf-after--p">Building the intermediate data set</h4><p name="4a01" id="4a01" class="graf graf--p graf-after--h4">Our first step is to load both files and split the messages into <strong class="markup--strong markup--p-strong">words</strong>.</p><figure name="e5d5" id="e5d5" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/4e28509ca2a5a7501b8bdf537abe49bb.js"></script></figure><p name="5119" id="5119" class="graf graf--p graf-after--figure">Then, we’re hashing each message into 1,000 <strong class="markup--strong markup--p-strong">word buckets</strong>. As you can see, each message is turned into a <strong class="markup--strong markup--p-strong">sparse vector</strong> holding bucket numbers and occurrences.</p><figure name="fb68" id="fb68" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8727c4e5798c1228a78476e3a8e17bf8.js"></script></figure><p name="65f4" id="65f4" class="graf graf--p graf-after--figure">The next step is to <strong class="markup--strong markup--p-strong">label</strong> our features: 1 for spam, 0 for non-spam. The result is a collected of <strong class="markup--strong markup--p-strong">labeled samples</strong> which are ready for use.</p><figure name="ce12" id="ce12" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7d4743dbf913c1760cbc785b8c536937.js"></script></figure><p name="46be" id="46be" class="graf graf--p graf-after--figure">Finally, we split the data set 80/20 for <strong class="markup--strong markup--p-strong">training</strong> and <strong class="markup--strong markup--p-strong">test</strong> and cache both RDDs as we will use them repeatedly.</p><figure name="4516" id="4516" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/1722c3663f793f33b90224dfceb61795.js"></script></figure><p name="992d" id="992d" class="graf graf--p graf-after--figure">Now we’re going to train a number of models with this data set. To measure their accuracy, here’s the <strong class="markup--strong markup--p-strong">scoring function</strong> we’re going to use: simply predict all samples in the test set, compare the predicted label with the real label and compute accuracy.</p><figure name="bab5" id="bab5" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/46e5dc9b688e64c799b3054a4baaa380.js"></script></figure><h4 name="3d3d" id="3d3d" class="graf graf--h4 graf-after--figure">Classifying the data set with Spark MLLib</h4><p name="4d45" id="4d45" class="graf graf--p graf-after--h4">We’re going to use the following classification algorithms:</p><ul class="postList"><li name="e73e" id="e73e" class="graf graf--li graf-after--p"><a href="https://en.wikipedia.org/wiki/Logistic_regression" data-href="https://en.wikipedia.org/wiki/Logistic_regression" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Logistic regression</a> with the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" data-href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SGD</a> optimizer,</li><li name="5070" id="5070" class="graf graf--li graf-after--li">Logistic regression with the <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" data-href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">LBFGS</a> optimizer,</li><li name="b2e7" id="b2e7" class="graf graf--li graf-after--li"><a href="https://en.wikipedia.org/wiki/Support_vector_machine" data-href="https://en.wikipedia.org/wiki/Support_vector_machine" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Support Vector Machines</a>,</li><li name="c0ed" id="c0ed" class="graf graf--li graf-after--li"><a href="https://en.wikipedia.org/wiki/Decision_tree" data-href="https://en.wikipedia.org/wiki/Decision_tree" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Decision Trees</a>,</li><li name="82d3" id="82d3" class="graf graf--li graf-after--li"><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting" data-href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Gradient Boosted Trees</a>,</li><li name="2a96" id="2a96" class="graf graf--li graf-after--li"><a href="https://en.wikipedia.org/wiki/Random_forest" data-href="https://en.wikipedia.org/wiki/Random_forest" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Random Forests</a>,</li><li name="65eb" id="65eb" class="graf graf--li graf-after--li"><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" data-href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Naive Bayes</a>.</li></ul><h4 name="8933" id="8933" class="graf graf--h4 graf-after--li">Logistic Regression</h4><p name="d4fb" id="d4fb" class="graf graf--p graf-after--h4">Let’s start with Logistic Regression, the mother of all classifiers.</p><figure name="71c0" id="71c0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/5896647f67994781c2cffa1ec0d5a792.js"></script></figure><h4 name="e2be" id="e2be" class="graf graf--h4 graf-after--figure">Support Vector Machines</h4><p name="b173" id="b173" class="graf graf--p graf-after--h4">What about SVMs, another popular algorithm?</p><figure name="412b" id="412b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8f18b17cd9011a0cf3768bd0a33c37f6.js"></script></figure><h4 name="309b" id="309b" class="graf graf--h4 graf-after--figure">Trees</h4><p name="505d" id="505d" class="graf graf--p graf-after--h4">Now let’s try three variants of tree-based classification. The API is slightly different from previous algos.</p><figure name="9197" id="9197" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/f31b69c7c91ebbb4177032e3b9d1db9e.js"></script></figure><h4 name="5840" id="5840" class="graf graf--h4 graf-after--figure">Naive Bayes</h4><p name="7833" id="7833" class="graf graf--p graf-after--h4">Last but not least, let’s try the Naives Bayes classifier.</p><figure name="0a79" id="0a79" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/313fbcce4ca5d215779b818a17b5c34d.js"></script></figure><p name="80ac" id="80ac" class="graf graf--p graf-after--figure">It is vastly superior to all other algos. Let’s try to predict a couple of real-life samples.</p><figure name="17ae" id="17ae" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/b679cfc760668571f464fe2efbb4e684.js"></script></figure><p name="f179" id="f179" class="graf graf--p graf-after--figure">They were predicted correctly. This looks like a pretty good model. Now why don’t try to improve these scores? I’ve used default parameters for most of the algorithms, surely there is room for improvement :) You’ll find links to all APIs in the notebook, so feel free to tweak away!</p><h4 name="9591" id="9591" class="graf graf--h4 graf-after--p">This is great, but…</h4><p name="aade" id="aade" class="graf graf--p graf-after--h4">So far, we’ve only worked locally. This raises some questions:</p><ol class="postList"><li name="8143" id="8143" class="graf graf--li graf-after--p">how would we train on a <strong class="markup--strong markup--li-strong">much larger data set</strong>?</li><li name="5c0e" id="5c0e" class="graf graf--li graf-after--li">how would we deploy our model to <strong class="markup--strong markup--li-strong">production</strong>?</li><li name="bf32" id="bf32" class="graf graf--li graf-after--li">how could we know if our model would <strong class="markup--strong markup--li-strong">scale</strong>?</li></ol><p name="7fb2" id="7fb2" class="graf graf--p graf-after--li">These questions — scalability and deployment — are often the <strong class="markup--strong markup--p-strong">bane</strong> of Machine Learning projects. Going from “it works on my machine” to “it works in production at scale 24/7” usually requires <strong class="markup--strong markup--p-strong">a lot</strong> of work.</p><p name="dbe8" id="dbe8" class="graf graf--p graf-after--p graf--trailing">There is hope. Read on :)</p></div></div></section><section name="625c" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="266c" id="266c" class="graf graf--h3 graf--leading">SageMaker + XGBoost</h3><p name="5713" id="5713" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Solving these pain points is at the core of </strong><a href="http://aws.amazon.com/sagemaker" data-href="http://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon SageMaker</strong></a>. Let’s revisit our use case.</p><h4 name="2602" id="2602" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Built-in algorithms</strong></h4><p name="3b2a" id="3b2a" class="graf graf--p graf-after--h4">As we saw previously, there are plenty of classification algorithms. Picking the “right” one and its “best” implementation (good luck trying to define “right” and “best”) is not an easy task. Fortunately, SageMaker provides you with several <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">built-in algorithms</strong></a>. They have been implemented by Amazon, so I guess you can expect them to perform and scale correctly :)</p><blockquote name="d468" id="d468" class="graf graf--blockquote graf-after--p">You can also bring your own code, your own pre-trained model, etc. To be discussed in future articles! More SageMaker examples on <a href="https://github.com/awslabs/amazon-sagemaker-examples" data-href="https://github.com/awslabs/amazon-sagemaker-examples" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Github</a>: regression, multi-class classification, image classification, etc.</blockquote><p name="bb87" id="bb87" class="graf graf--p graf-after--blockquote">Here, we’re going to use <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">XGBoost</strong></a>, a popular implementation of Gradient Boosted Trees to build a binary classifier.</p><p name="d929" id="d929" class="graf graf--p graf-after--p">In a nutshell, the <a href="https://github.com/aws/sagemaker-python-sdk" data-href="https://github.com/aws/sagemaker-python-sdk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">SageMaker SDK</strong></a> will let us:</p><ul class="postList"><li name="8d38" id="8d38" class="graf graf--li graf-after--p">create managed infrastructure to <strong class="markup--strong markup--li-strong">train</strong> XGBoost on our data set,</li><li name="722a" id="722a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">store</strong> the model in SageMaker,</li><li name="ffb8" id="ffb8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">configure</strong> a REST endpoint to serve our model,</li><li name="8e1d" id="8e1d" class="graf graf--li graf-after--li">create managed infrastructure to <strong class="markup--strong markup--li-strong">deploy</strong> the model to the REST endpoint,</li><li name="1f4a" id="1f4a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">invoke</strong> the model on a couple of samples.</li></ul><p name="a562" id="a562" class="graf graf--p graf-after--li">Let’s do this!</p><h4 name="4789" id="4789" class="graf graf--h4 graf-after--p">Setting up storage and data</h4><p name="7628" id="7628" class="graf graf--p graf-after--h4">First things first: S3 will be used to store the data set and all artifacts (what a surprise). Let’s declare a few things, then. Hint: the S3 bucket <strong class="markup--strong markup--p-strong">must</strong> be in the same region as SageMaker.</p><figure name="1cf2" id="1cf2" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8757751b07f8aa49e809075b0a7290a5.js"></script></figure><p name="e52b" id="e52b" class="graf graf--p graf-after--figure">This implementation of XGBoost requires data to be either in <strong class="markup--strong markup--p-strong">CSV</strong> or <strong class="markup--strong markup--p-strong">libsvm</strong> format. Let’s try the latter, copy the resulting files to S3 and grab the SageMaker IAM role.</p><figure name="40c4" id="40c4" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/ae3200662999b0b83a7fe048fe62cbfc.js"></script></figure><p name="07a5" id="07a5" class="graf graf--p graf-after--figure">Looking good. Now let’s set up the training job.</p><h4 name="42bb" id="42bb" class="graf graf--h4 graf-after--p">Setting up the training job</h4><p name="c7d7" id="c7d7" class="graf graf--p graf-after--h4">Amazon SageMaker uses Docker containers to run training jobs. We need to pick the container name corresponding to the region we’re running in.</p><figure name="2611" id="2611" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/847d92171270311f63bcd89374902e3e.js"></script></figure><p name="0720" id="0720" class="graf graf--p graf-after--figure">Easy enough. Time to configure training. We’re going to:</p><ul class="postList"><li name="ea32" id="ea32" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Build a binary classifier,</strong></li><li name="ee15" id="ee15" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fetch the training and validation data sets in libsvm format from S3,</strong></li><li name="3aff" id="3aff" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Train for 100 iterations a single m4.4xlarge instance.</strong></li></ul><figure name="02b7" id="02b7" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/347efe35de1fdc5a965a376e2a91460c.js"></script></figure><p name="29f5" id="29f5" class="graf graf--p graf-after--figure">That’s quite a mouthful, but don’t panic:</p><ul class="postList"><li name="e7a5" id="e7a5" class="graf graf--li graf-after--p">Parameters common to all algorithms are defined in the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">CreateTrainingJob API</a> documentation.</li><li name="8929" id="8929" class="graf graf--li graf-after--li">Algorithm-specific parameters are defined on the algorithm page, e.g. <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">XGBoost</a>.</li></ul><h4 name="9647" id="9647" class="graf graf--h4 graf-after--li">Training and saving the model</h4><p name="7131" id="7131" class="graf graf--p graf-after--h4">OK, let’s get this party going. Time to start training.</p><figure name="f276" id="f276" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/a9658a80d70a798957785894a1e32469.js"></script></figure><p name="5d15" id="5d15" class="graf graf--p graf-after--figure">6 minutes later, our model is ready. Of course, this is a bit long for such a small data set :) However, if we had millions of lines, <strong class="markup--strong markup--p-strong">we could have started a</strong> <strong class="markup--strong markup--p-strong">training job on multiple instances with the exact same code</strong>. Pretty cool, huh?</p><p name="ed3c" id="ed3c" class="graf graf--p graf-after--p">OK, let’s <strong class="markup--strong markup--p-strong">save this model</strong> in SageMaker. Pretty straightforward with the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CreateModel API</a>.</p><figure name="fbdc" id="fbdc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7f2950edf4a3815ba6129507004c4a56.js"></script></figure><h4 name="13db" id="13db" class="graf graf--h4 graf-after--figure">Creating the endpoint</h4><p name="3706" id="3706" class="graf graf--p graf-after--h4">Here comes the <strong class="markup--strong markup--p-strong">really</strong> good part. We’re going to deploy this model and invoke it. Yes, just like that.</p><p name="7451" id="7451" class="graf graf--p graf-after--p">First, we need to create an endpoint configuration with the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CreateEndpointConfig API</a>: we’ll use a <strong class="markup--strong markup--p-strong">single m4.xlarge for inference</strong>, with <strong class="markup--strong markup--p-strong">100% of traffic going to our model</strong> (we’ll look at A/B testing in a future post).</p><figure name="8340" id="8340" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7edf42a19cdd4f827ad3fc3fb5358fe3.js"></script></figure><h4 name="7a5c" id="7a5c" class="graf graf--h4 graf-after--figure">Deploying the model</h4><p name="3eff" id="3eff" class="graf graf--p graf-after--h4">Now we can deploy our trained model on this endpoint with the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html" data-href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CreateEndpoint API</a>.</p><figure name="e2ca" id="e2ca" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/540590eb5557250a7dc1d9f2c53e256e.js"></script></figure><h4 name="878d" id="878d" class="graf graf--h4 graf-after--figure">Invoking the endpoint</h4><p name="baee" id="baee" class="graf graf--p graf-after--h4">We’re now ready to invoke the endpoint. Let’s grab a couple of samples (in libsvm format) from the data set and predict them.</p><figure name="a88c" id="a88c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/329a976570464143314b9862168428f0.js"></script></figure><p name="a944" id="a944" class="graf graf--p graf-after--figure">Both samples are predicted correctly. Woohoo.</p><p name="848c" id="848c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Conclusion</strong></p><p name="f8da" id="f8da" class="graf graf--p graf-after--p">As you can see, SageMaker helps you run your Machine Learning projects end to end: notebook experimentation, model training, model hosting, model deployment.</p><p name="e8b2" id="e8b2" class="graf graf--p graf-after--p">If you’re curious about other ways you can use SageMaker (and if you can’t wait for the inevitable future posts!), here’s a overview I recorded recently.</p><figure name="d261" id="d261" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/ym7NEYEx9x4?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="b76b" id="b76b" class="graf graf--p graf-after--figure graf--trailing">That’s it for today. Thank you very much for reading.</p></div></div></section><section name="4192" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="573b" id="573b" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">This monster post was written while listening over and over (it WAS a long post) to this legendary Foreigner show from 1981.</em></p><figure name="04c0" id="04c0" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/yTkHV3hD010?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/1980158a900f"><time class="dt-published" datetime="2017-12-18T21:32:43.203Z">December 18, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>