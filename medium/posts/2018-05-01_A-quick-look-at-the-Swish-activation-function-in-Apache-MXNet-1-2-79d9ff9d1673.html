<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A quick look at the Swish activation function in Apache MXNet 1.2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A quick look at the Swish activation function in Apache MXNet 1.2</h1>
</header>
<section data-field="subtitle" class="p-summary">
Apache MXNet 1.2 is right around the corner. As hinted at by the change log, this looks like a major release, but in this post, we’ll focus…
</section>
<section data-field="body" class="e-content">
<section name="5c94" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="766f" id="766f" class="graf graf--h3 graf--leading graf--title">A quick look at the Swish activation function in Apache MXNet 1.2</h3><p name="f303" id="f303" class="graf graf--p graf-after--h3"><a href="https://github.com/apache/incubator-mxnet" data-href="https://github.com/apache/incubator-mxnet" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> 1.2 is <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.2.0.rc1" data-href="https://github.com/apache/incubator-mxnet/releases/tag/1.2.0.rc1" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">right around the corner</a>. As hinted at by the change log, this looks like a major release, but in this post, we’ll focus on a new activation function: <strong class="markup--strong markup--p-strong">Swish</strong>.</p><h4 name="351e" id="351e" class="graf graf--h4 graf-after--p">A quick recap on activation functions</h4><p name="3415" id="3415" class="graf graf--p graf-after--h4">In deep neural networks, the purpose of the activation function is to introduce non-linearity, i.e. to <strong class="markup--strong markup--p-strong">enforce a non-linear decision threshold on neuron outputs</strong>. In a way, we’re trying to mimic — in a simplistic way, no doubt — the behavior of biological neurons which either fire or not.</p><p name="7d85" id="7d85" class="graf graf--p graf-after--p">Over time, a number of <a href="https://en.wikipedia.org/wiki/Activation_function" data-href="https://en.wikipedia.org/wiki/Activation_function" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">activation functions</a> have been designed, each new one trying to overcome the shortcomings of its predecessors. For example, the popular <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" data-href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Rectified Linear Unit function</strong></a> (aka ReLU) improved on the Sigmoid function by solving the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" data-href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">vanishing gradient problem</a>.</p><p name="0977" id="0977" class="graf graf--p graf-after--p">Of course, the race for better activation function never stopped. In late 2017, a new function was discovered: <strong class="markup--strong markup--p-strong">Swish</strong>.</p><h4 name="4c8d" id="4c8d" class="graf graf--h4 graf-after--p">The Swish function</h4><p name="5efc" id="5efc" class="graf graf--p graf-after--h4">By automatically combining different mathematical operators, Prajit Ramachandran, Barret Zoph and Quoc V evaluated the performance of a large number of candidate activation functions (“Searching for Activation Functions”, <a href="https://arxiv.org/abs/1710.05941" data-href="https://arxiv.org/abs/1710.05941" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research paper</a>). One of them, which they named <strong class="markup--strong markup--p-strong">Swish</strong>, turned out to be better than others.</p><figure name="c0ff" id="c0ff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YjlMCL5eN25GGBDISql6_g.png" data-width="844" data-height="560" src="https://cdn-images-1.medium.com/max/800/1*YjlMCL5eN25GGBDISql6_g.png"><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong"><em class="markup--em markup--figure-em">f</em>(<em class="markup--em markup--figure-em">x</em>)=<em class="markup--em markup--figure-em">x</em>⋅sigmoid(<em class="markup--em markup--figure-em">βx</em>)</strong> — Source: research paper.</figcaption></figure><p name="f92a" id="f92a" class="graf graf--p graf-after--figure">As you can see, if the <em class="markup--em markup--p-em">β </em>parameter is small, Swish is close to the linear when the <em class="markup--em markup--p-em">β </em>parameter is small and close to ReLU when it’s large. The sweet spot for <em class="markup--em markup--p-em">β </em>seems to be <strong class="markup--strong markup--p-strong">between 1 and 2</strong>: it creates a non-monotonic “bump” for negative values which seems to have interesting properties (more details in the research paper).</p><p name="249d" id="249d" class="graf graf--p graf-after--p">As highlighted by the authors: “<em class="markup--em markup--p-em">simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2</em>”.</p><p name="1a34" id="1a34" class="graf graf--p graf-after--p">This sounds like an easy improvement, doesn’t it? Let’s test it on MXNet!</p><h4 name="3f2d" id="3f2d" class="graf graf--h4 graf-after--p">Swish in MXNet</h4><p name="3739" id="3739" class="graf graf--p graf-after--h4">Swish is available for the <a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Gluon API</strong></a> in MXNet 1.2. It’s defined in <em class="markup--em markup--p-em">incubator-mxnet/python/mxnet/gluon/nn/activations.py</em> and using it in our Gluon code is as easy as: <em class="markup--em markup--p-em">nn.Swish().</em></p><p name="2b6c" id="2b6c" class="graf graf--p graf-after--p">In order to evaluate its performance, we’re going to train <strong class="markup--strong markup--p-strong">two different versions of the </strong><a href="https://arxiv.org/abs/1409.1556" data-href="https://arxiv.org/abs/1409.1556" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">VGG16</strong></a><strong class="markup--strong markup--p-strong"> convolution neural network on </strong><a href="https://www.cs.toronto.edu/~kriz/cifar.html" data-href="https://www.cs.toronto.edu/~kriz/cifar.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">CIFAR-10</strong></a>:</p><ul class="postList"><li name="32ed" id="32ed" class="graf graf--li graf-after--p">VGG16 with batch normalization as implemented in the <a href="https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Gluon model zoo</a>, i.e. using ReLU (<em class="markup--em markup--li-em">incubator-mxnet/python/mxnet/gluon/model_zoo/vision/vgg.py</em>)</li><li name="c5b0" id="c5b0" class="graf graf--li graf-after--li">The same network <strong class="markup--strong markup--li-strong">modified to use Swish for the convolution layers and the fully connected layers</strong>.</li></ul><p name="cb9e" id="cb9e" class="graf graf--p graf-after--li">This is pretty straightforward: starting from the master branch, we simply create a <em class="markup--em markup--p-em">vggswish.py</em> file and replace ReLU by Swish, e.g.:</p><pre name="1fb9" id="1fb9" class="graf graf--pre graf-after--p">self.features.add(nn.Dense(4096, activation=&#39;relu&#39;, weight_initializer=&#39;normal&#39;, bias_initializer=&#39;zeros&#39;))</pre><pre name="9290" id="9290" class="graf graf--pre graf-after--pre">---&gt; </pre><pre name="0d37" id="0d37" class="graf graf--pre graf-after--pre">self.features.add(nn.Dense(4096, weight_initializer=&#39;normal&#39;, bias_initializer=&#39;zeros&#39;))<br>self.features.add(nn.Swish())</pre><p name="f51d" id="f51d" class="graf graf--p graf-after--pre">Then, we plug this new set of models into <em class="markup--em markup--p-em">incubator-mxnet/python/mxnet/gluon/model_zoo/__init__.py</em> and voila!</p><p name="2190" id="2190" class="graf graf--p graf-after--p">Here’s the full <a href="https://gist.github.com/juliensimon/a464b762e39eafce1b2958f99f1398a7" data-href="https://gist.github.com/juliensimon/a464b762e39eafce1b2958f99f1398a7" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">diff</a> if you’re interested. There’s really not much to it.</p><blockquote name="b30c" id="b30c" class="graf graf--blockquote graf-after--p">Of course, you’ll need to build and install: you should know how to do this by now ;) If you’ve already built the master branch, you can get away with just installing the Python API again. If you’re not comfortable with this, no worries: just wait for an official 1.2 installation package :)</blockquote><h4 name="c909" id="c909" class="graf graf--h4 graf-after--blockquote">Training on CIFAR-10</h4><p name="6ba7" id="6ba7" class="graf graf--p graf-after--h4">MXNet contains an <strong class="markup--strong markup--p-strong">image classification script</strong> which lets us train with a variety of network architectures and data sets (<em class="markup--em markup--p-em">incubator-mxnet/example/gluon/image_classification.py</em>). Exactly what we need!</p><p name="d759" id="d759" class="graf graf--p graf-after--p">We’ll use <strong class="markup--strong markup--p-strong">SGD with epochs steps</strong>, dividing the learning rate by 10 each time.</p><pre name="ac4a" id="ac4a" class="graf graf--pre graf-after--p">python image_classification.py --model vgg16_bn --batch-size=128 --lr=0.1 --lr-steps=&#39;10,20,30&#39; --epochs=40</pre><pre name="b209" id="b209" class="graf graf--pre graf-after--pre">python image_classification.py --model vgg16_bn_swish --batch-size=128 --lr=0.1 --lr-steps=&#39;10,20,30&#39; --epochs=40</pre><p name="ad4a" id="ad4a" class="graf graf--p graf-after--pre">Here are the results.</p><figure name="4a14" id="4a14" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rTiZ8IJqG5BAy6SaoV91ZQ.png" data-width="1330" data-height="942" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*rTiZ8IJqG5BAy6SaoV91ZQ.png"><figcaption class="imageCaption">VGG16 with ReLU vs VGG16 with Swish</figcaption></figure><p name="5178" id="5178" class="graf graf--p graf-after--figure">It’s always difficult to draw conclusions from a single example (although I did run a bunch of different trainings with consistent results). Here, we can see than <strong class="markup--strong markup--p-strong">the Swish version seems to train faster and validate as well as the ReLU version</strong>.</p><p name="eb14" id="eb14" class="graf graf--p graf-after--p">Top validation accuracies are respectively 0.866186 at epoch #14 and 0.866001 at epoch #19. A minimal difference, but then again VGG16 isn’t a very deep network.</p><h4 name="3fba" id="3fba" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="7c8c" id="7c8c" class="graf graf--p graf-after--h4">So now we have <strong class="markup--strong markup--p-strong">another activation function in our arsenal</strong>. It’s still quite new, so we need to experiment and learn when to use it or not. In case you’re curious, MXNet 1.2 also adds support for the ELU and SELU functions, so why not read about these as well? A Deep Learning engineer’s day is never done :)</p><p name="3837" id="3837" class="graf graf--p graf-after--p graf--trailing">That’s it for today. Thank you for reading. Happy to read your feedback here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="78f7" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c9ce" id="c9ce" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">This post was written in New Orleans, so obviously…</em></p><figure name="378d" id="378d" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/poD-gi3A4zw" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/79d9ff9d1673"><time class="dt-published" datetime="2018-05-01T22:25:41.037Z">May 1, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/a-quick-look-at-the-swish-activation-function-in-apache-mxnet-1-2-79d9ff9d1673" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>