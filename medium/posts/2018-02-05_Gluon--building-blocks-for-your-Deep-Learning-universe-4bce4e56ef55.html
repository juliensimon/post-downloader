<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Gluon: building blocks for your Deep Learning universe</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Gluon: building blocks for your Deep Learning universe</h1>
</header>
<section data-field="subtitle" class="p-summary">
Launched in October 2017, Gluon is a new Open Source high-level API for Deep Learning developers. Right now, it’s available on top of…
</section>
<section data-field="body" class="e-content">
<section name="14be" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7a03" id="7a03" class="graf graf--h3 graf--leading graf--title">Gluon: building blocks for your Deep Learning universe</h3><p name="9714" id="9714" class="graf graf--p graf-after--h3"><a href="https://aws.amazon.com/blogs/aws/introducing-gluon-a-new-library-for-machine-learning-from-aws-and-microsoft/" data-href="https://aws.amazon.com/blogs/aws/introducing-gluon-a-new-library-for-machine-learning-from-aws-and-microsoft/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Launched in October 2017</a>, Gluon is a new Open Source <a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#gluon-api" data-href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#gluon-api" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">high-level API</a> for Deep Learning developers. Right now, it’s available on top of <a href="http://mxnet.incubator.apache.org" data-href="http://mxnet.incubator.apache.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a>.</p><p name="74e3" id="74e3" class="graf graf--p graf-after--p">Yet another API? Well, not quite. Here are <strong class="markup--strong markup--p-strong">ten reasons</strong> why you should take a good look at Gluon.</p><figure name="034e" id="034e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*D5Tp--YLShZ8iCAtGvvT1Q.png" data-width="1920" data-height="1080" src="https://cdn-images-1.medium.com/max/800/1*D5Tp--YLShZ8iCAtGvvT1Q.png"><figcaption class="imageCaption">Source: <a href="https://www.quantamagazine.org" data-href="https://www.quantamagazine.org" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Quanta Magazine</a></figcaption></figure><h4 name="a3a2" id="a3a2" class="graf graf--h4 graf-after--figure">1 — Extraordinary documentation</h4><p name="0cf5" id="0cf5" class="graf graf--p graf-after--h4">I’m not exaggerating. Calling it documentation doesn’t do it justice: Gluon actually comes with a <a href="http://gluon.mxnet.io/index.html" data-href="http://gluon.mxnet.io/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">full-fledged book</strong></a><strong class="markup--strong markup--p-strong"> on Deep Learning</strong>!</p><p name="aa03" id="aa03" class="graf graf--p graf-after--p">Concepts, how to implement them from scratch, how to implement them with Gluon, pretty much all network architectures from perceptrons to Generative Adversial Networks… and a ton of <a href="https://github.com/zackchase/mxnet-the-straight-dope" data-href="https://github.com/zackchase/mxnet-the-straight-dope" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">notebooks</strong></a><strong class="markup--strong markup--p-strong">.</strong></p><p name="472d" id="472d" class="graf graf--p graf-after--p">VERY impressive work by my colleague <a href="https://github.com/zackchase" data-href="https://github.com/zackchase" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Zach Lipton</a>. If you’d like to help him out, I’m sure he’d be happy to review your pull requests ;)</p><h4 name="a0f5" id="a0f5" class="graf graf--h4 graf-after--p">2— Plenty of pre-defined layers and loss functions</h4><p name="1e53" id="1e53" class="graf graf--p graf-after--h4">Gluon includes an extensive collection of <a href="https://mxnet.incubator.apache.org/api/python/gluon/nn.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/nn.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">pre-defined layers</strong></a>: from basic ones (Dense, Activation, Dropout, Embedding, etc.) to Convolution (2D, 3D, transposed) to Pooling (average, max and global max in 1D, 2D and 3D).</p><p name="03a0" id="03a0" class="graf graf--p graf-after--p">You’ll also find <a href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">layers for recurrent networks</strong></a> (RNN, LSTM, GRU), as well as <strong class="markup--strong markup--p-strong">individuals cells</strong>. The latter allow you full control over your networks should you need to build them cell by cell.</p><p name="2743" id="2743" class="graf graf--p graf-after--p">In addition, you’ll find a collection of <a href="https://mxnet.incubator.apache.org/api/python/gluon/contrib.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/contrib.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">experimental features</strong></a> contributed by the Gluon community, such as convolutional recurrent cells.</p><p name="ba53" id="ba53" class="graf graf--p graf-after--p">Last but not least, Gluon also includes a nice collection of <a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">loss functions</strong></a>, from basic ones to more advanced ones like the Triplet Loss function used to build face recognition models.</p><h4 name="d030" id="d030" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">3 — Simple definition of models</strong></h4><p name="85c7" id="85c7" class="graf graf--p graf-after--h4">For reference, this is how we’d define a simple network with the <a href="https://mxnet.incubator.apache.org/api/python/symbol/symbol.html" data-href="https://mxnet.incubator.apache.org/api/python/symbol/symbol.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">symbolic API</a> in Apache MXNet.</p><pre name="2158" id="2158" class="graf graf--pre graf-after--p">import mxnet as mx<br>from mxnet import sym,mod</pre><pre name="c5bb" id="c5bb" class="graf graf--pre graf-after--pre">data = sym.Variable(&#39;data&#39;)<br>fc1 = sym.FullyConnected(data, name=&#39;fc1&#39;, num_hidden=128)<br>relu1 = sym.Activation(fc1, name=&#39;relu1&#39;, act_type=&quot;relu&quot;)<br>fc2 = sym.FullyConnected(relu1, name=&#39;fc2&#39;, num_hidden=64)<br>relu2 = sym.Activation(fc2, name=&#39;relu1&#39;, act_type=&quot;relu&quot;)<br>out = sym.FullyConnected(relu2, name=&#39;out&#39;, num_hidden=10)<br>mod = mod.Module(out)</pre><p name="4608" id="4608" class="graf graf--p graf-after--pre">Here’s the same network defined with Gluon. All we have to do is to add layers sequentially.</p><pre name="0ad5" id="0ad5" class="graf graf--pre graf-after--p">import mxnet as mx<br>from mxnet.gluon import nn</pre><pre name="2a69" id="2a69" class="graf graf--pre graf-after--pre">net = nn.Sequential()<br>with net.name_scope():<br>   net.add(nn.Dense(128, activation=&quot;relu&quot;))<br>   net.add(nn.Dense(64, activation=&quot;relu&quot;))<br>   net.add(nn.Dense(10))</pre><p name="6a21" id="6a21" class="graf graf--p graf-after--pre">A bit clearer, isn’t it? :)</p><h4 name="1792" id="1792" class="graf graf--h4 graf-after--p">4— Automatic shape of input layer</h4><p name="143e" id="143e" class="graf graf--p graf-after--h4">As you can see above, we don’t have to define the <strong class="markup--strong markup--p-strong">input shape</strong> when building a network. With Gluon, all we have to do is initialize parameters and forward data to the network.</p><p name="5f4b" id="5f4b" class="graf graf--p graf-after--p">For instance, this is how we’d apply the network above to a 256-float vector.</p><pre name="684d" id="684d" class="graf graf--pre graf-after--p">net.collect_params().initialize()</pre><pre name="402d" id="402d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em"># Define a random 256-float vector and forward it to the network<br></em>data = mx.nd.random_uniform(low=0, high=1, shape=(1,256))<br>net(data)</pre><pre name="abff" id="abff" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">[[ 1.3353475e-03 -1.1403845e-02  8.6122309e-05  1.3773030e-02<br>   9.9888537e-03  6.7939619e-03 -1.8021716e-02 -6.2033422e-03<br>  -1.3288442e-02  1.0132480e-02]]</strong></pre><p name="85a1" id="85a1" class="graf graf--p graf-after--pre">This is an advantage over Keras where we’d have to build the input shape into the model definition.</p><pre name="97e2" id="97e2" class="graf graf--pre graf-after--p">from keras.model import Sequential<br>from keras.layers import Dense</pre><pre name="162a" id="162a" class="graf graf--pre graf-after--pre">model = Sequential() <br>model.add(Dense(128, activation=&#39;relu&#39;, input_shape=(<strong class="markup--strong markup--pre-strong">256</strong>,)))<br>model.add(Dense(64, activation=&#39;relu&#39;))<br>model.add(Dense(10))</pre><h4 name="6f39" id="6f39" class="graf graf--h4 graf-after--pre">5—Intuitive access to network layers and parameters</h4><p name="8409" id="8409" class="graf graf--p graf-after--h4">Gluon makes it intuitive to explore network layers, as well as their <a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#mxnet.gluon.Parameter" data-href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#mxnet.gluon.Parameter" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">parameters</a>.</p><p name="72ce" id="72ce" class="graf graf--p graf-after--p">Here’s how we can iterate through layers.</p><pre name="a860" id="a860" class="graf graf--pre graf-after--p">for layer in net:<br>  print(layer)<br>  print(layer.params)</pre><pre name="44f7" id="44f7" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Dense(64 -&gt; 128, Activation(relu))<br>sequential2_dense0_ (<br>  Parameter sequential2_dense0_weight (shape=(128L, 64L), dtype=&lt;type &#39;numpy.float32&#39;&gt;)<br>  Parameter sequential2_dense0_bias (shape=(128L,), dtype=&lt;type &#39;numpy.float32&#39;&gt;)<br>)</strong></pre><pre name="81c4" id="81c4" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Dense(128 -&gt; 64, Activation(relu))<br>sequential2_dense1_ (<br>  Parameter sequential2_dense1_weight (shape=(64L, 128L), dtype=&lt;type &#39;numpy.float32&#39;&gt;)<br>  Parameter sequential2_dense1_bias (shape=(64L,), dtype=&lt;type &#39;numpy.float32&#39;&gt;)<br>)</strong></pre><pre name="dbde" id="dbde" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Dense(64 -&gt; 10, linear)<br>sequential2_dense2_ (<br>  Parameter sequential2_dense2_weight (shape=(10L, 64L), dtype=&lt;type &#39;numpy.float32&#39;&gt;)<br>  Parameter sequential2_dense2_bias (shape=(10L,), dtype=&lt;type &#39;numpy.float32&#39;&gt;)<br>)</strong></pre><p name="78d2" id="78d2" class="graf graf--p graf-after--pre">Reading and writing parameters is equally straightforward.</p><pre name="f2d2" id="f2d2" class="graf graf--pre graf-after--p">params=net[0].weight.data()<br>print(&quot;%s %s&quot; % (type(params), params.shape))</pre><pre name="d6d6" id="d6d6" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">&lt;class &#39;mxnet.ndarray.ndarray.NDArray&#39;&gt; (128L, 64L)</strong></pre><pre name="a9ce" id="a9ce" class="graf graf--pre graf-after--pre">params[0][0]=0.123<br>print(params)</pre><pre name="af84" id="af84" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">[[ 0.123 -0.0177393  -0.00650402 ... -0.04026533 -0.04062188<br>  -0.03885795]<br> [ 0.05647313  0.0380233   0.01031513 ...  0.0654735   0.04788432<br>  -0.03103536]<br> [ 0.02013787  0.01294949  0.02260739 ... -0.0699827   0.01811036<br>  -0.05699452]<br> ...<br> [-0.04240721  0.01670218  0.0533151  ...  0.000951    0.05940091<br>   0.00070946]<br> [-0.00068477  0.00757013 -0.04234412 ... -0.04753195  0.01538438<br>  -0.04391037]<br> [-0.01510854 -0.03736208  0.01939485 ... -0.04374463 -0.03795088<br>  -0.01618673]]</strong></pre><h4 name="9b91" id="9b91" class="graf graf--h4 graf-after--pre">6 — Flexible data loading and transformation</h4><p name="ad54" id="ad54" class="graf graf--p graf-after--h4">The <a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html" data-href="https://mxnet.incubator.apache.org/api/python/gluon/data.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Data API</a> provides convenient methods to load datasets stored in <a href="https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html" data-href="https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">NDArrays</strong></a> (which is how MXNet stores tensors), <strong class="markup--strong markup--p-strong">numpy arrays</strong>, <a href="https://mxnet.incubator.apache.org/api/python/io/io.html#module-mxnet.recordio" data-href="https://mxnet.incubator.apache.org/api/python/io/io.html#module-mxnet.recordio" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">RecordIO</strong></a><strong class="markup--strong markup--p-strong"> files</strong> and <strong class="markup--strong markup--p-strong">image folders</strong>.</p><pre name="9ae1" id="9ae1" class="graf graf--pre graf-after--p">train_data = gluon.data.DataLoader(<br>    gluon.data.ArrayDataset(X, y),<br>    batch_size=batch_size, <br>    shuffle=True)</pre><p name="be13" id="be13" class="graf graf--p graf-after--pre">We can also download <strong class="markup--strong markup--p-strong">popular datasets</strong> like MNIST, Fashion MNIST, CIFAR-10 and CIFAR-100.</p><pre name="80b4" id="80b4" class="graf graf--pre graf-after--p">train_data = mx.gluon.data.DataLoader(<br>    mx.gluon.data.vision.MNIST(train=True))</pre><p name="deb7" id="deb7" class="graf graf--p graf-after--pre">Transformations can be applied at loading time by providing a <strong class="markup--strong markup--p-strong">transform function</strong>. For example, here’s how we would normalize pixel values for the MNIST dataset.</p><pre name="b81d" id="b81d" class="graf graf--pre graf-after--p">def transform(data, label):   <br>    return data.astype(np.float32)/255, label.astype(np.float32)</pre><pre name="151b" id="151b" class="graf graf--pre graf-after--pre">train_data = mx.gluon.data.DataLoader(<br>    mx.gluon.data.vision.MNIST(<br>       train=True, <br>       transform=transform))</pre><h4 name="6df5" id="6df5" class="graf graf--h4 graf-after--pre">7 — Rich model zoo</h4><p name="66f3" id="66f3" class="graf graf--p graf-after--h4">The Gluon model zoo is more complete than its counterparts in <a href="https://mxnet.apache.org/model_zoo/index.html" data-href="https://mxnet.apache.org/model_zoo/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a>, <a href="https://keras.io/applications/" data-href="https://keras.io/applications/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Keras</a> and <a href="http://pytorch.org/docs/master/torchvision/models.html" data-href="http://pytorch.org/docs/master/torchvision/models.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PyTorch</a>.</p><p name="147c" id="147c" class="graf graf--p graf-after--p">At the time of writing, you can grab pre-trained versions of <a href="https://arxiv.org/abs/1404.5997" data-href="https://arxiv.org/abs/1404.5997" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">AlexNet</strong></a><strong class="markup--strong markup--p-strong">, </strong><a href="https://arxiv.org/abs/1608.06993" data-href="https://arxiv.org/abs/1608.06993" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">DenseNet</strong></a><strong class="markup--strong markup--p-strong">, </strong><a href="http://arxiv.org/abs/1512.00567" data-href="http://arxiv.org/abs/1512.00567" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Inception V3</strong></a><strong class="markup--strong markup--p-strong">, </strong><a href="https://arxiv.org/abs/1512.03385" data-href="https://arxiv.org/abs/1512.03385" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">ResNet V1, ResNet V2</strong></a><strong class="markup--strong markup--p-strong">, </strong><a href="https://arxiv.org/abs/1602.07360" data-href="https://arxiv.org/abs/1602.07360" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">SqueezeNet</strong></a><strong class="markup--strong markup--p-strong">, </strong><a href="https://arxiv.org/abs/1409.1556" data-href="https://arxiv.org/abs/1409.1556" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">VGG</strong></a><strong class="markup--strong markup--p-strong"> and </strong><a href="https://arxiv.org/abs/1704.04861" data-href="https://arxiv.org/abs/1704.04861" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">MobileNet</strong></a>, in multiple <strong class="markup--strong markup--p-strong">depths</strong> and <strong class="markup--strong markup--p-strong">configurations</strong>.</p><p name="d591" id="d591" class="graf graf--p graf-after--p">All of these will come in handy for transfer learning and fine-tuning. Downloading a model couldn’t be simpler.</p><pre name="17de" id="17de" class="graf graf--pre graf-after--p">from mxnet.gluon.model_zoo import vision<br>net = vision.squeezenet1_1(pretrained=True)</pre><h4 name="14ac" id="14ac" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">8 — Imperative-style execution</strong></h4><p name="d657" id="d657" class="graf graf--p graf-after--h4">In traditional Deep Learning frameworks like Tensorflow and Apache MXNet, network definition and training run in symbolic mode (aka define-then-run).</p><blockquote name="9123" id="9123" class="graf graf--blockquote graf-after--p">In October 2017, Tensorflow introduced an experimental imperative mode, aka <a href="https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html" data-href="https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">eager mode</a>.</blockquote><p name="3086" id="3086" class="graf graf--p graf-after--blockquote">Here’s a typical example using Apache MXNet.</p><pre name="03e6" id="03e6" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"># Define network with symbolic API<br></em>...<br>mod = mod.Module(out)<br><em class="markup--em markup--pre-em"># Train network<br></em>mod.bind(<br>  data_shapes=iter.provide_data,<br>  label_shapes=iter.provide_label)<br>mod.fit(train_iter, num_epoch=50)</pre><p name="ee68" id="ee68" class="graf graf--p graf-after--pre">There are good reasons for doing this! Since a symbolic network is <strong class="markup--strong markup--p-strong">pre-defined</strong>, its execution graph can be <strong class="markup--strong markup--p-strong">optimized for speed and memory</strong> prior to training, then run with highly-efficient C++ primitives: all of this makes it <strong class="markup--strong markup--p-strong">more efficient</strong> than its imperative counterpart written in Python. However, it comes at the expense of <strong class="markup--strong markup--p-strong">flexibility</strong> (networks cannot be modified) and <strong class="markup--strong markup--p-strong">visibility</strong> (networks are hard / impossible to inspect).</p><p name="ddec" id="ddec" class="graf graf--p graf-after--p">In contrast, Gluon relies exclusively on <strong class="markup--strong markup--p-strong">imperative</strong> (aka define-by-run) programming: network definition and training loop are based on Python code, allowing us to use all <strong class="markup--strong markup--p-strong">language features</strong> (loops, conditional execution, classes, etc.) for maximal <strong class="markup--strong markup--p-strong">flexibility</strong>.</p><p name="137b" id="137b" class="graf graf--p graf-after--p">To illustrate this, here’s a typical training loop.</p><pre name="d83d" id="d83d" class="graf graf--pre graf-after--p">for e in range(epochs):<br>    cumulative_loss = 0<br>    for i, (data, label) in enumerate(train_data):<br>        data = data.as_in_context(model_ctx)<br>        label = label.as_in_context(model_ctx)<br>        with autograd.record():<br>            output = net(data)<br>            loss = softmax_cross_entropy(output, label)<br>        loss.backward()<br>        trainer.step(data.shape[0])<br>        cumulative_loss += nd.sum(loss).asscalar()</pre><p name="1f9a" id="1f9a" class="graf graf--p graf-after--pre">Thanks to imperative programming, it’s possible to <strong class="markup--strong markup--p-strong">debug every step of the training process</strong>: inspecting parameters, saving them to disk, tweaking them if certain conditions happen, etc. Even inside of Jupyter notebooks, we can use the <strong class="markup--strong markup--p-strong">Python debugger</strong> by inserting a single line of code. This is invaluable when trying to understand why training goes wrong.</p><pre name="254a" id="254a" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">import pdb; pdb.set_trace()</code></pre><blockquote name="c460" id="c460" class="graf graf--blockquote graf-after--pre">I have one minor gripe about the lack of a high-level API to train a model, similar to <em class="markup--em markup--blockquote-em">model.fit()</em> in MXNet or Keras. Sure, it’s easy to write, but hopefully the Gluon team will add it. Lazyness is a virtue ;)</blockquote><h4 name="d107" id="d107" class="graf graf--h4 graf-after--blockquote">9 — Combining custom objects and built-in objects</h4><p name="b8e0" id="b8e0" class="graf graf--p graf-after--h4">Gluon makes it very easy to <strong class="markup--strong markup--p-strong">define your own objects</strong>. Here’s a class for a multi-layer perceptron. Once again, the imperative programming style allows us to define the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">forward()</em></strong> operation exactly the way we want it: we could apply conditional processing based on network parameters, number of epochs, etc.</p><pre name="c934" id="c934" class="graf graf--pre graf-after--p">class MLP(Block):<br>    def __init__(self, **kwargs):<br>        super(MLP, self).__init__(**kwargs)<br>        with self.name_scope():<br>            self.dense0 = nn.Dense(128)<br>            self.dense1 = nn.Dense(64)<br>            self.dense2 = nn.Dense(10)<br>        <br>    def forward(self, x):<br>        x = nd.relu(self.dense0(x))<br>        x = nd.relu(self.dense1(x))<br>        return self.dense2(x)</pre><p name="01a9" id="01a9" class="graf graf--p graf-after--pre">We can also define <strong class="markup--strong markup--p-strong">custom layers</strong>, as highlighted by <a href="http://gluon.mxnet.io/chapter03_deep-neural-networks/custom-layer.html" data-href="http://gluon.mxnet.io/chapter03_deep-neural-networks/custom-layer.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this example</a> taken from the Gluon documentation. As you can see, they can be <strong class="markup--strong markup--p-strong">seamlessly</strong> integrated with the rest of the Gluon API, so we still rely on existing objects to make our life easier.</p><pre name="edbe" id="edbe" class="graf graf--pre graf-after--p">class CenteredLayer(Block):<br>    def __init__(self, **kwargs):<br>        super(CenteredLayer, self).__init__(**kwargs)<br>    def forward(self, x):<br>        return x - nd.mean(x)</pre><pre name="9a7a" id="9a7a" class="graf graf--pre graf-after--pre">net = nn.Sequential()<br>with net.name_scope():<br>    net = nn.Sequential()<br>    net.add(nn.Dense(128))<br>    net.add(nn.Dense(10))<br>    net.add(CenteredLayer())</pre><h4 name="6197" id="6197" class="graf graf--h4 graf-after--pre">10 — Flexibility and speed: pick two</h4><p name="505f" id="505f" class="graf graf--p graf-after--h4">We discussed earlier the benefits of imperative programming while noting that performance would be inferior to symbolic programming.</p><p name="3ca9" id="3ca9" class="graf graf--p graf-after--p">Let’s run a quick test by <strong class="markup--strong markup--p-strong">predicting 1,000 MNIST images</strong> with this simple multi-layer perceptron (for the sake of brevity, I’ll just show the network definition)</p><pre name="aaf3" id="aaf3" class="graf graf--pre graf-after--p">net = nn.Sequential()<br>with net.name_scope():<br>  net.add(nn.Dense(256, activation=&quot;relu&quot;))<br>  net.add(nn.Dense(128, activation=&quot;relu&quot;))<br>  net.add(nn.Dense(2))<br>  # initialize the parameters<br>  net.collect_params().initialize()<br>  return net</pre><p name="592c" id="592c" class="graf graf--p graf-after--pre">Total prediction time is <strong class="markup--strong markup--p-strong">0.37 second</strong>.</p><p name="91c4" id="91c4" class="graf graf--p graf-after--p">Now, let’s change replace the <em class="markup--em markup--p-em">Sequential</em> object with its hybrid equivalent. This will allow Gluon to compile the network to <strong class="markup--strong markup--p-strong">symbolic form</strong> and to use <strong class="markup--strong markup--p-strong">optimized lower-level primitives</strong>.</p><pre name="3237" id="3237" class="graf graf--pre graf-after--p">net = nn.HybridSequential()<br>with net.name_scope():<br>  net.add(nn.Dense(256, activation=&quot;relu&quot;))<br>  net.add(nn.Dense(128, activation=&quot;relu&quot;))<br>  net.add(nn.Dense(2))<br>  # initialize the parameters<br>  net.collect_params().initialize()<br>  return net</pre><pre name="20cb" id="20cb" class="graf graf--pre graf-after--pre">net.hybridize()</pre><p name="0968" id="0968" class="graf graf--p graf-after--pre">This time, total prediction time is <strong class="markup--strong markup--p-strong">0.21 second</strong>, almost <strong class="markup--strong markup--p-strong">2x faster</strong>. Is there a catch? Well, yes: you lose the flexibility to write a custom <em class="markup--em markup--p-em">forward()</em> function as well as the ability to debug it. Still, once you’ve successfully built and trained a network, <em class="markup--em markup--p-em">hybridizing it is a easy way to improve inference performance</em>.</p><p name="c238" id="c238" class="graf graf--p graf-after--p">For reference, let’s run the same test with the <strong class="markup--strong markup--p-strong">symbolic API</strong> of Apache MXNet.</p><pre name="1009" id="1009" class="graf graf--pre graf-after--p">data = mx.sym.Variable(&#39;data&#39;)<br>data = mx.sym.Flatten(data=data)<br>fc1  = mx.sym.FullyConnected(data=data, name=&#39;fc1&#39;, num_hidden=64)<br>act1 = mx.sym.Activation(data=fc1, name=&#39;relu1&#39;, act_type=&quot;relu&quot;)<br>fc2  = mx.sym.FullyConnected(data=act1, name=&#39;fc2&#39;, num_hidden = 64)<br>act2 = mx.sym.Activation(data=fc2, name=&#39;relu2&#39;, act_type=&quot;relu&quot;)<br>fc3  = mx.sym.FullyConnected(data=act2, name=&#39;fc3&#39;, num_hidden=10)<br>mlp  = mx.sym.SoftmaxOutput(data=fc3, name=&#39;softmax&#39;)</pre><p name="cc8b" id="cc8b" class="graf graf--p graf-after--pre">Prediction time is<strong class="markup--strong markup--p-strong"> 0.16 second</strong>, more than <strong class="markup--strong markup--p-strong">30% faster</strong> than the hybridized version. When <strong class="markup--strong markup--p-strong">top speed</strong> is required — for inference and even more so for training —<strong class="markup--strong markup--p-strong"> </strong>the highly-optimized primitives of MXNet remains the best option.</p><h4 name="5714" id="5714" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="b4bc" id="b4bc" class="graf graf--p graf-after--h4">Gluon has a lot going for it. I think it improves on symbolic MXNet and even on Keras in several respects . The <strong class="markup--strong markup--p-strong">documentation</strong> and the <strong class="markup--strong markup--p-strong">model zoo</strong> alone are worth the price of admission, especially if you’re beginning with Deep Learning. Go try it out and tell me what *you* think :)</p><p name="9f33" id="9f33" class="graf graf--p graf-after--p graf--trailing">As always, thanks for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="803e" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9dcb" id="9dcb" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">Subatomic particles, gamma rays, black holes, lightspeed. Proper Metal material \m/</em></p><figure name="b801" id="b801" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/vY1CZRwV-aw?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/4bce4e56ef55"><time class="dt-published" datetime="2018-02-05T14:04:35.846Z">February 5, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>