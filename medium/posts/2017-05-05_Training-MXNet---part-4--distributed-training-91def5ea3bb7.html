<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Training MXNet — part 4: distributed training</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Training MXNet — part 4: distributed training</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 3, we worked with the CIFAR-10 data set and learned how to tweak optimisation parameters. We ended up training a 110-layer ResNext…
</section>
<section data-field="body" class="e-content">
<section name="0ef9" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="dcfe" id="dcfe" class="graf graf--h3 graf--leading graf--title">Training MXNet — part 4: distributed training</h3><p name="1a5f" id="1a5f" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" data-href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" class="markup--anchor markup--p-anchor" target="_blank">part 3</a>, we worked with the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" data-href="https://www.cs.toronto.edu/~kriz/cifar.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener" target="_blank">CIFAR-10</a> data set and learned how to tweak optimisation parameters. We ended up training a 110-layer ResNext model using all 4 GPUs of a <a href="https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" data-href="https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">g2.8xlarge</a> instance… which took about 12 hours.</p><p name="dd69" id="dd69" class="graf graf--p graf-after--p">In this article, I’ll show you how to use multiple instances to dramatically speed up training. Buckle up!</p><figure name="6c24" id="6c24" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d5beAIGIAj-xco2_E1jq8Q.png" data-width="1213" data-height="662" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*d5beAIGIAj-xco2_E1jq8Q.png"><figcaption class="imageCaption">Training CIFAR-10 on 4 instances and 32 GPUs. Read on!</figcaption></figure><h4 name="372f" id="372f" class="graf graf--h4 graf-after--figure">Creating the master node</h4><p name="522f" id="522f" class="graf graf--p graf-after--h4">We’re going to work with <a href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" data-href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">p2.8xlarge</a> instances running the <strong class="markup--strong markup--p-strong">Deep Learning AMI</strong>, <a href="https://aws.amazon.com/about-aws/whats-new/2017/04/deep-learning-ami-for-ubuntu-v-1-3-apr-2017-now-supports-caffe2/" data-href="https://aws.amazon.com/about-aws/whats-new/2017/04/deep-learning-ami-for-ubuntu-v-1-3-apr-2017-now-supports-caffe2/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ubuntu edition</a>. However, you can easily replicate this with any kind of EC2 instance or even on a bunch of PCs running under your desk :)</p><p name="98da" id="98da" class="graf graf--p graf-after--p">Let’s get started. We’re going to configure the master node the way we like it and then we’ll <strong class="markup--strong markup--p-strong">clone</strong> it to add more instances to our MXNet cluster. The first step is to go to the Marketplace section of the EC2 console and locate the Deep Learning AMI.</p><figure name="1a71" id="1a71" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uJwv3Lm9f2S2-xiMsn0A5A.png" data-width="1836" data-height="396" src="https://cdn-images-1.medium.com/max/800/1*uJwv3Lm9f2S2-xiMsn0A5A.png"></figure><p name="11b8" id="11b8" class="graf graf--p graf-after--figure">Then, select the instance type you’d like to use. Please be mindful of instance costs: a p2.8xlarge is going to cost $7.20 per hour. Don’t worry, you can actually use <strong class="markup--strong markup--p-strong">any instance type</strong>, as MXNet is able to use either the CPU(s) or the GPU(s) of the instance. Obviously, GPU instances will be much faster than t2.micros :)</p><p name="eb72" id="eb72" class="graf graf--p graf-after--p">A few more clicks and you’re done. Just make sure the <strong class="markup--strong markup--p-strong">SSH port</strong> is open and that you have created a new <strong class="markup--strong markup--p-strong">key pair</strong> for the instance (let’s call it <em class="markup--em markup--p-em">ec2</em>). After a few minutes, you can <em class="markup--em markup--p-em">ssh</em> into the master node using the <em class="markup--em markup--p-em">ubuntu</em> user (not the <em class="markup--em markup--p-em">ec2-user</em>).</p><h4 name="dbff" id="dbff" class="graf graf--h4 graf-after--p">Enabling distributed training in MXNET</h4><p name="4dde" id="4dde" class="graf graf--p graf-after--h4">By default, distributed training is not enabled in the source distribution, which means we probably have to rebuild MXNet from source. If your build already includes distributed training, you can skip this section.</p><p name="f41d" id="f41d" class="graf graf--p graf-after--p">The Deep Learning AMI includes the MXNet sources: we just have to make them our own and refresh them to the latest stable version (<strong class="markup--strong markup--p-strong">0.9.5</strong> at the time of writing).</p><figure name="a65b" id="a65b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/563af7489039509b78067e39989952dd.js"></script></figure><p name="3bc7" id="3bc7" class="graf graf--p graf-after--figure">Then, we need to configure our <strong class="markup--strong markup--p-strong">build options</strong>. The last one actually enables distributed training.</p><figure name="f935" id="f935" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/23d1464892c46c0a047641036225a27e.js"></script></figure><p name="f129" id="f129" class="graf graf--p graf-after--figure">Now we can build and install the library. No need to add dependencies, as they’re already included in the AMI. I’m running a parallel make on 32 cores because that’s what a p2.8xlarge has.</p><figure name="c360" id="c360" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/d01dcdb9a6e708a7fc9dbf81634f0f6a.js"></script></figure><p name="fc44" id="fc44" class="graf graf--p graf-after--figure">Once the library is installed, it’s a good idea to run a quick Python check.</p><figure name="7d78" id="7d78" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*X5pp1OEKVYcIjV678m6mkg.png" data-width="496" data-height="138" src="https://cdn-images-1.medium.com/max/800/1*X5pp1OEKVYcIjV678m6mkg.png"></figure><p name="c53b" id="c53b" class="graf graf--p graf-after--figure">Ok, this looks good. Let’s move on.</p><h4 name="26e8" id="26e8" class="graf graf--h4 graf-after--p">Opening ports for distributed training</h4><p name="2c26" id="2c26" class="graf graf--p graf-after--h4">The master node and the worker nodes need to talk to one another to share the <strong class="markup--strong markup--p-strong">data set</strong> as well as <strong class="markup--strong markup--p-strong">training results</strong>. Thus, we need to alter the configuration of our security group to allow this.</p><p name="8e0a" id="8e0a" class="graf graf--p graf-after--p">The absolute simplest way to do this is to allow <strong class="markup--strong markup--p-strong">all TCP</strong> communication between instances of the MXNet cluster, i.e. instances using the <strong class="markup--strong markup--p-strong">same security group</strong>.</p><p name="e624" id="e624" class="graf graf--p graf-after--p">To do this, go to the EC2 console and edit the inbound rules of the security group of the master node. Add a rule allowing <strong class="markup--strong markup--p-strong">all TCP traffic</strong> and use the actual name of the security group to <strong class="markup--strong markup--p-strong">restrict</strong> source traffic.</p><figure name="a6d8" id="a6d8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*T3qQeoMpVSTBP4Z0gAXlgQ.png" data-width="1720" data-height="150" src="https://cdn-images-1.medium.com/max/800/1*T3qQeoMpVSTBP4Z0gAXlgQ.png"></figure><p name="f55c" id="f55c" class="graf graf--p graf-after--figure">Our instance is now ready. Let’s create the worker nodes.</p><h4 name="e817" id="e817" class="graf graf--h4 graf-after--p">Creating the worker nodes</h4><p name="bcd9" id="bcd9" class="graf graf--p graf-after--h4">We’re going to create a <strong class="markup--strong markup--p-strong">new AMI</strong> based on the master node. Then, we’ll use it to launch the workers. Locate your instance in the EC2 console and create an image.</p><figure name="004a" id="004a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tgE58qRXMUAab-Jzypl5_w.png" data-width="1162" data-height="402" src="https://cdn-images-1.medium.com/max/800/1*tgE58qRXMUAab-Jzypl5_w.png"></figure><p name="65a2" id="65a2" class="graf graf--p graf-after--figure">After a few minutes, you’ll see the new AMI in the “Images” section of the EC2 console. You can now use it to launch your worker nodes.</p><p name="b440" id="b440" class="graf graf--p graf-after--p">Nothing complicated here: select the <strong class="markup--strong markup--p-strong">instance type</strong>, the <strong class="markup--strong markup--p-strong">number</strong> of instances you’d like to launch (3 in my case) and <strong class="markup--strong markup--p-strong">the same security group</strong> as the master node.</p><p name="848b" id="848b" class="graf graf--p graf-after--p">A few more minutes and your instances are ready.</p><figure name="bc7f" id="bc7f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*gisH2Q96WSZKOdbmnC5kHA.png" data-width="1082" data-height="250" src="https://cdn-images-1.medium.com/max/800/1*gisH2Q96WSZKOdbmnC5kHA.png"></figure><p name="dc4d" id="dc4d" class="graf graf--p graf-after--figure">Lovely. Write down the private IP adresses of each instance, we’re going to need them in a second.</p><h4 name="1568" id="1568" class="graf graf--h4 graf-after--p">Configuring the cluster</h4><p name="cdde" id="cdde" class="graf graf--p graf-after--h4">Let’s log in to the master node, move to the <em class="markup--em markup--p-em">tools</em> directory and look at the launcher.</p><figure name="df65" id="df65" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d7cgot72NFlDKhGtGcqXlA.png" data-width="477" data-height="85" src="https://cdn-images-1.medium.com/max/800/1*d7cgot72NFlDKhGtGcqXlA.png"></figure><p name="1386" id="1386" class="graf graf--p graf-after--figure">This is the tool we’ll use to start training on all nodes (master node included). It does two things:</p><ul class="postList"><li name="4415" id="4415" class="graf graf--li graf-after--p">using rsync, <strong class="markup--strong markup--li-strong">copy the data set</strong> in <em class="markup--em markup--li-em">/tmp/mxnet</em> on each node. Alternatively, we could avoid this by sharing the data set across the nodes with home-made NFS or <a href="https://aws.amazon.com/efs/" data-href="https://aws.amazon.com/efs/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon EFS</a>.</li><li name="7318" id="7318" class="graf graf--li graf-after--li">using ssh, <strong class="markup--strong markup--li-strong">run the python script</strong> that starts training. As you can see, other protocols are available, but we won’t look at them today.</li></ul><h4 name="e50b" id="e50b" class="graf graf--h4 graf-after--li">Creating the hosts file</h4><p name="e440" id="e440" class="graf graf--p graf-after--h4"><em class="markup--em markup--p-em">launch.py</em> needs the private IP address of all nodes (including the master node) to be declared in a file. It should look something like this.</p><figure name="6053" id="6053" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Z-QWGfoYwkBEc-aiIcRZ9w.png" data-width="530" data-height="84" src="https://cdn-images-1.medium.com/max/800/1*Z-QWGfoYwkBEc-aiIcRZ9w.png"></figure><h4 name="25c8" id="25c8" class="graf graf--h4 graf-after--figure">Configuring SSH</h4><p name="9860" id="9860" class="graf graf--p graf-after--h4">We need password-less <em class="markup--em markup--p-em">ssh</em> access between the master node and the worker nodes. If you already have this in place, you can skip this section.</p><p name="d3e2" id="d3e2" class="graf graf--p graf-after--p">We’ll keep things simple by creating a new key pair on our local computer and distributing it across the cluster.</p><blockquote name="1bc1" id="1bc1" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong">PLEASE</strong> do not reuse the <em class="markup--em markup--blockquote-em">ec2</em> key pair, it’s bad practice. Also, some of you may be tempted to bake the key pair in the AMI to avoid distributing it to all instances, but I would recommend against doing that since it means storing the private key on all nodes instead of just the master node. And <a href="https://heipei.github.io/2015/02/26/SSH-Agent-Forwarding-considered-harmful/" data-href="https://heipei.github.io/2015/02/26/SSH-Agent-Forwarding-considered-harmful/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">ssh agent forwarding isn’t great either</a>.</blockquote><figure name="d7b4" id="d7b4" class="graf graf--figure graf--iframe graf-after--blockquote"><script src="https://gist.github.com/juliensimon/eee83def5c7a78d95e704b646ab37734.js"></script></figure><p name="4383" id="4383" class="graf graf--p graf-after--figure">Next, still from our local computer, we’re going to copy the <strong class="markup--strong markup--p-strong">public key</strong> to all nodes (including the master node) and the <strong class="markup--strong markup--p-strong">private key</strong> to the master node only.</p><figure name="63fc" id="63fc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/f91d5936537fe1f011aa0e8688ce3e16.js"></script></figure><p name="46a3" id="46a3" class="graf graf--p graf-after--figure">Finally, on the master node, we’ll start <em class="markup--em markup--p-em">ssh-agent</em> and add the <em class="markup--em markup--p-em">mxnet</em> identity.</p><figure name="b6f1" id="b6f1" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8035ad6829a3a2767d9e7317c2efd682.js"></script></figure><p name="03ee" id="03ee" class="graf graf--p graf-after--figure">You should now be able to log in <strong class="markup--strong markup--p-strong">from the master node to each worker node</strong> (including the master node itself). Please make sure that this is working properly before going on.</p><figure name="91ab" id="91ab" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/2593cc3174b503d3359c3b25f723eaef.js"></script></figure><p name="8f38" id="8f38" class="graf graf--p graf-after--figure">If it does, you’re ready to train, buddy :)</p><h4 name="7a67" id="7a67" class="graf graf--h4 graf-after--p">Launching distributed training</h4><p name="034f" id="034f" class="graf graf--p graf-after--h4">Here’s the magic command: 4 nodes listed in the <em class="markup--em markup--p-em">hosts</em> file will receive a copy of the data set in <em class="markup--em markup--p-em">/tmp/mxnet </em>via <em class="markup--em markup--p-em">rsync</em>. Then, the master node will run the <em class="markup--em markup--p-em">train_cifar10.py</em> script on each node, training a 110-layer ResNext model on all 8 GPUs.</p><figure name="663c" id="663c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/da33a2aa889782bf13954d7447164498.js"></script></figure><blockquote name="cdf1" id="cdf1" class="graf graf--blockquote graf-after--figure">If you’re running on CPU instances, just remove the gpus parameters.</blockquote><p name="b200" id="b200" class="graf graf--p graf-after--blockquote">The PS_VERBOSE variable will output extra information. Very useful in case something goes wrong ;)</p><p name="ac07" id="ac07" class="graf graf--p graf-after--p">You can check progress by logging in on the different nodes and running the ‘<em class="markup--em markup--p-em">nvidia-smi -l</em>’ command.</p><figure name="2291" id="2291" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d5beAIGIAj-xco2_E1jq8Q.png" data-width="1213" data-height="662" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*d5beAIGIAj-xco2_E1jq8Q.png"></figure><p name="dd61" id="dd61" class="graf graf--p graf-after--figure">So how fast is this? As I mentioned before, it took about 12 hours to run 300 epochs on the 4 GPUs of a g2.8xlarge instance. The combined 32 GPUs of the 4 p2.8xlarge instances did it in <strong class="markup--strong markup--p-strong">91 minutes!</strong></p><p name="a1f7" id="a1f7" class="graf graf--p graf-after--p">That’s an <strong class="markup--strong markup--p-strong">8x speedup</strong>, which kind of makes sense since we have <strong class="markup--strong markup--p-strong">8x more GPUs</strong>. I had read about it and now I see it with my own eyes: <strong class="markup--strong markup--p-strong">linear scaling</strong> indeed! This makes me want to push it to 256 GPUs: it would only require 16 p2.16xlarge after all :D</p><p name="6c8d" id="6c8d" class="graf graf--p graf-after--p">Last but not least, my colleagues Naveen Swamy and Joseph Spisak wrote a very interesting <a href="https://aws.amazon.com/blogs/compute/distributed-deep-learning-made-easy/" data-href="https://aws.amazon.com/blogs/compute/distributed-deep-learning-made-easy/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">blog post</a> on how to automate most of this stuff using AWS CloudFormation. This is definitely worth reading if you’re running everything in AWS.</p><p name="30ad" id="30ad" class="graf graf--p graf-after--p graf--trailing">That’s it for today. Thank you very much for reading and for all the friendly support I’ve been receiving lately. It means a lot to me!</p></div></div></section><section name="1628" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="5a81" id="5a81" class="graf graf--p graf--leading">Next:</p><p name="dad0" id="dad0" class="graf graf--p graf-after--p graf--trailing"><a href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" data-href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" class="markup--anchor markup--p-anchor" target="_blank">Part 5 — Distributed training, EFS edition</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/91def5ea3bb7"><time class="dt-published" datetime="2017-05-05T12:45:39.173Z">May 5, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>