<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>An introduction to the MXNet API — part 3</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">An introduction to the MXNet API — part 3</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 2, we discussed how Symbols allow us to define computation graphs processing data stored in NDArrays (which we studied in part 1).
</section>
<section data-field="body" class="e-content">
<section name="4009" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="050c" id="050c" class="graf graf--h3 graf--leading graf--title">An introduction to the MXNet API — part 3</h3><p name="7ea0" id="7ea0" class="graf graf--p graf-after--h3"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-2-ce761513124e" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-2-ce761513124e" class="markup--anchor markup--p-anchor" target="_blank">In part 2</a>, we discussed how Symbols allow us to define computation graphs processing data stored in NDArrays (which we studied in <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" class="markup--anchor markup--p-anchor" target="_blank">part 1</a>).</p><p name="a26c" id="a26c" class="graf graf--p graf-after--p">In this article, we’re going to use what we learned on <em class="markup--em markup--p-em">Symbols</em> and <em class="markup--em markup--p-em">NDArrays</em> to prepare some data and build a neural network. Then, we’ll use the <a href="http://mxnet.io/api/python/module.html" data-href="http://mxnet.io/api/python/module.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Module API</a> to train the network and predict results.</p><h4 name="649f" id="649f" class="graf graf--h4 graf-after--p">Defining our data set</h4><p name="9a08" id="9a08" class="graf graf--p graf-after--h4">Our (imaginary) data set is composed of <strong class="markup--strong markup--p-strong">1000 data samples</strong></p><ul class="postList"><li name="980e" id="980e" class="graf graf--li graf-after--p">Each sample has <strong class="markup--strong markup--li-strong">100 features</strong>.</li><li name="90b8" id="90b8" class="graf graf--li graf-after--li">A feature is represented by a <strong class="markup--strong markup--li-strong">float value between 0 and 1</strong>.</li><li name="55e6" id="55e6" class="graf graf--li graf-after--li">Samples are split in <strong class="markup--strong markup--li-strong">10 categories</strong>. The purpose of the network will be to predict the correct category for a given sample.</li><li name="ed61" id="ed61" class="graf graf--li graf-after--li">We’ll use 800 samples for <strong class="markup--strong markup--li-strong">training</strong> and 200 samples for <strong class="markup--strong markup--li-strong">validation</strong>.</li><li name="d78a" id="d78a" class="graf graf--li graf-after--li">We’ll use a <strong class="markup--strong markup--li-strong">batch size</strong> of 10 for training and validation</li></ul><pre name="df70" id="df70" class="graf graf--pre graf-after--li">import mxnet as mx<br>import numpy as np<br>import logging</pre><pre name="ee6f" id="ee6f" class="graf graf--pre graf-after--pre">logging.basicConfig(level=logging.INFO)</pre><pre name="1b16" id="1b16" class="graf graf--pre graf-after--pre">sample_count = 1000<br>train_count = 800<br>valid_count = sample_count - train_count</pre><pre name="19d3" id="19d3" class="graf graf--pre graf-after--pre">feature_count = 100<br>category_count = 10<br>batch=10</pre><h4 name="74bf" id="74bf" class="graf graf--h4 graf-after--pre">Generating the data set</h4><p name="c1e8" id="c1e8" class="graf graf--p graf-after--h4">Let’s use a uniform distribution to generate the 1000 samples. They are stored in an <em class="markup--em markup--p-em">NDArray</em> named ‘X’: <strong class="markup--strong markup--p-strong">1000 lines, 100 columns</strong>.</p><pre name="ece8" id="ece8" class="graf graf--pre graf-after--p">X = mx.nd.uniform(low=0, high=1, shape=(sample_count,feature_count))</pre><pre name="b5b2" id="b5b2" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; X.shape<br>(1000L, 100L)<br>&gt;&gt;&gt; X.asnumpy()<br>array([[ 0.70029777,  0.28444085,  0.46263582, ...,  0.73365158,<br>         0.99670047,  0.5961988 ],<br>       [ 0.34659418,  0.82824177,  0.72929877, ...,  0.56012964,<br>         0.32261589,  0.35627609],<br>       [ 0.10939316,  0.02995235,  0.97597599, ...,  0.20194994,<br>         0.9266268 ,  0.25102937],<br>       ...,<br>       [ 0.69691515,  0.52568913,  0.21130568, ...,  0.42498392,<br>         0.80869114,  0.23635457],<br>       [ 0.3562004 ,  0.5794751 ,  0.38135922, ...,  0.6336484 ,<br>         0.26392782,  0.30010447],<br>       [ 0.40369365,  0.89351988,  0.88817406, ...,  0.13799617,<br>         0.40905532,  0.05180593]], dtype=float32)</pre><p name="d5c0" id="d5c0" class="graf graf--p graf-after--pre">The categories for these 1000 samples are represented as <strong class="markup--strong markup--p-strong">integers</strong> in the 0–9 range. They are randomly generated and stored in an <em class="markup--em markup--p-em">NDArray</em> named ‘Y’.</p><pre name="60cf" id="60cf" class="graf graf--pre graf-after--p">Y = mx.nd.empty((sample_count,))<br>for i in range(0,sample_count-1):<br>  Y[i] = np.random.randint(0,category_count)</pre><pre name="827a" id="827a" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; Y.shape<br>(1000L,)<br>&gt;&gt;&gt; Y[0:10].asnumpy()<br>array([ 3.,  3.,  1.,  9.,  4.,  7.,  3.,  5.,  2.,  2.], dtype=float32)</pre><h4 name="396f" id="396f" class="graf graf--h4 graf-after--pre">Splitting the data set</h4><p name="8136" id="8136" class="graf graf--p graf-after--h4">Next, we’re splitting the data set <strong class="markup--strong markup--p-strong">80/20</strong> for <strong class="markup--strong markup--p-strong">training</strong> and <strong class="markup--strong markup--p-strong">validation</strong>. We use the <em class="markup--em markup--p-em">NDArray.crop</em> function to do this. Here, the data set is completely random, so we can use the top 80% for training and the bottom 20% for validation. In real life, we’d probably <strong class="markup--strong markup--p-strong">shuffle</strong> the data set first, in order to avoid potential bias on sequentially-generated data.</p><pre name="6487" id="6487" class="graf graf--pre graf-after--p">X_train = mx.nd.crop(X, begin=(0,0), end=(train_count,feature_count-1))</pre><pre name="95ca" id="95ca" class="graf graf--pre graf-after--pre">X_valid = mx.nd.crop(X, begin=(train_count,0), end=(sample_count,feature_count-1))</pre><pre name="0f8c" id="0f8c" class="graf graf--pre graf-after--pre">Y_train = Y[0:train_count]</pre><pre name="e0c3" id="e0c3" class="graf graf--pre graf-after--pre">Y_valid = Y[train_count:sample_count]</pre><p name="886a" id="886a" class="graf graf--p graf-after--pre">Our data is now ready!</p><h4 name="5541" id="5541" class="graf graf--h4 graf-after--p">Building the network</h4><p name="6e1d" id="6e1d" class="graf graf--p graf-after--h4">Our network is pretty simple. Let’s look at each layer:</p><ul class="postList"><li name="2890" id="2890" class="graf graf--li graf-after--p">The <strong class="markup--strong markup--li-strong">input layer</strong> is represented by a <em class="markup--em markup--li-em">Symbol</em> named ‘data’. We’ll bind it to the actual input data later on.</li></ul><pre name="70f8" id="70f8" class="graf graf--pre graf-after--li">data = mx.sym.Variable(&#39;data&#39;)</pre><ul class="postList"><li name="1ab4" id="1ab4" class="graf graf--li graf-after--pre"><em class="markup--em markup--li-em">fc1</em>, the <strong class="markup--strong markup--li-strong">first hidden layer</strong> is built from <strong class="markup--strong markup--li-strong">64 fully-connected neurons</strong>, i.e. each feature in the input layer is connected to all 64 neurons. As you can see, we use the high-level <em class="markup--em markup--li-em">Symbol.FullyConnected</em> function, which is much more convenient than building each connection manually!</li></ul><pre name="5582" id="5582" class="graf graf--pre graf-after--li">fc1 = mx.sym.FullyConnected(data, name=&#39;fc1&#39;, num_hidden=64)</pre><ul class="postList"><li name="48b5" id="48b5" class="graf graf--li graf-after--pre">Each output of <em class="markup--em markup--li-em">fc1</em> goes through an <a href="https://en.wikipedia.org/wiki/Activation_function" data-href="https://en.wikipedia.org/wiki/Activation_function" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">activation function</strong></a>. Here we use a <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" data-href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">rectified linear unit</a>, aka ‘relu’. I promised minimal theory, so let’s just say that an activation function is how we decide whether a neuron should “fire” or not, i.e. whether its inputs are meaningful enough in predicting the correct result.</li></ul><pre name="7f79" id="7f79" class="graf graf--pre graf-after--li">relu1 = mx.sym.Activation(fc1, name=&#39;relu1&#39;, act_type=&quot;relu&quot;)</pre><ul class="postList"><li name="4ffe" id="4ffe" class="graf graf--li graf-after--pre"><em class="markup--em markup--li-em">fc2</em>, the <strong class="markup--strong markup--li-strong">second hidden layer</strong> is built from <strong class="markup--strong markup--li-strong">10 fully-connected neurons</strong>, which map to our <strong class="markup--strong markup--li-strong">10 categories</strong>. Each neuron outputs a float value of arbitrary scale. The largest of the 10 values represents the <strong class="markup--strong markup--li-strong">most likely category</strong> for the data sample.</li></ul><pre name="6cfc" id="6cfc" class="graf graf--pre graf-after--li">fc2 = mx.sym.FullyConnected(relu1, name=&#39;fc2&#39;, num_hidden=category_count)</pre><ul class="postList"><li name="a6e6" id="a6e6" class="graf graf--li graf-after--pre">The <strong class="markup--strong markup--li-strong">output layer</strong> applies the <a href="https://en.wikipedia.org/wiki/Softmax_function" data-href="https://en.wikipedia.org/wiki/Softmax_function" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Softmax function</strong></a> to the 10 values coming from the <em class="markup--em markup--li-em">fc2</em> layer: they are transformed into 10 values between 0 and 1 that add up to 1. Each value represents the <strong class="markup--strong markup--li-strong">predicted probability for each category</strong>, the largest one pointing at the most likely category.</li></ul><pre name="e2c5" id="e2c5" class="graf graf--pre graf-after--li">out = mx.sym.SoftmaxOutput(fc2, name=&#39;softmax&#39;)<br>mod = mx.mod.Module(out)</pre><h4 name="7a89" id="7a89" class="graf graf--h4 graf-after--pre">Building the data iterator</h4><p name="fb89" id="fb89" class="graf graf--p graf-after--h4">In part 1, we saw that neural networks not trained one sample at a time, as this is quite inefficient from a performance point of view. Instead, we use <strong class="markup--strong markup--p-strong">batches</strong>, i.e. <strong class="markup--strong markup--p-strong">a fixed number of samples</strong>.</p><p name="70d3" id="70d3" class="graf graf--p graf-after--p">In order to deliver these batches to the network, we need to build an <strong class="markup--strong markup--p-strong">iterator</strong> using the <em class="markup--em markup--p-em">NDArrayIter</em> function. Its parameters are the <strong class="markup--strong markup--p-strong">training data</strong>, the categories (MXNet calls these <strong class="markup--strong markup--p-strong">labels</strong>) and the <strong class="markup--strong markup--p-strong">batch size</strong>.</p><p name="096a" id="096a" class="graf graf--p graf-after--p">As you can see, we can indeed iterate on the data set, 10 samples and 10 labels at a time. We then call the <em class="markup--em markup--p-em">reset()</em> function to restore the iterator to its original state.</p><pre name="9cbc" id="9cbc" class="graf graf--pre graf-after--p">train_iter = mx.io.NDArrayIter(data=X_train,label=Y_train,batch_size=batch)</pre><pre name="426b" id="426b" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; for batch in train_iter:<br>...   print batch.data<br>...   print batch.label<br>...<br>[&lt;NDArray 10x99 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>[&lt;NDArray 10 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>[&lt;NDArray 10x99 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>[&lt;NDArray 10 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>[&lt;NDArray 10x99 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br>[&lt;NDArray 10 <a href="http://twitter.com/cpu" data-href="http://twitter.com/cpu" class="markup--anchor markup--pre-anchor" title="Twitter profile for @cpu" rel="noopener" target="_blank">@cpu</a>(0)&gt;]<br><em class="markup--em markup--pre-em">&lt;edited for brevity&gt;<br>&gt;&gt;&gt; </em>train_iter.reset()</pre><p name="6155" id="6155" class="graf graf--p graf-after--pre">Our network is now ready for training!</p><h4 name="5a39" id="5a39" class="graf graf--h4 graf-after--p">Training the model</h4><p name="db4b" id="db4b" class="graf graf--p graf-after--h4">First, let’s <strong class="markup--strong markup--p-strong">bind</strong> the input symbol to the actual data set (samples and labels). This is where the iterator comes in handy.</p><pre name="dbdb" id="dbdb" class="graf graf--pre graf-after--p">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)</pre><p name="dd56" id="dd56" class="graf graf--p graf-after--pre">Next, let’s <strong class="markup--strong markup--p-strong">initialize</strong> the neuron weights in the network. This is actually a very important step: initializing them with the “right” technique will help the network learn <strong class="markup--strong markup--p-strong">much faster</strong>. The Xavier initializer (named after his inventor, Xavier Glorot —<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" data-href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> PDF</a>) is one of these techniques.</p><pre name="7dc8" id="7dc8" class="graf graf--pre graf-after--p"># Allowed, but not efficient<br>mod.init_params()<br># Much better<br>mod.init_params(initializer=mx.init.Xavier(magnitude=2.))</pre><p name="7812" id="7812" class="graf graf--p graf-after--pre">Next, we need to define the <strong class="markup--strong markup--p-strong">optimization</strong> parameters:</p><ul class="postList"><li name="1f56" id="1f56" class="graf graf--li graf-after--p">we’re using the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" data-href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Stochastic Gradient Descent</strong></a> algorithm (aka SGD), which has long been used for Machine Learning and Deep Learning application.</li><li name="e2cc" id="e2cc" class="graf graf--li graf-after--li">we’re setting the <strong class="markup--strong markup--li-strong">learning rate</strong> to 0.1, a pretty typical value for SGD.</li></ul><pre name="fa49" id="fa49" class="graf graf--pre graf-after--li">mod.init_optimizer(optimizer=&#39;sgd&#39;, optimizer_params=((&#39;learning_rate&#39;, 0.1), ))</pre><p name="18d9" id="18d9" class="graf graf--p graf-after--pre">And finally, we can train the network! We’re doing it over 50 <strong class="markup--strong markup--p-strong">epochs</strong>, which means the full data set will flow 50 times through the network (in batches of 10 samples).</p><pre name="3abf" id="3abf" class="graf graf--pre graf-after--p">mod.fit(train_iter, num_epoch=50)</pre><pre name="2222" id="2222" class="graf graf--pre graf-after--pre">INFO:root:Epoch[0] Train-accuracy=0.097500<br>INFO:root:Epoch[0] Time cost=0.085<br>INFO:root:Epoch[1] Train-accuracy=0.122500<br>INFO:root:Epoch[1] Time cost=0.074<br>INFO:root:Epoch[2] Train-accuracy=0.153750<br>INFO:root:Epoch[2] Time cost=0.087<br>INFO:root:Epoch[3] Train-accuracy=0.162500<br>INFO:root:Epoch[3] Time cost=0.082<br>INFO:root:Epoch[4] Train-accuracy=0.192500<br>INFO:root:Epoch[4] Time cost=0.094<br>INFO:root:Epoch[5] Train-accuracy=0.210000<br>INFO:root:Epoch[5] Time cost=0.108<br>INFO:root:Epoch[6] Train-accuracy=0.222500<br>INFO:root:Epoch[6] Time cost=0.104<br>INFO:root:Epoch[7] Train-accuracy=0.243750<br>INFO:root:Epoch[7] Time cost=0.110<br>INFO:root:Epoch[8] Train-accuracy=0.263750<br>INFO:root:Epoch[8] Time cost=0.101<br>INFO:root:Epoch[9] Train-accuracy=0.286250<br>INFO:root:Epoch[9] Time cost=0.097<br>INFO:root:Epoch[10] Train-accuracy=0.306250<br>INFO:root:Epoch[10] Time cost=0.100<br>...<br>INFO:root:Epoch[20] Train-accuracy=0.507500<br>...<br>INFO:root:Epoch[30] Train-accuracy=0.718750<br>...<br>INFO:root:Epoch[40] Train-accuracy=0.923750<br>...<br>INFO:root:Epoch[50] Train-accuracy=0.998750<br>INFO:root:Epoch[50] Time cost=0.077</pre><p name="3d9b" id="3d9b" class="graf graf--p graf-after--pre">As we can see, the training accuracy rises rapidly and reaches <strong class="markup--strong markup--p-strong">99+%</strong> after 50 epochs. It looks like our network was able to learn the training set. That’s pretty impressive!</p><p name="a801" id="a801" class="graf graf--p graf-after--p">But how does it perform against the validation set?</p><h4 name="f6e8" id="f6e8" class="graf graf--h4 graf-after--p">Validating the model</h4><p name="1e6a" id="1e6a" class="graf graf--p graf-after--h4">Now we’re going to throw new data samples at the network, i.e. the 20% that <strong class="markup--strong markup--p-strong">haven’t</strong> been used for training.</p><p name="0ac9" id="0ac9" class="graf graf--p graf-after--p">First, we’re building an iterator. This time, we’re using the <strong class="markup--strong markup--p-strong">validation</strong> samples and labels.</p><pre name="7809" id="7809" class="graf graf--pre graf-after--p">pred_iter = mx.io.NDArrayIter(data=X_valid,label=Y_valid, batch_size=batch)</pre><p name="2bc0" id="2bc0" class="graf graf--p graf-after--pre">Next, using the <em class="markup--em markup--p-em">Module.iter_predict</em>() function, we’re going to run these samples through the network. As we do this, we’re going to compare the <strong class="markup--strong markup--p-strong">predicted label</strong> with the <strong class="markup--strong markup--p-strong">actual label</strong>. We’ll keep track of the score and display the <strong class="markup--strong markup--p-strong">validation accuracy</strong>, i.e. how well the network did on the validation set.</p><pre name="4d78" id="4d78" class="graf graf--pre graf-after--p">pred_count = valid_count<br>correct_preds = total_correct_preds = 0</pre><pre name="46f8" id="46f8" class="graf graf--pre graf-after--pre">for preds, i_batch, batch in mod.iter_predict(pred_iter):<br>    label = batch.label[0].asnumpy().astype(int)<br>    pred_label = preds[0].asnumpy().argmax(axis=1)<br>    correct_preds = np.sum(pred_label==label)<br>    total_correct_preds = total_correct_preds + correct_preds</pre><pre name="2b72" id="2b72" class="graf graf--pre graf-after--pre">print(&#39;Validation accuracy: %2.2f&#39; % (1.0*total_correct_preds/pred_count))</pre><p name="c742" id="c742" class="graf graf--p graf-after--pre">There is quite a bit happening here :)</p><p name="910d" id="910d" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">iter_predict</em>() returns:</p><ul class="postList"><li name="ce01" id="ce01" class="graf graf--li graf-after--p"><em class="markup--em markup--li-em">i_batch</em>: the batch number</li><li name="3c44" id="3c44" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">batch</em>: an array of NDArrays. Here, it holds a single <em class="markup--em markup--li-em">NDArray</em> storing the current batch. We’re using it to find the labels of the 10 data samples in the current batch. We store them in the <em class="markup--em markup--li-em">label</em> numpy array (10 elements).</li><li name="b238" id="b238" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">preds</em>: an array of <em class="markup--em markup--li-em">NDArrays</em>. Here, it holds a single <em class="markup--em markup--li-em">NDArray</em> storing predicted labels for the current batch: for each sample, we have <strong class="markup--strong markup--li-strong">predicted probabilities for all 10 categories </strong>(10x10 matrix). Thus, we’re using <em class="markup--em markup--li-em">argmax</em>() to find the <strong class="markup--strong markup--li-strong">index</strong> of the highest value, i.e. the <strong class="markup--strong markup--li-strong">most likely category</strong>. Thus, <em class="markup--em markup--li-em">pred_label</em> is a 10-element array holding the predicted category for each data sample in the current batch.</li></ul><p name="d2e3" id="d2e3" class="graf graf--p graf-after--li">Then, we’re comparing the number of equal values in <em class="markup--em markup--p-em">label</em> and <em class="markup--em markup--p-em">pred_label</em> using <em class="markup--em markup--p-em">Numpy.sum()</em>.</p><p name="bbfa" id="bbfa" class="graf graf--p graf-after--p">Finally, we compute and display the validation accuracy.</p><pre name="de32" id="de32" class="graf graf--pre graf-after--p">Validation accuracy: 0.09</pre><p name="c5d0" id="c5d0" class="graf graf--p graf-after--pre">What? 9%? <strong class="markup--strong markup--p-strong">This is really bad!</strong> If you needed proof that our data set was random, there you have it!</p><p name="9cbb" id="9cbb" class="graf graf--p graf-after--p">The bottom line is that you can indeed train a neural network to learn <strong class="markup--strong markup--p-strong">anything</strong>, but if your data is <strong class="markup--strong markup--p-strong">meaningless</strong> (like ours here), it won’t be able to predict anything. <strong class="markup--strong markup--p-strong">Garbage in, garbage out!</strong></p><p name="4a20" id="4a20" class="graf graf--p graf-after--p graf--trailing">If you read this far, I guess you deserve to get the full code for this example ;) Please take some time to use it on your own data, it’s the best way to learn.</p></div></div></section><section name="37ed" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="7ec5" id="7ec5" class="graf graf--p graf--leading">Next :</p><ul class="postList"><li name="8171" id="8171" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" class="markup--anchor markup--li-anchor" target="_blank">Part 4</a>: Using a pre-trained model for image classification (Inception v3)</li><li name="1261" id="1261" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" class="markup--anchor markup--li-anchor" target="_blank">Part 5</a>: More pre-trained models (VGG16 and ResNet-152)</li><li name="e12d" id="e12d" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--li-anchor" target="_blank">Part 6</a>: Real-time object detection on a Raspberry Pi (and it speaks, too!)</li></ul><figure name="5c99" id="5c99" class="graf graf--figure graf--iframe graf-after--li graf--trailing"><script src="https://gist.github.com/juliensimon/7cfef0423b0183e891774a289e156b49.js"></script></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/1803112ba3a8"><time class="dt-published" datetime="2017-04-12T22:37:42.209Z">April 12, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
