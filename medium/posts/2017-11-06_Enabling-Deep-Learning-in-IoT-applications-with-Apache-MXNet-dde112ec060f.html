<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Enabling Deep Learning in IoT applications with Apache MXNet</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Enabling Deep Learning in IoT applications with Apache MXNet</h1>
</header>
<section data-field="subtitle" class="p-summary">
Apache MXNet [1] is an Open Source library for Deep Learning. Thanks to a high-level API available in several languages (Python, C++, R…
</section>
<section data-field="body" class="e-content">
<section name="2927" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="e73e" id="e73e" class="graf graf--h3 graf--leading graf--title">Enabling Deep Learning in IoT applications with Apache MXNet</h3><p name="8d8a" id="8d8a" class="graf graf--p graf-after--h3"><a href="https://mxnet.incubator.apache.org/" data-href="https://mxnet.incubator.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> [1] is an Open Source library for Deep Learning. Thanks to a high-level API available in several languages (Python, C++, R, etc.), software developers and researchers can build Deep Learning models to help their applications make smarter decisions.</p><figure name="01b6" id="01b6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Grf_YJoC2A748ZPYHlB-vw.png" data-width="650" data-height="366" src="https://cdn-images-1.medium.com/max/800/1*Grf_YJoC2A748ZPYHlB-vw.png"></figure><p name="f2c5" id="f2c5" class="graf graf--p graf-after--figure">However, many state of the art models have hefty <strong class="markup--strong markup--p-strong">compute</strong>, <strong class="markup--strong markup--p-strong">storage</strong> and <strong class="markup--strong markup--p-strong">power consumption</strong> requirements which make them impractical — or even impossible — to use on resource-constrained devices. For example, the 500MB+<a href="https://arxiv.org/abs/1409.1556" data-href="https://arxiv.org/abs/1409.1556" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> VGG-16</a> Convolution Neural Network (CNN) model is too large to fit on a Raspberry Pi 3, which is not a tiny IoT device in itself. Other models may fit, but they usually suffer from pretty <strong class="markup--strong markup--p-strong">slow inference times</strong>, a significant problem when fast processing is required.</p><p name="3a20" id="3a20" class="graf graf--p graf-after--p">Is IoT then doomed to helplessly watch the AI revolution go by?</p><p name="0800" id="0800" class="graf graf--p graf-after--p">Of course not. Apache MXNet is <strong class="markup--strong markup--p-strong">IoT-friendly</strong> in several ways. In addition, several AWS services also make it pretty easy to deploy MXNet models <strong class="markup--strong markup--p-strong">at the Edge</strong>. Let’s dive deeper.</p><h4 name="d2c1" id="d2c1" class="graf graf--h4 graf-after--p">Lazy evaluation</h4><p name="8731" id="8731" class="graf graf--p graf-after--h4">One of the key features of Apache MXNet is <strong class="markup--strong markup--p-strong">lazy evaluation</strong>: data processing operations are only executed when strictly necessary. This allows many <strong class="markup--strong markup--p-strong">optimization</strong> techniques to be applied, such as avoiding unnecessary calculations or reusing memory buffers. Obviously, this behavior greatly contributes to <strong class="markup--strong markup--p-strong">speed</strong> and <strong class="markup--strong markup--p-strong">memory efficiency</strong>, both key advantages for IoT projects.</p><h4 name="854a" id="854a" class="graf graf--h4 graf-after--p">Shrinking models</h4><p name="599e" id="599e" class="graf graf--p graf-after--h4">In the last few years, significant advances have been made in shrinking Deep Learning models without losing accuracy. Thanks to operations like <strong class="markup--strong markup--p-strong">pruning</strong> (removing useless connections), <strong class="markup--strong markup--p-strong">quantization</strong> (using smaller weights and activation values) and <strong class="markup--strong markup--p-strong">compression</strong> (encoding weights and activation values), researchers have managed to compress large CNNs by a factor of 35 or more [2]. Even complex models now end up in the 5–10MB range, definitely within reach of smaller IoT devices.</p><p name="7186" id="7186" class="graf graf--p graf-after--p">Amazingly, some of these bleeding edge techniques are available in Apache MXNet. You can use<a href="https://devblogs.nvidia.com/parallelforall/mixed-precision-training-deep-neural-networks/" data-href="https://devblogs.nvidia.com/parallelforall/mixed-precision-training-deep-neural-networks/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> <strong class="markup--strong markup--p-strong">mixed precision training</strong></a>, which relies on 16-bit floats instead of 32-bit floats to deliver 2x compression with no loss of accuracy [3]. Thanks to a recent<a href="https://github.com/hpi-xnor/BMXNet" data-href="https://github.com/hpi-xnor/BMXNet" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> research project</a>, it’s also possible to use <strong class="markup--strong markup--p-strong">Binary Neural Networks</strong>, where weights are encoded using only +1 and -1 values. This technique yields 20x to 30x compression, with only limited loss of accuracy [4].</p><h4 name="0108" id="0108" class="graf graf--h4 graf-after--p">Optimizing math operations to speed up inference</h4><p name="7d12" id="7d12" class="graf graf--p graf-after--h4">Of course, as they involve less computation, smaller models also tend to be faster. Another technique to speed up MXNet is to use an <strong class="markup--strong markup--p-strong">acceleration software library</strong> such as<a href="https://software.intel.com/en-us/mkl" data-href="https://software.intel.com/en-us/mkl" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> Intel MKL</a> or<a href="https://github.com/Maratyszcza/NNPACK" data-href="https://github.com/Maratyszcza/NNPACK" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> NNPACK</a>.</p><p name="45d9" id="45d9" class="graf graf--p graf-after--p">These libraries provide accelerated math processing routines that are critical to the performance of Deep Learning. Both support the Intel architecture, with NNPACK also supporting ARM v7 and v8, both popular choices for embedded applications. In the same vein, MXNet can leverage performance-oriented libraries for <strong class="markup--strong markup--p-strong">image processing</strong> and <strong class="markup--strong markup--p-strong">memory allocation</strong>, namely<a href="https://www.libjpeg-turbo.org/" data-href="https://www.libjpeg-turbo.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> libjpeg-turbo</a>,<a href="https://github.com/gperftools/" data-href="https://github.com/gperftools/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> gperftools</a> and<a href="http://jemalloc.net/" data-href="http://jemalloc.net/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> jemalloc</a>.</p><p name="ef1a" id="ef1a" class="graf graf--p graf-after--p">All these need to be configured at <strong class="markup--strong markup--p-strong">build time</strong>. You’ll find<a href="https://mxnet.incubator.apache.org/get_started/install.html" data-href="https://mxnet.incubator.apache.org/get_started/install.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> detailed instructions</a> on the MXNet website.</p><h4 name="970f" id="970f" class="graf graf--h4 graf-after--p">Deploying MXNet models at the Edge with AWS services</h4><p name="eb0f" id="eb0f" class="graf graf--p graf-after--h4">Performance is great, but what about deployment? How can IoT devices living at the edge of the network use Deep Learning capabilities?</p><p name="2b72" id="2b72" class="graf graf--p graf-after--p">MXNet models can be<a href="https://aws.amazon.com/blogs/compute/seamlessly-scale-predictions-with-aws-lambda-and-mxnet/" data-href="https://aws.amazon.com/blogs/compute/seamlessly-scale-predictions-with-aws-lambda-and-mxnet/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> combined with <strong class="markup--strong markup--p-strong">AWS Lambda</strong></a>, a compute service that lets you run code without provisioning or managing servers. Lambda functions can be triggered by many different types of events, such as an IoT message matching a rule defined on the<a href="https://aws.amazon.com/iot/" data-href="https://aws.amazon.com/iot/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> AWS IoT</a> gateway. Thus, embedded applications that may be too constrained or too rigid to support on-device deployment can rely on <strong class="markup--strong markup--p-strong">cloud-based models</strong> in a simple and scalable way.</p><p name="cbb1" id="cbb1" class="graf graf--p graf-after--p">On more powerful IoT devices,<a href="https://aws.amazon.com/greengrass/" data-href="https://aws.amazon.com/greengrass/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> <strong class="markup--strong markup--p-strong">AWS Greengrass</strong></a> provides a local Lambda execution environment able to sync back and forth with the Cloud when network connectivity is available: you’ll find a detailed example in this<a href="https://aws.amazon.com/blogs/aws/aws-greengrass-run-aws-lambda-functions-on-connected-devices/" data-href="https://aws.amazon.com/blogs/aws/aws-greengrass-run-aws-lambda-functions-on-connected-devices/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> blog post</a>. This service can be used to deploy and update <strong class="markup--strong markup--p-strong">MXNet models embedded in Lambda functions</strong>, now allowing IoT devices to run inference locally without any cloud-based support.</p><h4 name="b534" id="b534" class="graf graf--h4 graf-after--p">Wrapping up</h4><p name="9efa" id="9efa" class="graf graf--p graf-after--h4">As you can see, Deep Learning and IoT are not worlds apart. Quite the contrary, in fact. We can’t wait to see the devices and applications that will be built on top of Apache MXNet and AWS services. Science is definitely catching up with fiction: exciting times!</p><p name="54d9" id="54d9" class="graf graf--p graf-after--p graf--trailing">Thank you for reading.</p></div></div></section><section name="19b9" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c329" id="c329" class="graf graf--p graf--leading">[1] Chen et al. «<a href="https://www.cs.cmu.edu/~muli/file/mxnet-learning-sys.pdf" data-href="https://www.cs.cmu.edu/~muli/file/mxnet-learning-sys.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</a>», 2015.</p><p name="3a64" id="3a64" class="graf graf--p graf-after--p">[2] Han et al. «<a href="https://arxiv.org/abs/1510.00149" data-href="https://arxiv.org/abs/1510.00149" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding </a>», 2016.</p><p name="ecd9" id="ecd9" class="graf graf--p graf-after--p">[3] Micikevicius et al. «<a href="https://arxiv.org/abs/1710.03740" data-href="https://arxiv.org/abs/1710.03740" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Mixed Precision Training</a>», 2017.</p><p name="2823" id="2823" class="graf graf--p graf-after--p graf--trailing">[4] Yang et al. «<a href="https://arxiv.org/pdf/1705.09864" data-href="https://arxiv.org/pdf/1705.09864" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet</a>», 2017.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/dde112ec060f"><time class="dt-published" datetime="2017-11-06T21:42:34.428Z">November 6, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/enabling-deep-learning-in-iot-applications-with-apache-mxnet-dde112ec060f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>