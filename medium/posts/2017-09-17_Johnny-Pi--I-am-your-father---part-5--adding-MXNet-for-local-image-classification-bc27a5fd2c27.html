<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Johnny Pi, I am your father — part 5: adding MXNet for local image classification</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Johnny Pi, I am your father — part 5: adding MXNet for local image classification</h1>
</header>
<section data-field="subtitle" class="p-summary">
In the previous post, we learned how to use Amazon Rekognition to let our robot detect faces and labels in pictures taken with its own…
</section>
<section data-field="body" class="e-content">
<section name="6b9e" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6a72" id="6a72" class="graf graf--h3 graf--leading graf--title">Johnny Pi, I am your father — part 5: adding MXNet for local image classification</h3><p name="fe27" id="fe27" class="graf graf--p graf-after--h3">In the <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" class="markup--anchor markup--p-anchor" target="_blank">previous post</a>, we learned how to use <a href="https://aws.amazon.com/rekognition/" data-href="https://aws.amazon.com/rekognition/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon Rekognition</a> to let our robot detect faces and labels in pictures taken with its own camera. This is a very cool feature, but could we do the same thing with <strong class="markup--strong markup--p-strong">local resources </strong>only, i.e. without relying on a network connection and a cloud-based API?</p><p name="4ead" id="4ead" class="graf graf--p graf-after--p">Or course we can. In this post, I’ll show you how to classify images using a local Deep Learning model. We’ll use the <a href="http://mxnet.io" data-href="http://mxnet.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> library, which I’ve covered extensively in <a href="https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4" data-href="https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4" class="markup--anchor markup--p-anchor" target="_blank">another series</a>. Let’s get to work.</p><figure name="564c" id="564c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CaMqAGbhJDCXdZvx0Ys7TA.png" data-width="1786" data-height="930" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*CaMqAGbhJDCXdZvx0Ys7TA.png"></figure><h4 name="cdc8" id="cdc8" class="graf graf--h4 graf-after--figure">Sending commands</h4><p name="1352" id="1352" class="graf graf--p graf-after--h4">We’re going to use the same MQTT topic as for Rekognition, namely <em class="markup--em markup--p-em">JohnnyPi/see</em>: we’ll just add an extra word to specify whether Rekognition or MXNet should be used.</p><p name="6405" id="6405" class="graf graf--p graf-after--p">Since we’re not invoking any additional AWS API, there is no additional IAM setup required (and there was much rejoicing!).</p><h4 name="e2ec" id="e2ec" class="graf graf--h4 graf-after--p">Installing Apache MXNet on the Raspberry Pi.</h4><p name="9dcd" id="9dcd" class="graf graf--p graf-after--h4">We have two options: build from source (as explained in <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--p-anchor" target="_blank">this post</a>) or install directly with <em class="markup--em markup--p-em">pip</em>. Let’s go for the second option, which is obviously easier and faster.</p><blockquote name="f44a" id="f44a" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">If you’re interested in building from source, <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--blockquote-anchor" target="_blank">here</a> are the instructions.</blockquote><figure name="f449" id="f449" class="graf graf--figure graf--iframe graf-after--blockquote"><script src="https://gist.github.com/juliensimon/f0aa2459836af9283228ad1a95d00903.js"></script></figure><h4 name="d097" id="d097" class="graf graf--h4 graf-after--figure">Picking a Deep Learning model for image classification</h4><p name="e9ed" id="e9ed" class="graf graf--p graf-after--h4">Now what about the model? Given the limited processing power and storage capabilities of the Pi, it’s obvious we’re never be able to train a model with it. Fortunately, we can pick pre-trained models from MXNet’s <a href="https://mxnet.incubator.apache.org/model_zoo/" data-href="https://mxnet.incubator.apache.org/model_zoo/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model zoo</a>. As we found <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--p-anchor" target="_blank">earlier</a>, we need a model that is <strong class="markup--strong markup--p-strong">small enough</strong> to fit in the Pi’s memory (only 1GB). <a href="https://arxiv.org/pdf/1512.00567.pdf" data-href="https://arxiv.org/pdf/1512.00567.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Inception v3</a> is such a model.</p><blockquote name="1115" id="1115" class="graf graf--blockquote graf-after--p">Published in December 2015, <a href="https://arxiv.org/abs/1512.00567" data-href="https://arxiv.org/abs/1512.00567" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">Inception v3</a> is an evolution of the <a href="https://arxiv.org/abs/1409.4842" data-href="https://arxiv.org/abs/1409.4842" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">GoogleNet</a> model (which won the <a href="http://image-net.org/challenges/LSVRC/2014/" data-href="http://image-net.org/challenges/LSVRC/2014/" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener" target="_blank">2014 ImageNet challenge</a>). Inception v3 is <strong class="markup--strong markup--blockquote-strong">15–25% more accurate</strong> than the best models available at the time, while being <strong class="markup--strong markup--blockquote-strong">six times cheaper computationally</strong> and using at least <strong class="markup--strong markup--blockquote-strong">five times less parameters</strong> (i.e. less RAM is required to use the model).</blockquote><p name="d7b8" id="d7b8" class="graf graf--p graf-after--blockquote">Our version of Inception v3 has been pre-trained on the <a href="http://image-net.org" data-href="http://image-net.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> dataset, which holds over 1.2 million pictures of animals and objects classified in 1,000 categories. Let’s fetch it from the model zoo.</p><figure name="255f" id="255f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/e9e6ef71b3a84e18bd2537639e75b7a8.js"></script></figure><p name="b833" id="b833" class="graf graf--p graf-after--figure">OK, now it’s time to write some code!</p><h4 name="d844" id="d844" class="graf graf--h4 graf-after--p">Code overview</h4><p name="d60b" id="d60b" class="graf graf--p graf-after--h4">With MXNet added into the mix, here’s what should now happen when we send an MQTT message to the <em class="markup--em markup--p-em">JohnnyPi/see</em> topic.</p><p name="cb40" id="cb40" class="graf graf--p graf-after--p">First, we’ll take a picture with the Pi camera. If the message starts with ‘<em class="markup--em markup--p-em">reko</em>’, we’ll use Rekognition just like we did in the previous post. No changes here.</p><p name="1b1d" id="1b1d" class="graf graf--p graf-after--p">If the message starts with ‘<em class="markup--em markup--p-em">mxnet</em>’, then we will:</p><ul class="postList"><li name="5728" id="5728" class="graf graf--li graf-after--p">load the image and convert it in a format than can be fed to the MXNet model,</li><li name="9f8e" id="9f8e" class="graf graf--li graf-after--li">use the model the predict the categories for this image,</li><li name="3331" id="3331" class="graf graf--li graf-after--li">extract the 5 most likely categories (aka ‘top 5’),</li><li name="2a47" id="2a47" class="graf graf--li graf-after--li">build a text message with the top 1 category and use Polly to play it,</li><li name="223e" id="223e" class="graf graf--li graf-after--li">if the message ends with ‘tweet’, send a tweet containing both the image and the message.</li></ul><p name="6e17" id="6e17" class="graf graf--p graf-after--li">This is what the updated callback looks like.</p><blockquote name="1380" id="1380" class="graf graf--blockquote graf-after--p">As usual, you’ll find the full code on <a href="https://github.com/juliensimon/johnnypi/tree/master/part5" data-href="https://github.com/juliensimon/johnnypi/tree/master/part5" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Github</a>.</blockquote><figure name="f05f" id="f05f" class="graf graf--figure graf--iframe graf-after--blockquote"><script src="https://gist.github.com/juliensimon/8325e7b9ef65e8de58423296631dd9d9.js"></script></figure><h4 name="fa8f" id="fa8f" class="graf graf--h4 graf-after--figure">Loading and classifying the image</h4><p name="b360" id="b360" class="graf graf--p graf-after--h4">I’ve already covered these exact steps in full detail in a <a href="https://medium.com/towards-data-science/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" data-href="https://medium.com/towards-data-science/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" class="markup--anchor markup--p-anchor" target="_blank">previous post</a>, so I’ll stick to a quick summary here.</p><p name="d810" id="d810" class="graf graf--p graf-after--p">In <em class="markup--em markup--p-em">inception.load_image()</em>, we use <a href="http://opencv.org" data-href="http://opencv.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenCV</a> and <a href="http://www.numpy.org" data-href="http://www.numpy.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Numpy</a> to load the image and transform it a 4-dimension array corresponding to the input layer on the Inception v3 model: (1L, 3L, 224L, 224L) : one sample with three 224x244 images (red, green and blue).</p><p name="7924" id="7924" class="graf graf--p graf-after--p">In <em class="markup--em markup--p-em">inception.predict()</em>, we forward the sample through the model and receive a vector of 1,000 probabilities, one for each class.</p><p name="7f9e" id="7f9e" class="graf graf--p graf-after--p">In <em class="markup--em markup--p-em">inception.get_top_categories(), </em>we sort these probabilities and find the top 5 classes.</p><h4 name="2e8a" id="2e8a" class="graf graf--h4 graf-after--p">Speaking and tweeting</h4><p name="4d0a" id="4d0a" class="graf graf--p graf-after--h4">This part is almost identical to what we did with Rekognition, so no need to repeat ourselves.</p><h4 name="045d" id="045d" class="graf graf--h4 graf-after--p">Testing</h4><p name="8c5a" id="8c5a" class="graf graf--p graf-after--h4">Once again, we’ll use <a href="http://www.mqttfx.org/" data-href="http://www.mqttfx.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MQTT.fx</a> to send commands to the <em class="markup--em markup--p-em">JohnnyPi/see</em> topic:</p><ul class="postList"><li name="4e0f" id="4e0f" class="graf graf--li graf-after--p">two for Rekognition: ‘<em class="markup--em markup--li-em">reko</em>’ and ‘<em class="markup--em markup--li-em">reko tweet</em>’). We’ve already tested Rekognition in the previous post, so let’s skip these here.</li><li name="39ce" id="39ce" class="graf graf--li graf-after--li">two for MXNet: ‘<em class="markup--em markup--li-em">mxnet</em>’ and ‘<em class="markup--em markup--li-em">mxnet tweet</em>’.</li></ul><p name="d6fb" id="d6fb" class="graf graf--p graf-after--li">Let’s try a first object.</p><figure name="2030" id="2030" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Y5FaatCdV5FF-TMCaB7NHg.jpeg" data-width="640" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*Y5FaatCdV5FF-TMCaB7NHg.jpeg"></figure><p name="25e6" id="25e6" class="graf graf--p graf-after--figure">The output is:</p><pre name="777e" id="777e" class="graf graf--pre graf-after--p">Topic=JohnnyPi/see<br>Message=mxnet<br>forward pass in 3.14714694023<br>probability=0.467848, class=n03085013 computer keyboard, keypad<br>probability=0.156991, class=n04152593 screen, CRT screen<br>probability=0.090207, class=n03782006 monitor<br>probability=0.079132, class=n03793489 mouse, computer mouse<br>probability=0.048446, class=n03642806 laptop, laptop computer<br>[(0.46784759, &#39;n03085013 computer keyboard, keypad&#39;), (0.15699127, &#39;n04152593 screen, CRT screen&#39;), (0.090206854, &#39;n03782006 monitor&#39;), (0.079132304, &#39;n03793489 mouse, computer mouse&#39;), (0.0484455, &#39;n03642806 laptop, laptop computer&#39;)]<br>I&#39;m 46% sure that this is a computer keyboard, keypad.</pre><p name="ba72" id="ba72" class="graf graf--p graf-after--pre">Let’s try another one.</p><figure name="a029" id="a029" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*imWy4CRnW_NRDLftd7KTuA.jpeg" data-width="640" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*imWy4CRnW_NRDLftd7KTuA.jpeg"></figure><p name="3016" id="3016" class="graf graf--p graf-after--figure">The output is:</p><pre name="5d7c" id="5d7c" class="graf graf--pre graf-after--p">Topic=JohnnyPi/see<br>Message=mxnet<br>forward pass in 3.1429579258<br>probability=0.511771, class=n03602883 joystick<br>probability=0.185790, class=n03584254 iPod<br>probability=0.099805, class=n03691459 loudspeaker, speaker, speaker unit, loudspeaker system, speaker system<br>probability=0.022552, class=n04074963 remote control, remote<br>probability=0.017473, class=n03891332 parking meter<br>[(0.5117712, &#39;n03602883 joystick&#39;), (0.18578961, &#39;n03584254 iPod&#39;), (0.09980496, &#39;n03691459 loudspeaker, speaker, speaker unit, loudspeaker system, speaker system&#39;), (0.022552039, &#39;n04074963 remote control, remote&#39;), (0.017472528, &#39;n03891332 parking meter&#39;)]<br>I&#39;m 51% sure that this is a joystick.</pre><p name="3337" id="3337" class="graf graf--p graf-after--pre">And let’s tweet a last one:</p><figure name="79c5" id="79c5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FHDCuq6ppPpzmYO3mD7DqQ.png" data-width="1158" data-height="948" src="https://cdn-images-1.medium.com/max/800/1*FHDCuq6ppPpzmYO3mD7DqQ.png"></figure><p name="68cf" id="68cf" class="graf graf--p graf-after--figure">The output is indeed:</p><pre name="c0b0" id="c0b0" class="graf graf--pre graf-after--p">Topic=JohnnyPi/see<br>Message=mxnet tweet<br>forward pass in 3.00172805786<br>probability=0.460716, class=n04557648 water bottle<br>probability=0.098160, class=n01990800 isopod<br>probability=0.079460, class=n04116512 rubber eraser, rubber<br>probability=0.055721, class=n04286575 spotlight, spot<br>probability=0.032628, class=n02823750 beer glass<br>[(0.46071553, &#39;n04557648 water bottle&#39;), (0.098159559, &#39;n01990800 isopod&#39;), (0.079459898, &#39;n04116512 rubber eraser, rubber, pencil eraser&#39;), (0.055721201, &#39;n04286575 spotlight, spot&#39;), (0.03262835, &#39;n02823750 beer glass&#39;)]<br>I&#39;m 46% sure that this is a water bottle.</pre><h4 name="7731" id="7731" class="graf graf--h4 graf-after--pre">What’s next</h4><p name="9578" id="9578" class="graf graf--p graf-after--h4">Our robot is now able to move, measure distance to objects, detect faces and classify objects. In the next part, I’ll show you how to use an <a href="https://aws.amazon.com/iotbutton/" data-href="https://aws.amazon.com/iotbutton/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">IoT button</a> to send commands to the robot. Once this this complete, we’ll move on to building an Amazon Echo skill for voice control. Stay tuned :)</p><p name="1f31" id="1f31" class="graf graf--p graf-after--p graf--trailing">As always, thanks for reading.</p></div></div></section><section name="b118" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="5457" id="5457" class="graf graf--p graf--leading">Part 0: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36" class="markup--anchor markup--p-anchor" target="_blank">a sneak preview</a></p><p name="4525" id="4525" class="graf graf--p graf-after--p">Part 1: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce" data-href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener" target="_blank">moving around</a></p><p name="58a9" id="58a9" class="graf graf--p graf-after--p">Part 2: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86" data-href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener" target="_blank">the joystick</a></p><p name="a175" id="a175" class="graf graf--p graf-after--p">Part 3: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76" class="markup--anchor markup--p-anchor" target="_blank">cloud-based speech</a></p><p name="7aa6" id="7aa6" class="graf graf--p graf-after--p graf--trailing">Part 4: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" data-href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" class="markup--anchor markup--p-anchor" target="_blank">cloud-based vision</a></p></div></div></section><section name="f334" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="846f" id="846f" class="graf graf--p graf--leading graf--trailing"><em class="markup--em markup--p-em">This article was written while listening to Pink Floyd’s “Dark Side of the Moon”, which is pretty much the only thing that my jet-lagged brain can take right now :)</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/bc27a5fd2c27"><time class="dt-published" datetime="2017-09-17T09:39:06.445Z">September 17, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-5-adding-mxnet-for-local-image-classification-bc27a5fd2c27" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
