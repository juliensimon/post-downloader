<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Generative Adversarial Networks on Apache MXNet, part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Generative Adversarial Networks on Apache MXNet, part 1</h1>
</header>
<section data-field="subtitle" class="p-summary">
In several previous posts, I’ve shown you how to classify images using a variety of Convolution Neural Networks. Using a labeled training…
</section>
<section data-field="body" class="e-content">
<section name="28c4" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="cf59" id="cf59" class="graf graf--h3 graf--leading graf--title">Generative Adversarial Networks on Apache MXNet, part 1</h3><p name="65fe" id="65fe" class="graf graf--p graf-after--h3">In several previous posts, I’ve shown you how to classify images using a variety of Convolution Neural Networks. Using a <strong class="markup--strong markup--p-strong">labeled training set</strong> and applying a <strong class="markup--strong markup--p-strong">supervised learning</strong> process, AI delivers fantastic results on this problem and on similar ones, such as object detection or object segmentation.</p><p name="f6c5" id="f6c5" class="graf graf--p graf-after--p">Impressive as it is, this form of intelligence only deals with <strong class="markup--strong markup--p-strong">understanding</strong> representations of our world as it is (text, images, etc). What about <strong class="markup--strong markup--p-strong">inventing</strong> new representations? Could AI be able to generate <strong class="markup--strong markup--p-strong">brand new images</strong>, convincing enough to fool the human eye? Well, yes.</p><p name="8a9d" id="8a9d" class="graf graf--p graf-after--p">In this post, we’ll start to explore how!</p><h4 name="3b8c" id="3b8c" class="graf graf--h4 graf-after--p">Generative Adversarial Networks</h4><p name="bfc1" id="bfc1" class="graf graf--p graf-after--h4">A breakthrough happened in 2014, with the publication of “<a href="https://arxiv.org/abs/1406.2661" data-href="https://arxiv.org/abs/1406.2661" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Generative Adversarial Networks</em></a>”, by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio.</p><blockquote name="eca1" id="eca1" class="graf graf--blockquote graf-after--p">In my crystal ball, I see Ian Goodfellow winning a Turing Award for this. It might take 30 years, but mark my words.</blockquote><p name="99e1" id="99e1" class="graf graf--p graf-after--blockquote">Relying on an <strong class="markup--strong markup--p-strong">unlabeled data set</strong> and an <strong class="markup--strong markup--p-strong">unsupervised learning process</strong>, GANs are able to <strong class="markup--strong markup--p-strong">generate new images</strong> (people, animals, landscapes, etc.) or even <strong class="markup--strong markup--p-strong">alter parts of an existing image</strong> (like adding a smile to a person’s face).</p><h4 name="431f" id="431f" class="graf graf--h4 graf-after--p">An intuitive explanation</h4><p name="b3d6" id="b3d6" class="graf graf--p graf-after--h4">The original Goodfellow article uses the <strong class="markup--strong markup--p-strong">art forge</strong>r vs <strong class="markup--strong markup--p-strong">art expert</strong> analogy, which has been rehashed to death on countless blogs. I’ll let you read the original version and I’ll try to use a different analogy: <strong class="markup--strong markup--p-strong">cooking</strong>.</p><p name="1dda" id="1dda" class="graf graf--p graf-after--p">You’re the <strong class="markup--strong markup--p-strong">apprentice</strong> and I’m the <strong class="markup--strong markup--p-strong">chef</strong> (obviously!). Your goal would to cook a really nice <a href="http://www.marmiton.org/recettes/recette_boeuf-bourguignon_18889.aspx" data-href="http://www.marmiton.org/recettes/recette_boeuf-bourguignon_18889.aspx" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Boeuf Bourguignon</a>, but I wouldn’t give you <strong class="markup--strong markup--p-strong">any</strong> instructions. No list of ingredients, no recipe, nothing. My only request would be: “<strong class="markup--strong markup--p-strong">cook something with 20 ingredients</strong>”.</p><p name="4b62" id="4b62" class="graf graf--p graf-after--p">You’d go the pantry, pick 20 <strong class="markup--strong markup--p-strong">random</strong> ingredients, mix them together in a pot and show me the result. I’d look at it and of course the result would be nothing like what I expected</p><p name="a52f" id="a52f" class="graf graf--p graf-after--p">For each of the ingredients you selected, I’d give you a <strong class="markup--strong markup--p-strong">hint</strong> which would help you get a little bit closer to the actual recipe. For example, if you picked chicken, I could tell you: “Well, there is no chicken in this recipe but there is meat”. And if you used grape juice, I may say: “Hmm, the color is right but this is the wrong liquid” (red wine is required).</p><p name="417c" id="417c" class="graf graf--p graf-after--p">Resolved to improve, you’d go back to the pantry and try to make <strong class="markup--strong markup--p-strong">slightly better choices</strong>. The result would still be far off, but a <strong class="markup--strong markup--p-strong">little bit closer</strong> anyway. I’d give you more hints, you’d cook again and so on. After a number of <strong class="markup--strong markup--p-strong">iterations</strong> (and a massive waste of food), chances are you’d get very close to the <strong class="markup--strong markup--p-strong">actual recipe</strong> — assuming that I wouldn’t have lost my temper by then :D</p><h4 name="0b95" id="0b95" class="graf graf--h4 graf-after--p">A (slightly) more scientific explanation</h4><p name="cecd" id="cecd" class="graf graf--p graf-after--h4">Let’s replace the apprentice by the <strong class="markup--strong markup--p-strong">Generator</strong> and the chef by the <strong class="markup--strong markup--p-strong">Discriminator</strong>. Here is how GANs work.</p><ol class="postList"><li name="94ed" id="94ed" class="graf graf--li graf-after--p">The <strong class="markup--strong markup--li-strong">Generator</strong> model has <strong class="markup--strong markup--li-strong">no access to the data set</strong>. Using random data, it creates an image that is forwarded through the Detector model.</li><li name="83d9" id="83d9" class="graf graf--li graf-after--li">The <strong class="markup--strong markup--li-strong">Discriminator</strong> model learns how to recognise <strong class="markup--strong markup--li-strong">valid</strong> data samples (the ones included in the data) from <strong class="markup--strong markup--li-strong">invalid</strong> data samples (the ones computed by the Generator). The training process uses traditional techniques like gradient descent and back propagation.</li><li name="1921" id="1921" class="graf graf--li graf-after--li">The <strong class="markup--strong markup--li-strong">Generator</strong> model also learns, but in a different way. First, it treats its samples as <strong class="markup--strong markup--li-strong">valid</strong> (it’s trying to fool the Discriminator after all). Second, weights are updated using the <strong class="markup--strong markup--li-strong">gradients computed by the Discriminator</strong>.</li><li name="04f1" id="04f1" class="graf graf--li graf-after--li">Repeat!</li></ol><blockquote name="edf0" id="edf0" class="graf graf--blockquote graf-after--li">This is the key to understanding GANs: by treating its samples as valid and by applying the Discriminator weight updates, <strong class="markup--strong markup--blockquote-strong">the Generator progressively learns how to generate data samples that are closer and closer to the ones that the Discriminator considers as valid</strong>, i.e. the ones that are part of the data set.</blockquote><p name="3b6f" id="3b6f" class="graf graf--p graf-after--blockquote">Brilliant, brilliant idea (Turing award, I’m telling you).</p><h4 name="72dd" id="72dd" class="graf graf--h4 graf-after--p">Deep Convolutional GANs</h4><p name="c8b3" id="c8b3" class="graf graf--p graf-after--h4">GANs may be implemented using a number of different model architectures. Here, we will study a GAN based on <strong class="markup--strong markup--p-strong">Convolutional Neural Networks</strong>, as published in “<a href="https://arxiv.org/abs/1511.06434" data-href="https://arxiv.org/abs/1511.06434" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em></a>”, by Alec Radford, Luke Metz and Soumith Chintala (2016).</p><p name="ab84" id="ab84" class="graf graf--p graf-after--p">Let’s take a look at the <strong class="markup--strong markup--p-strong">Generator</strong> model. I’ve slightly updated the illustration included in the original article to reflect the exact code that we will use later on.</p><figure name="369c" id="369c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_hWtBcqL7YwsIrgAeFgtRw.png" data-width="1859" data-height="676" src="https://cdn-images-1.medium.com/max/800/1*_hWtBcqL7YwsIrgAeFgtRw.png"><figcaption class="imageCaption">The Generator network (data flows from left to right)</figcaption></figure><p name="5923" id="5923" class="graf graf--p graf-after--figure">Don’t panic, it’s not as bad as you think. We start from a random vector of 100 values. Using 5 transposed convolution operations (more on this in a minute), this vector is turned into an RGB 64x64 image (hence the 3 channels).</p><p name="b615" id="b615" class="graf graf--p graf-after--p">Now let’s look at the <strong class="markup--strong markup--p-strong">Discriminator</strong>. Wait, it’s almost identical (don’t forget to start from the left this time). Using 5 convolution operations, we turn an RGB 64x64 image into a probability: true for valid samples, false for invalid samples.</p><figure name="6f5f" id="6f5f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*u6g6DDPCXp8oWDLNcinhbw.png" data-width="1858" data-height="680" src="https://cdn-images-1.medium.com/max/800/1*u6g6DDPCXp8oWDLNcinhbw.png"><figcaption class="imageCaption">The Discriminator network (data flows from right to left)</figcaption></figure><p name="8213" id="8213" class="graf graf--p graf-after--figure">Still with me? Good. Now what about this convolution / transposed convolution thing?</p><h4 name="dec9" id="dec9" class="graf graf--h4 graf-after--p">A look at convolution</h4><p name="0093" id="0093" class="graf graf--p graf-after--h4">There are plenty of great tutorials out there. The best I’m aware of is part of the Theano documentation. Extremely detailed with <strong class="markup--strong markup--p-strong">beautiful</strong> animations. Read it and words like “kernel”, “padding” and “stride” will become crystal clear.</p><div name="b197" id="b197" class="graf graf--mixtapeEmbed graf-after--p"><a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" data-href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Convolution arithmetic tutorial - Theano 0.9.0 documentation</strong><br><em class="markup--em markup--mixtapeEmbed-em">Learning to use convolutional neural networks (CNNs) for the first time is generally an intimidating experience. A…</em>deeplearning.net</a><a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="389a4b19e8d31f54a7ce1bce73679fd1" data-thumbnail-img-id="0*K2OlEWy2aJRkBnnF." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*K2OlEWy2aJRkBnnF.);"></a></div><p name="8b28" id="8b28" class="graf graf--p graf-after--mixtapeEmbed">In a nutshell, convolution is typically used to <strong class="markup--strong markup--p-strong">reduce dimensions</strong>. This is why this operation is at the core of Convolutional (duh) Neural Networks: they start from a full image (say 224x224) and gradually <strong class="markup--strong markup--p-strong">shrink</strong> it through a series of convolutions which will only preserve the features that are meaningful for classification.</p><p name="430e" id="430e" class="graf graf--p graf-after--p">The formula to compute the size of the <strong class="markup--strong markup--p-strong">output image</strong> is actually quite simple.</p><figure name="4bef" id="4bef" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N8qzRzeFP6M1GS5i8g-TPA.png" data-width="171" data-height="45" src="https://cdn-images-1.medium.com/max/800/1*N8qzRzeFP6M1GS5i8g-TPA.png"><figcaption class="imageCaption">i: input, o: output, k: kernel, p: padding, s: stride</figcaption></figure><p name="c53b" id="c53b" class="graf graf--p graf-after--figure">We can apply it to the Discriminator network above and yes, it works. Woohoo.</p><figure name="f509" id="f509" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0eCDVqgzkFsG1JG-kND4Ig.png" data-width="930" data-height="220" src="https://cdn-images-1.medium.com/max/800/1*0eCDVqgzkFsG1JG-kND4Ig.png"></figure><h4 name="2f71" id="2f71" class="graf graf--h4 graf-after--figure">A look at transposed convolution</h4><p name="6a49" id="6a49" class="graf graf--p graf-after--h4">Transposed convolution is the reverse process, i.e. it <strong class="markup--strong markup--p-strong">increases dimensions</strong>. Don’t call it “Deconvolution”, it seems to aggravate some people ;)</p><p name="7cf1" id="7cf1" class="graf graf--p graf-after--p">The formula to compute the size of the <strong class="markup--strong markup--p-strong">output image</strong> is as follows.</p><figure name="01f4" id="01f4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8RIIPyAHFFkJWt753Z8XFg.png" data-width="177" data-height="19" src="https://cdn-images-1.medium.com/max/800/1*8RIIPyAHFFkJWt753Z8XFg.png"><figcaption class="imageCaption">i’: input, o’: output, k: kernel, p: padding, s: stride</figcaption></figure><p name="b11e" id="b11e" class="graf graf--p graf-after--figure">Applying it to the Generator network gives us the correct results too. Hopefully, this is starting to make sense and you now understand how it’s possible to generate a picture from a vector of random values :)</p><figure name="38f4" id="38f4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*x-33A0YroI42DvsRFG7eyA.png" data-width="920" data-height="222" src="https://cdn-images-1.medium.com/max/800/1*x-33A0YroI42DvsRFG7eyA.png"></figure><h4 name="05ca" id="05ca" class="graf graf--h4 graf-after--figure">Coding the Discriminator network</h4><p name="3b22" id="3b22" class="graf graf--p graf-after--h4">Apache MXNet has a couple of nice examples implementing this network architecture in R and Python. I’ll use Python for the rest of the post, but I’m sure R users will follow along.</p><div name="2588" id="2588" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" data-href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/apache/incubator-mxnet/tree/master/example/gan"><strong class="markup--strong markup--mixtapeEmbed-strong">apache/incubator-mxnet</strong><br><em class="markup--em markup--mixtapeEmbed-em">incubator-mxnet — Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware…</em>github.com</a><a href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="a7a19f3155ed8677ae5dd1822f931430" data-thumbnail-img-id="0*13DYtb3j5zwVmwk_." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*13DYtb3j5zwVmwk_.);"></a></div><p name="c2d2" id="c2d2" class="graf graf--p graf-after--mixtapeEmbed">Here’s the code for the <strong class="markup--strong markup--p-strong">Discriminator</strong> network, based on the illustration above. You’ll find extra details in the research article, e.g. why they use the LeakyRelu activation function and so on.</p><figure name="3cbf" id="3cbf" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/cdba4eb4359c6820862ce7b91565d30b.js"></script></figure><h4 name="53fa" id="53fa" class="graf graf--h4 graf-after--figure">Coding the Generator network</h4><p name="7ffc" id="7ffc" class="graf graf--p graf-after--h4">Here’s the code for the <strong class="markup--strong markup--p-strong">Generator</strong> network, based on the illustration above.</p><figure name="226b" id="226b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/758e69a2805728d703a21090ff53dcba.js"></script></figure><h4 name="9f6b" id="9f6b" class="graf graf--h4 graf-after--figure">Preparing MNIST</h4><p name="2140" id="2140" class="graf graf--p graf-after--h4">OK, now let’s take care of the data set. As you probably know, the <strong class="markup--strong markup--p-strong">MNIST</strong> data set contains 28x28 black and white images. We need to:</p><ul class="postList"><li name="3726" id="3726" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">reshape</strong> them to 64x64 images,</li><li name="c8f6" id="c8f6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">normalize</strong> pixel values between 0 and 1,</li><li name="dcbe" id="dcbe" class="graf graf--li graf-after--li">add <strong class="markup--strong markup--li-strong">two extra channels</strong> (identical to the original image),</li><li name="d3de" id="d3de" class="graf graf--li graf-after--li">set 10,000 samples aside to <strong class="markup--strong markup--li-strong">validate</strong> the Discriminator.</li></ul><p name="7400" id="7400" class="graf graf--p graf-after--li">Nothing MXNet-specific here, just good old Python data manipulation.</p><p name="e18e" id="e18e" class="graf graf--p graf-after--p">During Discriminator training, this data set will be served by a standard <em class="markup--em markup--p-em">NDArray</em> <strong class="markup--strong markup--p-strong">iterator</strong>.</p><figure name="5596" id="5596" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/5759a813f2a7cb7322d0627feb985b7c.js"></script></figure><h4 name="44a1" id="44a1" class="graf graf--h4 graf-after--figure">Preparing random data</h4><p name="beba" id="beba" class="graf graf--p graf-after--h4">We also need to provide random data to the Generator. We’ll do this with a <strong class="markup--strong markup--p-strong">custom iterator</strong>.</p><figure name="0e4d" id="0e4d" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/c3fec702b301cf17caaaeafa5d4783b0.js"></script></figure><p name="b737" id="b737" class="graf graf--p graf-after--figure">When <em class="markup--em markup--p-em">getdata</em>() is called, this iterator will return an <em class="markup--em markup--p-em">NDArray</em> shaped <strong class="markup--strong markup--p-strong">(batch size, random vector size, 1, 1)</strong>. We’ll use a 100-element random vector, so through multiple transposed convolutions, the Generator will indeed build a picture from a (100, 1, 1) sample.</p><h4 name="08cd" id="08cd" class="graf graf--h4 graf-after--p">The training loop</h4><p name="7677" id="7677" class="graf graf--p graf-after--h4">Time to look at the training code. This time, we cannot use the <em class="markup--em markup--p-em">Module.fit()</em> API. We have to write a <strong class="markup--strong markup--p-strong">custom training loop</strong> taking into account the fact that we’re going to use the Discriminator gradients to update the Generator.</p><p name="5205" id="5205" class="graf graf--p graf-after--p">Here are the steps:</p><ol class="postList"><li name="792c" id="792c" class="graf graf--li graf-after--p">Generate a batch of <strong class="markup--strong markup--li-strong">random samples</strong> (line 4).</li><li name="e3ed" id="e3ed" class="graf graf--li graf-after--li">Forward the batch through the <strong class="markup--strong markup--li-strong">Generator</strong> and grab the resulting <strong class="markup--strong markup--li-strong">images</strong> (lines 6–7).</li><li name="062a" id="062a" class="graf graf--li graf-after--li">Label these images as <strong class="markup--strong markup--li-strong">fake</strong>, forward them through the <strong class="markup--strong markup--li-strong">Discriminator</strong> and run back propagation (lines 10–12) : this lets the Discriminator learn to detect <strong class="markup--strong markup--li-strong">fake</strong> images.</li><li name="77f5" id="77f5" class="graf graf--li graf-after--li">Save the Discriminator <strong class="markup--strong markup--li-strong">gradients</strong> but do not update the Discriminator weights at the moment (line 13).</li><li name="e070" id="e070" class="graf graf--li graf-after--li">Label the current <strong class="markup--strong markup--li-strong">MNIST</strong> batch of images as <strong class="markup--strong markup--li-strong">real</strong> images, forward them through the <strong class="markup--strong markup--li-strong">Discriminator</strong> and run back propagation (lines 16–19) : this lets the Discriminator learn to detect <strong class="markup--strong markup--li-strong">real</strong> images.</li><li name="2ad4" id="2ad4" class="graf graf--li graf-after--li">Add the “fake images” <strong class="markup--strong markup--li-strong">gradients</strong> to the “real images” <strong class="markup--strong markup--li-strong">gradients</strong> and update the <strong class="markup--strong markup--li-strong">Discriminator</strong> <strong class="markup--strong markup--li-strong">weights</strong> (lines 20–23).</li><li name="30a9" id="30a9" class="graf graf--li graf-after--li">Label the Generator images as <strong class="markup--strong markup--li-strong">real </strong>this time, forward them through the Discriminator again and run back propagation (lines 26–28).</li><li name="b66e" id="b66e" class="graf graf--li graf-after--li">Get the Discriminator <strong class="markup--strong markup--li-strong">gradients</strong>: they would normally help the <strong class="markup--strong markup--li-strong">Discriminator</strong> learn how real images look like. However, we’re applying them to the <strong class="markup--strong markup--li-strong">Generator</strong> network instead, effectively helping it to <strong class="markup--strong markup--li-strong">forge</strong> better fake images (lines 29–31).</li></ol><figure name="a1ca" id="a1ca" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/b8fc358471fb9bead98c9e6b327d4e30.js"></script></figure><p name="9958" id="9958" class="graf graf--p graf-after--figure">Quite a mouthful! Congratulations if you got this far: you understood the core concepts of GANs.</p><h4 name="f2c3" id="f2c3" class="graf graf--h4 graf-after--p">Let’s run this thing</h4><p name="f1b8" id="f1b8" class="graf graf--p graf-after--h4">The <a href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" data-href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MXNet sample</a> includes code to visualize the images coming out of the Generator . The simplest way to view them is to copy the code in a Jupyter notebook and run it :)</p><p name="cc21" id="cc21" class="graf graf--p graf-after--p">After a few minutes (especially if you use a Volta-powered <a href="https://aws.amazon.com/about-aws/whats-new/2017/10/introducing-amazon-ec2-p3-instances/" data-href="https://aws.amazon.com/about-aws/whats-new/2017/10/introducing-amazon-ec2-p3-instances/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">p3 instance</a>), you should see something similar to this.</p><figure name="492a" id="492a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*F7FDObAQetw0ScCPPMjTFg.png" data-width="824" data-height="460" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*F7FDObAQetw0ScCPPMjTFg.png"></figure><p name="8f30" id="8f30" class="graf graf--p graf-after--figure">As you can see, random noise gradually turns into well-formed digits. It’s just math, but it’s still amazing…</p><blockquote name="5f53" id="5f53" class="graf graf--pullquote graf-after--p">In all chaos there is a cosmos, in all disorder a secret order— Carl Jung</blockquote><h4 name="6ba2" id="6ba2" class="graf graf--h4 graf-after--pullquote">So when do we stop training?</h4><p name="8f81" id="8f81" class="graf graf--p graf-after--h4">Common training metrics like accuracy mean nothing here. We have no way of knowing whether Generator images are getting better… except by looking at them.</p><p name="4dba" id="4dba" class="graf graf--p graf-after--p">An alternative would be to generate only fives (or any other digit), to run them through a <a href="https://medium.com@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62" data-href="https://medium.com@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">proper MNIST classifier</a> and to measure accuracy.</p><p name="7103" id="7103" class="graf graf--p graf-after--p">There is also ongoing research to use <strong class="markup--strong markup--p-strong">new metrics</strong> for GANs, such as the <a href="https://arxiv.org/abs/1701.07875" data-href="https://arxiv.org/abs/1701.07875" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Wasserstein distance</a>. Let’s keep this topic for another article :)</p><p name="fcda" id="fcda" class="graf graf--p graf-after--p graf--trailing">Thanks for reading. This is definitely a deeper dive than usual, but I hope you enjoyed it.</p></div></div></section><section name="54ac" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="635e" id="635e" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">Only one song is worthy here. Generator vs Discriminator, may the best model win!</em></p><figure name="0272" id="0272" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/ty66q5RAL3E?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/b6d39e6b5df1"><time class="dt-published" datetime="2017-11-13T18:23:59.908Z">November 13, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/generative-adversarial-networks-on-apache-mxnet-part-1-b6d39e6b5df1" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>