<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Training with Keras-MXNet on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Training with Keras-MXNet on Amazon SageMaker</h1>
</header>
<section data-field="subtitle" class="p-summary">
As previously discussed, Apache MXNet is now available as a backend for Keras 2, aka Keras-MXNet.
</section>
<section data-field="body" class="e-content">
<section name="2585" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="76a1" id="76a1" class="graf graf--h3 graf--leading graf--title">Training with Keras-MXNet on Amazon SageMaker</h3><p name="a226" id="a226" class="graf graf--p graf-after--h3">As <a href="https://medium.com/@julsimon/apache-mxnet-as-a-backend-for-keras-2-9993f97843e7" data-href="https://medium.com/@julsimon/apache-mxnet-as-a-backend-for-keras-2-9993f97843e7" class="markup--anchor markup--p-anchor" target="_blank">previously discussed</a>, <a href="https://mxnet.apache.org/" data-href="https://mxnet.apache.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Apache MXNet</strong></a> is now available as a backend for <a href="https://keras.io" data-href="https://keras.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Keras 2</strong></a>, aka <a href="https://github.com/awslabs/keras-apache-mxnet" data-href="https://github.com/awslabs/keras-apache-mxnet" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Keras-MXNet</strong></a>.</p><p name="6add" id="6add" class="graf graf--p graf-after--p">In this post, you will learn how to train Keras-MXNet jobs on <a href="https://aws.amazon.com/sagemaker" data-href="https://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon SageMaker</strong></a>. I’ll show you how to:</p><ul class="postList"><li name="292e" id="292e" class="graf graf--li graf-after--p">build <strong class="markup--strong markup--li-strong">custom Docker containers</strong> for CPU and GPU training,</li><li name="c8bb" id="c8bb" class="graf graf--li graf-after--li">configure <strong class="markup--strong markup--li-strong">multi-GPU training</strong>,</li><li name="1e62" id="1e62" class="graf graf--li graf-after--li">pass <strong class="markup--strong markup--li-strong">parameters</strong> to a Keras script,</li><li name="b576" id="b576" class="graf graf--li graf-after--li">save the <strong class="markup--strong markup--li-strong">trained models</strong> in Keras and MXNet formats.</li></ul><p name="b79f" id="b79f" class="graf graf--p graf-after--li">As usual, you’ll find my code on <a href="https://github.com/juliensimon/dlnotebooks/tree/master/keras/01-custom-container" data-href="https://github.com/juliensimon/dlnotebooks/tree/master/keras/01-custom-container" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a> :)</p><figure name="c342" id="c342" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*blHCT4f2pwuVyhx2.jpg" data-width="1330" data-height="570" src="https://cdn-images-1.medium.com/max/800/0*blHCT4f2pwuVyhx2.jpg"><figcaption class="imageCaption">That’s how it feels when your custom container runs without error :)</figcaption></figure><h3 name="e152" id="e152" class="graf graf--h3 graf-after--figure">Configuring Keras for MXNet</h3><p name="feca" id="feca" class="graf graf--p graf-after--h3">All it takes is really setting the ‘<em class="markup--em markup--p-em">backend</em>’ to ‘<em class="markup--em markup--p-em">mxnet’</em> in <em class="markup--em markup--p-em">.keras/keras.json</em>, but setting ‘<em class="markup--em markup--p-em">image_data_format</em>’ to ‘<em class="markup--em markup--p-em">channels_first</em>’ will make MXNet training faster.</p><blockquote name="9940" id="9940" class="graf graf--blockquote graf-after--p">When working with image data, the input shape can either be ‘channels_first’, i.e. (number of channels, height, width), or ‘channels_last’, i.e. (height, width, number of channels). For MNIST, this would either be (1, 28, 28) or (28, 28, 1) : one channel (black and white pictures), 28 pixels by 28 pixels. For ImageNet, it would be (3, 224, 224) or (224, 224, 3): three channels (red, green and blue), 224 pixels by 224 pixels.</blockquote><p name="3ba9" id="3ba9" class="graf graf--p graf-after--blockquote">Here’s the <strong class="markup--strong markup--p-strong">configuration file</strong> we’ll use for our container.</p><figure name="beaf" id="beaf" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/06f9a9af6728747fbce0e6261d2ee80b.js"></script></figure><h3 name="221f" id="221f" class="graf graf--h3 graf-after--figure">Building custom containers</h3><p name="1a66" id="1a66" class="graf graf--p graf-after--h3">SageMaker provides a collection of <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">built-in algorithms</a> as well as environments for TensorFlow and MXNet… but not for Keras. Fortunately, developers have the option to <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">build custom containers for training and prediction</a>.</p><p name="6b17" id="6b17" class="graf graf--p graf-after--p">Obviously, a number of <strong class="markup--strong markup--p-strong">conventions</strong> need to be defined for SageMaker to successfully invoke a custom container:</p><ul class="postList"><li name="cae4" id="cae4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Name of the training and prediction scripts</strong>: by default, they should respectively be set to ‘<em class="markup--em markup--li-em">train</em>’ and ‘<em class="markup--em markup--li-em">serve</em>’, be executable and have no extension. SageMaker will start training by running ‘<em class="markup--em markup--li-em">docker run your_container train</em>’.</li><li name="2487" id="2487" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Location of hyper parameters </strong>in the container: <em class="markup--em markup--li-em">/opt/ml/input/config/hyperparameters.json</em>.</li><li name="6a68" id="6a68" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Location of input data parameters </strong>in the container: <em class="markup--em markup--li-em">/opt/ml/input/data</em>.</li></ul><p name="46f5" id="46f5" class="graf graf--p graf-after--li">This will require some changes in our Keras script, the well-known example of <a href="https://github.com/awslabs/keras-apache-mxnet/blob/master/examples/mnist_cnn.py" data-href="https://github.com/awslabs/keras-apache-mxnet/blob/master/examples/mnist_cnn.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">learning MNIST with a simple CNN</a>. As you will see in a moment, they are quite minor and you won’t have any trouble adding them to your own code.</p><h4 name="43b6" id="43b6" class="graf graf--h4 graf-after--p">Building a CPU-based Docker container</h4><p name="aa4d" id="aa4d" class="graf graf--p graf-after--h4">Here’s the <strong class="markup--strong markup--p-strong">Docker file</strong>.</p><figure name="6c3b" id="6c3b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/74688c43c5a8b8840c34299c398f801d.js"></script></figure><p name="a702" id="a702" class="graf graf--p graf-after--figure">We start from an <strong class="markup--strong markup--p-strong">Ubuntu 16.04 image</strong> and install :</p><ul class="postList"><li name="718c" id="718c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Python 3</strong> as well as <strong class="markup--strong markup--li-strong">native dependencies</strong> for MXNet.</li><li name="d2bf" id="d2bf" class="graf graf--li graf-after--li">the latest and greatest packages of <strong class="markup--strong markup--li-strong">MXNet</strong> and <strong class="markup--strong markup--li-strong">Keras-MXNet</strong>.</li></ul><blockquote name="726a" id="726a" class="graf graf--blockquote graf-after--li">You don’t have to install pre-releases packages. I just like to live dangerously and add extra spice to my oh-so-quiet everyday life :*)</blockquote><p name="7aa3" id="7aa3" class="graf graf--p graf-after--blockquote">Once this is done, we clean up various caches to shrink the container size a bit. Then, we copy :</p><ul class="postList"><li name="1e49" id="1e49" class="graf graf--li graf-after--p">the <strong class="markup--strong markup--li-strong">Keras script</strong> to <em class="markup--em markup--li-em">/opt/program</em> with the proper name (‘<em class="markup--em markup--li-em">train</em>’) and we make it executable.</li></ul><blockquote name="8ff0" id="8ff0" class="graf graf--blockquote graf-after--li">For more flexibility, we could write a generic launcher that would fetch the actual training script from an S3 location passed as an hyper parameter. This is left as an exercise for the reader ;)</blockquote><ul class="postList"><li name="e35b" id="e35b" class="graf graf--li graf-after--blockquote">the <strong class="markup--strong markup--li-strong">Keras configuration file</strong> to <em class="markup--em markup--li-em">/root/.keras/keras.json</em>.</li></ul><p name="5a63" id="5a63" class="graf graf--p graf-after--li">Finally, we set the directory of our script as the <strong class="markup--strong markup--p-strong">work directory</strong> and add it to the <strong class="markup--strong markup--p-strong">path</strong>.</p><p name="7214" id="7214" class="graf graf--p graf-after--p">It’s not a long file, but as usual with these things, every detail counts.</p><h4 name="229c" id="229c" class="graf graf--h4 graf-after--p">Building a GPU-based Docker container</h4><p name="57c1" id="57c1" class="graf graf--p graf-after--h4">Now let’s build its GPU counterpart. It differs in only two ways:</p><ul class="postList"><li name="ca7b" id="ca7b" class="graf graf--li graf-after--p">we start from the <strong class="markup--strong markup--li-strong">CUDA 9.0 image</strong>, which is also based on Ubuntu 16.04. This one has all the CUDA libraries that MXNet needs (unlike the smaller 9.0-base, don’t bother trying it).</li><li name="2d3e" id="2d3e" class="graf graf--li graf-after--li">we install the <strong class="markup--strong markup--li-strong">CUDA 9.0-enabled MXNet</strong> package.</li></ul><p name="9bdf" id="9bdf" class="graf graf--p graf-after--li">Everything else is the same as before.</p><figure name="a086" id="a086" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/705e282bd5497b8a9f9351517871c56b.js"></script></figure><h4 name="8f5c" id="8f5c" class="graf graf--h4 graf-after--figure">Creating a Docker repository in Amazon ECR</h4><p name="753a" id="753a" class="graf graf--p graf-after--h4">SageMaker requires that the containers it fetches are hosted in <a href="http://aws.amazon.com/ecr" data-href="http://aws.amazon.com/ecr" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon ECR</strong></a>. Let’s create a repo and login to it.</p><figure name="c801" id="c801" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/08388e7358a618e62f92de1d409f6808.js"></script></figure><h4 name="2ca1" id="2ca1" class="graf graf--h4 graf-after--figure">Building and pushing our containers to ECR</h4><p name="c21d" id="c21d" class="graf graf--p graf-after--h4">OK, now it’s time to build both containers and push them to their repos. We’ll do this separately for the CPU and GPU versions. Strictly Docker stuff. Please refer to the notebook for details on variables.</p><figure name="da51" id="da51" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/edb9fedb661479a9db77c21fbddd9ed6.js"></script></figure><p name="8b48" id="8b48" class="graf graf--p graf-after--figure">Once we’re done, things should look like this and you should also see your two containers in ECR.</p><figure name="d33c" id="d33c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1sXM6ChcKtg6zLKdplBqag.png" data-width="1738" data-height="184" src="https://cdn-images-1.medium.com/max/800/1*1sXM6ChcKtg6zLKdplBqag.png"></figure><p name="19eb" id="19eb" class="graf graf--p graf-after--figure">The Docker part is over. Now let’s configure our training job in SageMaker.</p><h3 name="4ccd" id="4ccd" class="graf graf--h3 graf-after--p">Configuring the training job</h3><p name="2181" id="2181" class="graf graf--p graf-after--h3">This is actually quite underwhelming, which is great news: <strong class="markup--strong markup--p-strong">nothing really differs from training with a built-in algorithm!</strong></p><p name="2653" id="2653" class="graf graf--p graf-after--p">First we need to <strong class="markup--strong markup--p-strong">upload the MNIST data set</strong> from our local machine to S3. We’ve done this many times before, nothing new here.</p><figure name="0339" id="0339" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/c105a2d19ef6357253e1223bf2e50dae.js"></script></figure><p name="ddf7" id="ddf7" class="graf graf--p graf-after--figure">Then, we configure the training job by:</p><ul class="postList"><li name="d140" id="d140" class="graf graf--li graf-after--p">selecting one of the containers we just built and setting the usual parameters for SageMaker <a href="https://sagemaker.readthedocs.io/en/latest/estimators.html" data-href="https://sagemaker.readthedocs.io/en/latest/estimators.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">estimators,</strong></a></li><li name="b235" id="b235" class="graf graf--li graf-after--li">passing <strong class="markup--strong markup--li-strong">hyper parameters</strong> to the Keras script,</li><li name="5778" id="5778" class="graf graf--li graf-after--li">passing <strong class="markup--strong markup--li-strong">input data</strong> to the Keras script.</li></ul><figure name="6f60" id="6f60" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/907c86df678b5c85255fa5f6137b2549.js"></script></figure><p name="cea8" id="cea8" class="graf graf--p graf-after--figure">That’s it for training. The last part we’re missing is adapting our Keras script for SageMaker. Let’s get to it.</p><h3 name="1286" id="1286" class="graf graf--h3 graf-after--p">Adapting the Keras script for SageMaker</h3><p name="62c6" id="62c6" class="graf graf--p graf-after--h3">We need to take care of hyper parameters, input data, multi-GPU configuration, loading the data set and saving models.</p><h4 name="6d28" id="6d28" class="graf graf--h4 graf-after--p">Passing hyper parameters and input data configuration</h4><p name="306a" id="306a" class="graf graf--p graf-after--h4">As mentioned earlier, SageMaker copies hyper parameters to <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">/opt/ml/input/config/hyperparameters.json</em></strong>. All we have to do is read this file, extract parameters and set default values if needed.</p><figure name="d9dd" id="d9dd" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/72c41b0beba8b6ad809523869d097ca2.js"></script></figure><p name="934c" id="934c" class="graf graf--p graf-after--figure">In a similar fashion, SageMaker copies the input data configuration to <em class="markup--em markup--p-em">/opt/ml/input/data. </em>We’ll handle things in exactly the same way.</p><blockquote name="1d93" id="1d93" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">In this example, I don’t need this configuration info, but this is how you’d read it if you did :)</blockquote><h4 name="1dbd" id="1dbd" class="graf graf--h4 graf-after--blockquote">Loading the training and validation set</h4><p name="9f98" id="9f98" class="graf graf--p graf-after--h4">When training in file mode (which is the case here), SageMaker <strong class="markup--strong markup--p-strong">automatically</strong> copies the data set to <em class="markup--em markup--p-em">/opt/ml/input/&lt;channel_name&gt;</em>: here, we defined the <em class="markup--em markup--p-em">train</em> and <em class="markup--em markup--p-em">validation</em> channels, so we’ll simply read the MNIST files from the corresponding directories.</p><figure name="8f2e" id="8f2e" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/216609616f5fb1af0e2a18203ca2f243.js"></script></figure><h4 name="1f1e" id="1f1e" class="graf graf--h4 graf-after--figure">Configuring multi-GPU training</h4><p name="a427" id="a427" class="graf graf--p graf-after--h4">As explained in a previous post, Keras-MXNet makes it very easy to set up multi-GPU training. Depending on the <em class="markup--em markup--p-em">gpu_count</em> hyper parameter, we just need to wrap our model with a bespoke Keras API before compiling it:</p><figure name="9d57" id="9d57" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/492e137a28ada394711b2a2218da478f.js"></script></figure><p name="682b" id="682b" class="graf graf--p graf-after--figure">Ain’t life grand?</p><h4 name="9b98" id="9b98" class="graf graf--h4 graf-after--p">Saving models</h4><p name="031b" id="031b" class="graf graf--p graf-after--h4">The very last thing we need to do once training is complete is to save the model in <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">/opt/ml/model: </em></strong>SageMaker will grab all artefacts present in this directory, build a file called <em class="markup--em markup--p-em">model.tar.gz</em> and copy it to the S3 bucket used by the training job.</p><p name="3999" id="3999" class="graf graf--p graf-after--p">In fact, we’re going to save the trained model in two different formats : the <strong class="markup--strong markup--p-strong">Keras format</strong> (i.e. an HDF5 file) and the native <strong class="markup--strong markup--p-strong">MXNet format</strong> (i.e. a JSON file and a .<em class="markup--em markup--p-em">params</em> file). This will allow us to use it with both libraries!</p><figure name="fcf9" id="fcf9" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/e42ddb2c5a251ff22069d788c0974961.js"></script></figure><p name="1bda" id="1bda" class="graf graf--p graf-after--figure">That’s it. As you can see, it’s all about interfacing your script with SageMaker input and output. The bulk of your Keras code doesn’t require any modification.</p><h3 name="0f95" id="0f95" class="graf graf--h3 graf-after--p">Running the script</h3><p name="7de9" id="7de9" class="graf graf--p graf-after--h3">Alright, let’s run the GPU version! We’ll train on <strong class="markup--strong markup--p-strong">2 GPUs</strong> hosted in a <strong class="markup--strong markup--p-strong">p3.8xlarge</strong> instance.</p><figure name="f403" id="f403" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/16a252b6284bcfde3a29025ccb8c4c8e.js"></script></figure><p name="57be" id="57be" class="graf graf--p graf-after--figure">Let’s check the <strong class="markup--strong markup--p-strong">S3 bucket</strong>.</p><pre name="1639" id="1639" class="graf graf--pre graf-after--p">$ <strong class="markup--strong markup--pre-strong">aws</strong> s3 ls $BUCKET/keras-mxnet-gpu/output/keras-mxnet-mnist-cnn-2018-05-30-17-39-50-724/output/<br>2018-05-30 17:43:34    8916913 model.tar.gz<br>$ <strong class="markup--strong markup--pre-strong">aws</strong> s3 cp $BUCKET/keras-mxnet-gpu/output/keras-mxnet-mnist-cnn-2018-05-30-17-39-50-724/output/model.tar.gz .<br>$ <strong class="markup--strong markup--pre-strong">tar</strong> tvfz model.tar.gz<br>-rw-r--r-- 0/0   4822688 2018-05-30 17:43 <strong class="markup--strong markup--pre-strong">mnist-cnn-10.hd5</strong><br>-rw-r--r-- 0/0   4800092 2018-05-30 17:43 <strong class="markup--strong markup--pre-strong">mnist-cnn-10-0000.params</strong><br>-rw-r--r-- 0/0      4817 2018-05-30 17:43 <strong class="markup--strong markup--pre-strong">mnist-cnn-10-symbol.json</strong></pre><p name="3d40" id="3d40" class="graf graf--p graf-after--pre">Wunderbar, as they say on the other side of the Rhine ;) We can now use these models <strong class="markup--strong markup--p-strong">anywhere</strong> we like.</p><p name="ca57" id="ca57" class="graf graf--p graf-after--p">That’s it for today. Another (hopefully) nice example of using SageMaker to <strong class="markup--strong markup--p-strong">train your custom jobs on fully-managed infrastructure</strong>!</p><p name="6890" id="6890" class="graf graf--p graf-after--p graf--trailing">Happy to answer questions here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener noopener noopener noopener" target="_blank">Twitter</a>. For more content, please feel free to check out my <a href="https://www.youtube.com/juliensimonfr" data-href="https://www.youtube.com/juliensimonfr" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener nofollow noopener noopener noopener noopener noopener" target="_blank">YouTube channel</a>.</p></div></div></section><section name="853e" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="3e64" id="3e64" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">Time to burn… some clock cycles :)</em></p><figure name="5acd" id="5acd" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://www.youtube.com/embed/f6pJfDZEAf8?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/43a34bd668ca"><time class="dt-published" datetime="2018-06-02T10:28:08.518Z">June 2, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/training-with-keras-mxnet-on-amazon-sagemaker-43a34bd668ca" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
