<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Keras shoot-out, part 3: fine-tuning</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Keras shoot-out, part 3: fine-tuning</h1>
</header>
<section data-field="subtitle" class="p-summary">
Using Keras and the CIFAR-10 dataset, we previously compared the training performance of two Deep Learning libraries, Apache MXNet and…
</section>
<section data-field="body" class="e-content">
<section name="1682" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a34b" id="a34b" class="graf graf--h3 graf--leading graf--title">Keras shoot-out, part 3: fine-tuning</h3><p name="ff97" id="ff97" class="graf graf--p graf-after--h3">Using <a href="http://keras.io" data-href="http://keras.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Keras</a> and the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" data-href="https://www.cs.toronto.edu/~kriz/cifar.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CIFAR-10</a> dataset, we <a href="https://medium.com/@julsimon/keras-shoot-out-tensorflow-vs-mxnet-51ae2b30a9c0" data-href="https://medium.com/@julsimon/keras-shoot-out-tensorflow-vs-mxnet-51ae2b30a9c0" class="markup--anchor markup--p-anchor" target="_blank">previously</a> compared the training performance of two Deep Learning libraries, <a href="http://mxnet.io" data-href="http://mxnet.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> and <a href="http://tensorflow.org" data-href="http://tensorflow.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tensorflow</a>.</p><p name="f3cb" id="f3cb" class="graf graf--p graf-after--p">In this article, we’ll continue to explore this theme. I’ll show you how to:</p><ul class="postList"><li name="2c14" id="2c14" class="graf graf--li graf-after--p">save and load a trained model,</li><li name="e485" id="e485" class="graf graf--li graf-after--li">build a subset of CIFAR-10 using samples from two classes,</li><li name="37c4" id="37c4" class="graf graf--li graf-after--li">retrain the model on this subset to optimise prediction (aka “fine-tuning”),</li><li name="c301" id="c301" class="graf graf--li graf-after--li">improve training time by freezing layers.</li></ul><figure name="5902" id="5902" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Z4aYh6rOW0o4mwtyX3yGlQ.jpeg" data-width="600" data-height="255" src="https://cdn-images-1.medium.com/max/800/1*Z4aYh6rOW0o4mwtyX3yGlQ.jpeg"><figcaption class="imageCaption">Go ahead, global minimum, make my day.</figcaption></figure><h4 name="1631" id="1631" class="graf graf--h4 graf-after--figure">Preparing our pre-trained models</h4><p name="783b" id="783b" class="graf graf--p graf-after--h4">Using the same script (<em class="markup--em markup--p-em">~/keras/examples/cifar10_resnet50.py)</em>, I trained a <a href="https://github.com/KaimingHe/deep-residual-networks" data-href="https://github.com/KaimingHe/deep-residual-networks" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Resnet-50</a> model on CIFAR-10 using first MXNet 0.11, then Tensorflow 1.2. Here is the setup I used:</p><ul class="postList"><li name="30c5" id="30c5" class="graf graf--li graf-after--p">All 8 GPUs on a p2.8xlarge AWS instance,</li><li name="82c4" id="82c4" class="graf graf--li graf-after--li">batch size: 256,</li><li name="08fc" id="08fc" class="graf graf--li graf-after--li">Data augmentation: enabled,</li><li name="7994" id="7994" class="graf graf--li graf-after--li">Number of epochs: 200.</li></ul><blockquote name="974d" id="974d" class="graf graf--blockquote graf-after--li">To save the trained model, I only added one line of code: <em class="markup--em markup--blockquote-em">model.save(modelname). </em>That’s all there is to it!</blockquote><p name="b7d4" id="b7d4" class="graf graf--p graf-after--blockquote">Even with 8 GPUs, training takes time: about 2h for MXNet and about 3h30 for Tensorflow. Pretty heavy lifting! I uploaded the models to my own personal <a href="http://jsimon-deep-learning-models.s3-website-us-east-1.amazonaws.com" data-href="http://jsimon-deep-learning-models.s3-website-us-east-1.amazonaws.com" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model zoo</a>: feel free to grab them and run your own tests :)</p><ul class="postList"><li name="ca13" id="ca13" class="graf graf--li graf-after--p">MXNet : <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-mxnet011rc3-0200.h5" data-href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-mxnet011rc3-0200.h5" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">model</a> (HDF5, 91MB), <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-mxnet011rc3-0200.log" data-href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-mxnet011rc3-0200.log" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">training log</a> (text, 6MB), test accuracy: <strong class="markup--strong markup--li-strong">82.12%</strong></li><li name="4893" id="4893" class="graf graf--li graf-after--li">Tensorflow: <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-tensorflow12-0200.h5" data-href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-tensorflow12-0200.h5" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">model</a> (HDF5, 181MB), <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-tensorflow12-0200.log" data-href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-tensorflow12-0200.log" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">training log</a> (text, 6MB), test accuracy: <strong class="markup--strong markup--li-strong">75.48%</strong></li></ul><p name="ee30" id="ee30" class="graf graf--p graf-after--li">Now let’s see how we can load our models.</p><h4 name="476f" id="476f" class="graf graf--h4 graf-after--p">Loading a model</h4><p name="a8c8" id="a8c8" class="graf graf--p graf-after--h4">Depending on the backend configured in <em class="markup--em markup--p-em">~/.keras/keras.json</em>, we have to load one model or the other. Using <em class="markup--em markup--p-em">keras.backend.backend()</em>, it’s easy to figure which one to pick. Then, we simply call <em class="markup--em markup--p-em">keras.models.load_model()</em>.</p><p name="2451" id="2451" class="graf graf--p graf-after--p">What about setting the number of GPUs using for training? For Tensorflow, we’ll use session parameters. For MXNet, we’ll add an extra parameter to <em class="markup--em markup--p-em">model.compile().</em></p><figure name="6375" id="6375" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/52f25c1d6bf995e1860093fff6e086f2.js"></script></figure><p name="1aaf" id="1aaf" class="graf graf--p graf-after--figure">Ok, now let’s take care of data.</p><h4 name="0da2" id="0da2" class="graf graf--h4 graf-after--p">Extracting the subset</h4><p name="d3e7" id="d3e7" class="graf graf--p graf-after--h4">Keras provides convenient functions to load commonly used data sets, including CIFAR-10. Nice!</p><p name="ab23" id="ab23" class="graf graf--p graf-after--p">First, we need to extract a given number of samples from two classes. This is the purpose of the <em class="markup--em markup--p-em">get_samples()</em> function:</p><ul class="postList"><li name="a777" id="a777" class="graf graf--li graf-after--p">find the relevant number of sample indexes matching a given class,</li><li name="f825" id="f825" class="graf graf--li graf-after--li">extract samples (‘x’) and labels (‘y’) for these indexes,</li><li name="9071" id="9071" class="graf graf--li graf-after--li">normalise pixel values for samples, since we also did it when we trained the initial model.</li></ul><figure name="1274" id="1274" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/8a55d451b82d5bf4cdd635058b028fca.js"></script></figure><h4 name="feae" id="feae" class="graf graf--h4 graf-after--figure">Building the new data set</h4><p name="504f" id="504f" class="graf graf--p graf-after--h4">The next step is to build the new training and test sets with the <em class="markup--em markup--p-em">prepare_dataset()</em> function:</p><ul class="postList"><li name="a436" id="a436" class="graf graf--li graf-after--p">Concatenate samples for both categories,</li><li name="2498" id="2498" class="graf graf--li graf-after--li">Concatenate labels for both categories,</li><li name="91b6" id="91b6" class="graf graf--li graf-after--li">One-hot encode labels, e.g. convert ‘6’ to [0, 0, 0, 0, 0, 0, 1, 0, 0, 0].</li></ul><figure name="478c" id="478c" class="graf graf--figure graf--iframe graf-after--li"><script src="https://gist.github.com/juliensimon/99589ebc877d96675c0f14e1d44040a6.js"></script></figure><p name="c813" id="c813" class="graf graf--p graf-after--figure">The last step before actually retraining is to define the optimizer — for now, we’ll use the same one as for the original training — as well as the number of GPUs — we’ll stick to one, as this is the most likely setup for developers fine-tuning a model on their own machine.</p><figure name="93fa" id="93fa" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/db533719a2ed96826965dc4411329d13.js"></script></figure><h4 name="502b" id="502b" class="graf graf--h4 graf-after--figure">Fine-tuning the model</h4><p name="7a2f" id="7a2f" class="graf graf--p graf-after--h4">OK, we’re now ready to train the model. First, let’s predict our test set to see what the baseline is. Then, we train the model. Finally, we predict the test set again to see how much the model has improved.</p><figure name="56f0" id="56f0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/79a5b5d1b8f38506cbc1a0938348fe8e.js"></script></figure><p name="5b4b" id="5b4b" class="graf graf--p graf-after--figure">Full code is available on <a href="https://github.com/juliensimon/aws/tree/master/mxnet/keras" data-href="https://github.com/juliensimon/aws/tree/master/mxnet/keras" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a>. Now, let’s run it!</p><h4 name="7ff9" id="7ff9" class="graf graf--h4 graf-after--p">Is it a horse or a car?</h4><p name="4f31" id="4f31" class="graf graf--p graf-after--h4">Let’s fine-tune the model on cars and horses (classes 1 and 7) for 10 epochs. Here are the results.</p><ul class="postList"><li name="344f" id="344f" class="graf graf--li graf-after--p">MXNet: <strong class="markup--strong markup--li-strong">31 seconds</strong> per epoch, test accuracy: <strong class="markup--strong markup--li-strong">98.79%</strong> (<a href="https://gist.github.com/juliensimon/0ea6c0301e8f813f6f55c8715f07a4f6" data-href="https://gist.github.com/juliensimon/0ea6c0301e8f813f6f55c8715f07a4f6" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">log</a>).</li><li name="7ac4" id="7ac4" class="graf graf--li graf-after--li">Tensorflow: <strong class="markup--strong markup--li-strong">44 seconds</strong> per epoch, test accuracy: <strong class="markup--strong markup--li-strong">98.55%</strong> (<a href="https://gist.github.com/juliensimon/51e78fda60f1d50583febd3e8560afa8" data-href="https://gist.github.com/juliensimon/51e78fda60f1d50583febd3e8560afa8" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">log</a>).</li></ul><p name="37e9" id="37e9" class="graf graf--p graf-after--li">This is a MASSIVE accuracy improvement after <strong class="markup--strong markup--p-strong">just a few minutes</strong> of training. Just wait, we can do even better :)</p><h4 name="ad56" id="ad56" class="graf graf--h4 graf-after--p">Freezing layers</h4><p name="ea8f" id="ea8f" class="graf graf--p graf-after--h4">The many layers in our model have already learned the car and horse classes. So, it’s probably a waste of time to potentially retrain <strong class="markup--strong markup--p-strong">all</strong> of them. Maybe it would just be enough to retrain <strong class="markup--strong markup--p-strong">the last layer</strong>, i.e. the one that actually outputs the probability for the 10 classes.</p><p name="eb2f" id="eb2f" class="graf graf--p graf-after--p">Keras includes a very nice feature that lets us decide which layers of a model are trainable and which aren’t. Let’s use it to freeze <strong class="markup--strong markup--p-strong">all layers but the last one</strong> and try again.</p><figure name="acc8" id="acc8" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/8861a66e4666cd8c7d02f0a8c384f850.js"></script></figure><p name="3af9" id="3af9" class="graf graf--p graf-after--figure">Here are the results.</p><ul class="postList"><li name="29be" id="29be" class="graf graf--li graf-after--p">MXNet: <strong class="markup--strong markup--li-strong">12 seconds</strong> per epoch, test accuracy: <strong class="markup--strong markup--li-strong">97.29%</strong> (<a href="https://gist.github.com/juliensimon/d1e9b2cacc5ced3ed1a8a081d62105f3" data-href="https://gist.github.com/juliensimon/d1e9b2cacc5ced3ed1a8a081d62105f3" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">log</a>).</li><li name="d55b" id="d55b" class="graf graf--li graf-after--li">Tensorflow: <strong class="markup--strong markup--li-strong">13 seconds</strong> per epoch, test accuracy: <strong class="markup--strong markup--li-strong">98.35%</strong> (<a href="https://gist.github.com/juliensimon/da4e78116c883abcf11a0af831a81cd9" data-href="https://gist.github.com/juliensimon/da4e78116c883abcf11a0af831a81cd9" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">log</a>).</li></ul><p name="778c" id="778c" class="graf graf--p graf-after--li">As expected, freezing layers <strong class="markup--strong markup--p-strong">significantly</strong> reduces training time.</p><p name="15e9" id="15e9" class="graf graf--p graf-after--p">Training time is almost identical for both libraries. I guess the work boils down to running backpropagation on a single layer, which both libraries can do equally well on a single GPU. Scaling doesn’t seem to come into play here.</p><p name="4d7d" id="4d7d" class="graf graf--p graf-after--p">Accuracy is hardly impacted for Tensorflow, but there is a slight hit for MXNet. Hmmm. Maybe our optimisation parameters are not optimal. Let’s try one last thing :)</p><h4 name="4ebf" id="4ebf" class="graf graf--h4 graf-after--p">Tweaking the optimiser</h4><p name="6516" id="6516" class="graf graf--p graf-after--h4">Picking the right hyper parameters for SGD(learning rate, etc.) is tricky. Here, we’re only training for 10 epochs, which probably makes it even more difficult. One way out of this problem may be to use the <strong class="markup--strong markup--p-strong">AdaGrad</strong> optimizer, which <strong class="markup--strong markup--p-strong">automatically</strong> adapts the learning rate</p><blockquote name="2f42" id="2f42" class="graf graf--blockquote graf-after--p">For an excellent overview of SGD and its variants, please read <a href="http://ruder.io/optimizing-gradient-descent/" data-href="http://ruder.io/optimizing-gradient-descent/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">this post</a> by Sebastian Ruder. It’s by far the best I’ve seen.</blockquote><p name="728c" id="728c" class="graf graf--p graf-after--blockquote">Here are the results.</p><ul class="postList"><li name="e456" id="e456" class="graf graf--li graf-after--p">MXNet: <strong class="markup--strong markup--li-strong">12 seconds</strong> per epoch, test accuracy: <strong class="markup--strong markup--li-strong">98.90% </strong>after 9 epochs (<a href="https://gist.github.com/juliensimon/1e7edf23993cf36657c349fc15b5cbed" data-href="https://gist.github.com/juliensimon/1e7edf23993cf36657c349fc15b5cbed" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">log</a>).</li><li name="f303" id="f303" class="graf graf--li graf-after--li">Tensorflow: <strong class="markup--strong markup--li-strong">13 seconds</strong> per epoch, test accuracy: <strong class="markup--strong markup--li-strong">98.75% </strong>after 8 epochs (<a href="https://gist.github.com/juliensimon/963522ca6db35431544cf88bb130e8a4" data-href="https://gist.github.com/juliensimon/963522ca6db35431544cf88bb130e8a4" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">log</a>).</li></ul><p name="5338" id="5338" class="graf graf--p graf-after--li">AdaGrad works its magic indeed. It helps MXNet improve its accuracy and deliver the best score in this test. Tensorflow improves as well.</p><h4 name="c185" id="c185" class="graf graf--h4 graf-after--p">Going further</h4><p name="4ede" id="4ede" class="graf graf--p graf-after--h4">Here, we fine-tuned the model for two classes, but we still output 10 probabilities (one for each class). The next step would be to add an extra layer with <strong class="markup--strong markup--p-strong">only two outputs</strong>, forcing the model to decide on two categories only (and not ten).</p><p name="b015" id="b015" class="graf graf--p graf-after--p">We would need to modify our subset labels, i.e. one-hot encode them with only two values. Let’s keep this for a future article :)</p><h4 name="e233" id="e233" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="1b98" id="1b98" class="graf graf--p graf-after--h4">Fine-tuning is a very powerful way to improve the accuracy of a model in a <strong class="markup--strong markup--p-strong">very short period of time </strong>compared to training. It’s a great technique when you can’t or don’t want to spend time and money (re)training complex models on large data sets. Just make sure you understand how the model was trained (data set, data format, etc.) in order to retrain it appropriately.</p><p name="c749" id="c749" class="graf graf--p graf-after--p graf--trailing">That’s it for today. Thank you for reading.</p></div></div></section><section name="9a1d" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9181" id="9181" class="graf graf--p graf--leading graf--trailing"><em class="markup--em markup--p-em">No animals were harmed during the writing of this article, but there sure was a whole lot of creative swearing and keyboard smashing during the coding phase.</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/7d1548c51a41"><time class="dt-published" datetime="2017-09-10T12:46:23.375Z">September 10, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/keras-shoot-out-part-3-fine-tuning-7d1548c51a41" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
