<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration</h1>
</header>
<section data-field="subtitle" class="p-summary">
This was originally posted on the Hugging Face blog.
</section>
<section data-field="body" class="e-content">
<section name="ae6d" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3221" id="3221" class="graf graf--h3 graf--leading graf--title">Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration</h3><p name="2740" id="2740" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">This was originally posted on the </em><a href="https://huggingface.co/blog" data-href="https://huggingface.co/blog" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Hugging Face blog</em></a><em class="markup--em markup--p-em">.</em></p><figure name="346e" id="346e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*UqKxOEjTlF72EEsG.png" data-width="2400" data-height="1296" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*UqKxOEjTlF72EEsG.png"></figure><p name="ccf1" id="ccf1" class="graf graf--p graf-after--figure">The mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.</p><p name="8e93" id="8e93" class="graf graf--p graf-after--p">Today, we’re excited to announce that Intel has officially joined our <a href="https://huggingface.co/hardware" data-href="https://huggingface.co/hardware" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hardware Partner Program</a>. Thanks to the <a href="https://github.com/huggingface/optimum-intel" data-href="https://github.com/huggingface/optimum-intel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Optimum</a> open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.</p><p name="6e78" id="6e78" class="graf graf--p graf-after--p">Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.</p><p name="1585" id="1585" class="graf graf--p graf-after--p">Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.</p><p name="0fe8" id="0fe8" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“<em class="markup--em markup--p-em">We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.</em>”, says Wei Li, Intel Vice President &amp; General Manager, AI and Analytics.</p><p name="0251" id="0251" class="graf graf--p graf-after--p">In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference (<a href="https://huggingface.co/blog/bert-cpu-scaling-part-1" data-href="https://huggingface.co/blog/bert-cpu-scaling-part-1" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">part 1</a>, <a href="https://huggingface.co/blog/bert-cpu-scaling-part-2" data-href="https://huggingface.co/blog/bert-cpu-scaling-part-2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">part 2</a>) and achieved <a href="https://huggingface.co/blog/infinity-cpu-performance" data-href="https://huggingface.co/blog/infinity-cpu-performance" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">single-digit millisecond latency</a> for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for <a href="https://huggingface.co/blog/getting-started-habana" data-href="https://huggingface.co/blog/getting-started-habana" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Habana Gaudi</a> accelerators, which deliver up to 40% better price-performance than GPUs.</p><p name="4ba6" id="4ba6" class="graf graf--p graf-after--p">The next logical step was to expand on this work and share it with the ML community. Enter the <a href="https://github.com/huggingface/optimum-intel" data-href="https://github.com/huggingface/optimum-intel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Optimum Intel</a> open source library! Let’s take a deeper look at it.</p><h3 name="006b" id="006b" class="graf graf--h3 graf-after--p">Get Peak Transformers Performance with Optimum Intel</h3><p name="1428" id="1428" class="graf graf--p graf-after--h3"><a href="https://github.com/huggingface/optimum" data-href="https://github.com/huggingface/optimum" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Optimum</a> is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.</p><p name="d0d9" id="d0d9" class="graf graf--p graf-after--p"><a href="https://github.com/huggingface/optimum-intel" data-href="https://github.com/huggingface/optimum-intel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Optimum Intel</a> is part of Optimum and builds on top of the <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html" data-href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Intel Neural Compressor</a> (INC). INC is an <a href="https://github.com/intel/neural-compressor" data-href="https://github.com/intel/neural-compressor" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">open-source library</a> that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.</p><p name="8918" id="8918" class="graf graf--p graf-after--p">With Optimum Intel, you can apply state-of-the-art optimization techniques to your Transformers with minimal effort. Let’s look at a complete example.</p><h3 name="b314" id="b314" class="graf graf--h3 graf-after--p">Case study: Quantizing DistilBERT with Optimum Intel</h3><p name="2e22" id="2e22" class="graf graf--p graf-after--h3">In this example, we will run post-training quantization on a DistilBERT model fine-tuned for classification. Quantization is a process that shrinks memory and compute requirements by reducing the bit width of model parameters. For example, you can often replace 32-bit floating-point parameters with 8-bit integers at the expense of a small drop in prediction accuracy.</p><p name="b66c" id="b66c" class="graf graf--p graf-after--p">We have already fine-tuned the original model to classify product reviews for shoes according to their star rating (from 1 to 5 stars). You can view this <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews" data-href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model</a> and its <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized?" data-href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized?" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">quantized</a> version on the Hugging Face hub. You can also test the original model in this <a href="https://huggingface.co/spaces/juliensimon/amazon-shoe-reviews-spaces" data-href="https://huggingface.co/spaces/juliensimon/amazon-shoe-reviews-spaces" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Space</a>.</p><p name="8e83" id="8e83" class="graf graf--p graf-after--p">Let’s get started! All code is available in this <a href="https://gitlab.com/juliensimon/huggingface-demos/-/blob/main/amazon-shoes/03_optimize_inc_quantize.ipynb" data-href="https://gitlab.com/juliensimon/huggingface-demos/-/blob/main/amazon-shoes/03_optimize_inc_quantize.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">notebook</a>.</p><p name="028a" id="028a" class="graf graf--p graf-after--p">As usual, the first step is to install all required libraries. It’s worth mentioning that we have to work with a CPU-only version of PyTorch for the quantization process to work correctly.</p><pre name="6815" id="6815" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">pip -q uninstall torch -y <br>pip -q install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu<br>pip -q install transformers datasets optimum[intel] evaluate --upgrade</code></pre><p name="f0a7" id="f0a7" class="graf graf--p graf-after--pre">Then, we prepare an evaluation dataset to assess model performance during quantization. Starting from the dataset we used to fine-tune the original model, we only keep a few thousand reviews and their labels and save them to local storage.</p><p name="b1f2" id="b1f2" class="graf graf--p graf-after--p">Next, we load the original model, its tokenizer, and the evaluation dataset.</p><pre name="ae19" id="ae19" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">model_name = &quot;juliensimon/distilbert-amazon-shoe-reviews&quot; <br>model = AutoModelForSequenceClassification.from_pretrained(<br>        model_name, num_labels=5)<br>tokenizer = AutoTokenizer.from_pretrained(model_name) <br>eval_dataset = load_from_disk(&quot;./data/amazon_shoe_reviews_test&quot;)</code></pre><p name="faa0" id="faa0" class="graf graf--p graf-after--pre">Next, we define an evaluation function that computes model metrics on the evaluation dataset. This allows the Optimum Intel library to compare these metrics before and after quantization. For this purpose, the Hugging Face <a href="https://github.com/huggingface/evaluate/" data-href="https://github.com/huggingface/evaluate/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">evaluate</a> library is very convenient!</p><pre name="4d94" id="4d94" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">from evaluate import evaluator</code></pre><pre name="82fb" id="82fb" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">def eval_func(model): <br>    eval = evaluator(&quot;text-classification&quot;) <br>    results = eval.compute(<br>        model_or_pipeline=model,<br>        tokenizer=tokenizer,<br>        data=eval_dataset,<br>        metric=evaluate.load(&quot;accuracy&quot;),<br>        label_column=&quot;labels&quot;,<br>        label_mapping=model.config.label2id,<br>    )<br>    return results[&quot;accuracy&quot;]</code></pre><p name="d38e" id="d38e" class="graf graf--p graf-after--pre">We then set up the quantization job using a <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews/blob/main/quantize.yml" data-href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews/blob/main/quantize.yml" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">configuration file</a> that we download from the Hugging Face hub. You can find details on this file in the INC <a href="https://intel.github.io/neural-compressor/docs/tuning_strategies.html" data-href="https://intel.github.io/neural-compressor/docs/tuning_strategies.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentation</a>. Here, we go for post-training dynamic quantization with an acceptable accuracy drop of 3%.</p><pre name="d81a" id="d81a" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">quantization:<br>    approach: post_training_dynamic_quant<br>tuning:<br>    accuracy_criterion:<br>        relative: 0.03</code></pre><p name="a95e" id="a95e" class="graf graf--p graf-after--pre">Next, we create the corresponding quantization objects with the Optimum Intel API.</p><pre name="9874" id="9874" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">from optimum.intel.neural_compressor import IncOptimizer, IncQuantizer<br>from optimum.intel.neural_compressor.configuration import IncQuantizationConfig</code></pre><pre name="d9be" id="d9be" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">quantization_config = IncQuantizationConfig.from_pretrained(<br>    config_name_or_path=&quot;juliensimon/distilbert-amazon-shoe-reviews&quot;,<br>    config_file_name=&quot;quantize.yml&quot;<br>)<br>inc_quantizer = IncQuantizer(quantization_config, eval_func=eval_func)</code></pre><p name="d1e5" id="d1e5" class="graf graf--p graf-after--pre">We can now launch the quantization job.</p><pre name="c2b0" id="c2b0" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">inc_optimizer = IncOptimizer(model, quantizer=inc_quantizer)<br>inc_model = inc_optimizer.fit()</code></pre><p name="bc21" id="bc21" class="graf graf--p graf-after--pre">The log tells us that Optimum Intel has quantized 38 <code class="markup--code markup--p-code">Linear</code> operators.</p><pre name="7eb0" id="7eb0" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">[INFO] |*****Mixed Precision Statistics*****|<br>[INFO] +--------------+-----------+---------+<br>[INFO] |   Op Type    |   Total   |   INT8  |<br>[INFO] +--------------+-----------+---------+<br>[INFO] |    Linear    |     38    |    38   |<br>[INFO] +--------------+-----------+---------+<br>[INFO] Pass quantize model elapsed time: 900.74 ms</code></pre><p name="9905" id="9905" class="graf graf--p graf-after--pre">Comparing the first layer of the original model (<code class="markup--code markup--p-code">model.distilbert.transformer.layer[0]</code>) and its quantized version (<code class="markup--code markup--p-code">inc_model.distilbert.transformer.layer[0]</code>), we see that <code class="markup--code markup--p-code">Linear</code> has indeed been replaced by <code class="markup--code markup--p-code">DynamicQuantizedLinear</code>, its quantized equivalent.</p><pre name="9cc0" id="9cc0" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code"># Original model</code></pre><pre name="6e92" id="6e92" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">TransformerBlock(<br>  (attention): MultiHeadSelfAttention(<br>    (dropout): Dropout(p=0.1, inplace=False)<br>    (q_lin): Linear(in_features=768, out_features=768, bias=True)<br>    (k_lin): Linear(in_features=768, out_features=768, bias=True)<br>    (v_lin): Linear(in_features=768, out_features=768, bias=True)<br>    (out_lin): Linear(in_features=768, out_features=768, bias=True)<br>  )<br>  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)<br>  (ffn): FFN(<br>    (dropout): Dropout(p=0.1, inplace=False)<br>    (lin1): Linear(in_features=768, out_features=3072, bias=True)<br>    (lin2): Linear(in_features=3072, out_features=768, bias=True)<br>  )<br>  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)<br>)</code></pre><pre name="09dd" id="09dd" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code"># Quantized model</code></pre><pre name="d247" id="d247" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">TransformerBlock(<br>  (attention): MultiHeadSelfAttention(<br>    (dropout): Dropout(p=0.1, inplace=False)<br>    (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)<br>    (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)<br>    (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)<br>    (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)<br>  )<br>  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)<br>  (ffn): FFN(<br>    (dropout): Dropout(p=0.1, inplace=False)<br>    (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_channel_affine)<br>    (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)<br>  )<br>  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)<br>)</code></pre><p name="8197" id="8197" class="graf graf--p graf-after--pre">Very well, but how does this impact accuracy and prediction time?</p><p name="7bb2" id="7bb2" class="graf graf--p graf-after--p">Before and after each quantization step, Optimum Intel runs the evaluation function on the current model. Interestingly, the accuracy of the quantized model is now a bit higher (<code class="markup--code markup--p-code">0.6906</code>) than the original model (<code class="markup--code markup--p-code">0.6893</code>). Indeed, quantization can make models more robust and less prone to over-fitting, and help increase accuracy. Likewise, we see that the quantized model predicts the evaluation set 13% faster than the original model. Not bad for a few lines of code!</p><p name="bb29" id="bb29" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">+--------------------+-----------+---------------+------------------+<br>[INFO] | Info Type | Baseline | Tune 1 result | Best tune result |<br>[INFO] | Accuracy | 0.6893 | 0.6906 | 0.6906 |<br>[INFO] | Duration (seconds) | 106.3149 | 92.4634 | 92.4634 |<br>+--------------------+-----------+---------------+------------------+</code></p><p name="0491" id="0491" class="graf graf--p graf-after--p">Accuracy didn’t drop, and Optimum Intel stopped the quantization job after the first step. Had accuracy dropped more than the allowed 3%, Optimum Intel would have tried to quantize different parts of the models until it would have reached an acceptable drop, or the maximum number of trials.</p><p name="bd50" id="bd50" class="graf graf--p graf-after--p">Finally, we save the model and its configuration file to local storage.</p><pre name="df22" id="df22" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">model_dir = &quot;./model_inc&quot;<br>inc_optimizer.save_pretrained(model_dir)</code></pre><p name="8699" id="8699" class="graf graf--p graf-after--pre">Once we’ve created a new <a href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized" data-href="https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">repository</a> on the Hugging Face hub and pushed the quantized model to it, we can load it in the usual way.</p><pre name="a6b0" id="a6b0" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">from optimum.intel.neural_compressor.quantization import IncQuantizedModelForSequenceClassification</code></pre><pre name="f93a" id="f93a" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">inc_model = IncQuantizedModelForSequenceClassification.from_pretrained(<br>    &quot;juliensimon/distilbert-amazon-shoe-reviews-quantized&quot;<br>)</code></pre><h3 name="6518" id="6518" class="graf graf--h3 graf-after--pre">We’re only getting started</h3><p name="3856" id="3856" class="graf graf--p graf-after--h3">In this example, we showed you how to easily quantize models post-training with Optimum Intel, and that’s just the beginning. The library supports other types of quantization as well as pruning, a technique that zeroes or removes model parameters that have little or no impact on the predicted outcome.</p><p name="ed02" id="ed02" class="graf graf--p graf-after--p">We are excited to partner with Intel to bring Hugging Face users peak efficiency on the latest Intel Xeon CPUs and Intel AI libraries. Please <a href="https://github.com/huggingface/optimum-intel" data-href="https://github.com/huggingface/optimum-intel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">give Optimum Intel a star</a> to get updates, and stay tuned for many upcoming features!</p><p name="0014" id="0014" class="graf graf--p graf-after--p graf--trailing"><em class="markup--em markup--p-em">Many thanks to </em><a href="https://github.com/echarlaix" data-href="https://github.com/echarlaix" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Ella Charlaix</em></a><em class="markup--em markup--p-em"> for her help on this post.</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/b76e65214739"><time class="dt-published" datetime="2022-06-17T07:02:08.608Z">June 17, 2022</time></a>.</p><p><a href="https://medium.com/@julsimon/intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration-b76e65214739" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
