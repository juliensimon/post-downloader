<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Exploring (ahem) AWS DeepLens</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Exploring (ahem) AWS DeepLens</h1>
</header>
<section data-field="subtitle" class="p-summary">
Before you ask: everything in this post in based on publicly available information. No secrets, animals or Deep Learning hardware have been…
</section>
<section data-field="body" class="e-content">
<section name="224c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2b3f" id="2b3f" class="graf graf--h3 graf--leading graf--title">Exploring (ahem) AWS DeepLens</h3><blockquote name="eb87" id="eb87" class="graf graf--blockquote graf--hasDropCapModel graf-after--h3">Before you ask: everything in this post in based on publicly available information. No secrets, animals or Deep Learning hardware have been harmed during the writing process :-P</blockquote><p name="9f32" id="9f32" class="graf graf--p graf-after--blockquote"><a href="https://aws.amazon.com/deeplens/" data-href="https://aws.amazon.com/deeplens/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS DeepLens</a> was one of the most surprising launches at at re:Invent 2017. Built on top of AWS services such as <a href="http://aws.amazon.com/lambda" data-href="http://aws.amazon.com/lambda" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Lambda</a> and <a href="http://aws.amazon.com/greengrass" data-href="http://aws.amazon.com/greengrass" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Greengrass</a>, this Intel-powered camera lets developers experiment with Deep Learning in a fun and practical way.</p><p name="bb38" id="bb38" class="graf graf--p graf-after--p">Out of the box, <a href="http://docs.aws.amazon.com/deeplens/latest/dg/deeplens-templated-projects-overview.html" data-href="http://docs.aws.amazon.com/deeplens/latest/dg/deeplens-templated-projects-overview.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">a number of projects</a> can be deployed to the camera in just a few clicks: face detection, object recognition, activity detection, neural art, etc.</p><figure name="0382" id="0382" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oEH-JDjv0RtzJVJmk4nyDQ.png" data-width="1200" data-height="628" src="https://cdn-images-1.medium.com/max/800/1*oEH-JDjv0RtzJVJmk4nyDQ.png"><figcaption class="imageCaption">Try explaining to your kids that this *is* serious work…</figcaption></figure><p name="f648" id="f648" class="graf graf--p graf-after--figure">The overall process looks like this:</p><ol class="postList"><li name="7081" id="7081" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Train an Deep Learning model</strong> in the cloud with <a href="https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4" data-href="https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4" class="markup--anchor markup--li-anchor" target="_blank">Apache MXNet</a>.</li><li name="8cee" id="8cee" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Write a Lambda function</strong> using the <a href="http://docs.aws.amazon.com/deeplens/latest/dg/deeplens-device-library.html" data-href="http://docs.aws.amazon.com/deeplens/latest/dg/deeplens-device-library.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">DeepLens SDK</a> to run inference on images coming from the camera.</li><li name="df13" id="df13" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Bundle both</strong> in a DeepLens project.</li><li name="9f9f" id="9f9f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Deploy the project</strong> to your DeepLens camera (using Greengrass, although this is completely transparent)</li><li name="183a" id="183a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Run the project</strong> on your DeepLens camera, view the project video stream and receive <a href="https://aws.amazon.com/iot/" data-href="https://aws.amazon.com/iot/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">AWS IoT</a> messages sent by the Lambda function.</li></ol><p name="38f9" id="38f9" class="graf graf--p graf-after--li">This is seriously cool and fun, but I want to know how this really works. Don’t you? Yeah, I thought so :)</p><h4 name="733a" id="733a" class="graf graf--h4 graf-after--p">Models are not always what they seem</h4><p name="9499" id="9499" class="graf graf--p graf-after--h4">In the DeepLens console, we can easily see that <strong class="markup--strong markup--p-strong">project models are deployed from S3</strong>. Here’s how it looks for the face detection project.</p><figure name="189c" id="189c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FX2BJ3Z-JPxwa7BUkttEOw.png" data-width="1718" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*FX2BJ3Z-JPxwa7BUkttEOw.png"></figure><p name="77b1" id="77b1" class="graf graf--p graf-after--figure">Let’s take a look, then.</p><pre name="03ef" id="03ef" class="graf graf--pre graf-after--p">$ aws s3 ls s3://deeplens-managed-resources/models/SSDFacialDetect/<br>2017-11-23 08:57:20   54559092 mxnet_deploy_ssd_FP16_FUSED.bin<br>2017-11-23 08:57:20     127913 mxnet_deploy_ssd_FP16_FUSED.xml</pre><p name="7e1e" id="7e1e" class="graf graf--p graf-after--pre">Huh? This is <strong class="markup--strong markup--p-strong">not</strong> what an MXNet looks like. <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" class="markup--anchor markup--p-anchor" target="_blank">As explained before</a>, we should see a <strong class="markup--strong markup--p-strong">JSON file</strong> holding the model definition and <strong class="markup--strong markup--p-strong">PARAMS file</strong> storing model weights.</p><figure name="846d" id="846d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2ZuUhCOy18k6PoLOLBeEVA.png" data-width="625" data-height="261" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*2ZuUhCOy18k6PoLOLBeEVA.png"></figure><p name="06f9" id="06f9" class="graf graf--p graf-after--figure">Let’s <em class="markup--em markup--p-em">ssh</em> to the camera and try to figure this out.</p><h4 name="24a0" id="24a0" class="graf graf--h4 graf-after--p">Exploring DeepLens</h4><p name="3552" id="3552" class="graf graf--p graf-after--h4">After a few minutes, well… bingo.</p><pre name="5b78" id="5b78" class="graf graf--pre graf-after--p">aws_cam@Deepcam:/opt/intel$ ls<br>deeplearning_deploymenttoolkit  deeplearning_deploymenttoolkit_2017.1.0.5675  <br>intel_sdp_products.db  <br>intel_sdp_products.tgz.db  <br>ism  <br>opencl</pre><p name="59cd" id="59cd" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Intel Deep Learning Deployment Toolkit</strong>. This sounds exciting. A few seconds of googling later, we learn <a href="https://software.intel.com/en-us/inference-engine-devguide-introduction" data-href="https://software.intel.com/en-us/inference-engine-devguide-introduction" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a> that this SDK includes:</p><ul class="postList"><li name="fffa" id="fffa" class="graf graf--li graf-after--p">A <strong class="markup--strong markup--li-strong">Model Optimizer</strong>, which converts our trained model into an optimized <strong class="markup--strong markup--li-strong">Intermediate Representation</strong> (IR).</li><li name="2956" id="2956" class="graf graf--li graf-after--li">An <strong class="markup--strong markup--li-strong">Inference Engine</strong> optimized for the underlying hardware platform.</li></ul><figure name="9456" id="9456" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*IJpotXt4XuTIAk9MkyB-uQ.png" data-width="961" data-height="279" src="https://cdn-images-1.medium.com/max/800/1*IJpotXt4XuTIAk9MkyB-uQ.png"><figcaption class="imageCaption">Source: Intel</figcaption></figure><p name="1b5e" id="1b5e" class="graf graf--p graf-after--figure">This makes <strong class="markup--strong markup--p-strong">a lot</strong> of sense. Although it’s perfectly capable to run in resource-constrained environments — as demonstrated by my <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--p-anchor" target="_blank">Raspberry Pi experiment </a>— Apache MXNet is not the best option here. First, it carries a lot of code (training, data loading, etc.) which is useless in an <strong class="markup--strong markup--p-strong">inference context</strong>. Second, it simply cannot compete with a platform-specific implementation making full use of <strong class="markup--strong markup--p-strong">dedicated hardware</strong>, <strong class="markup--strong markup--p-strong">special instructions</strong> and so on.</p><p name="0f6a" id="0f6a" class="graf graf--p graf-after--p">So now, the model files in S3 make sense. The XML file is the <strong class="markup--strong markup--p-strong">model description</strong> and the BIN file is the <strong class="markup--strong markup--p-strong">model in IR form</strong>.</p><p name="5b4d" id="5b4d" class="graf graf--p graf-after--p">What kind of <strong class="markup--strong markup--p-strong">hardware</strong> is it optimized for ? Let’s look at the hardware for a second.</p><h4 name="8eba" id="8eba" class="graf graf--h4 graf-after--p">Under the hood</h4><p name="2100" id="2100" class="graf graf--p graf-after--h4">The CPU is pretty obvious. It’s a <a href="https://ark.intel.com/products/96486/Intel-Atom-x5-E3930-Processor-2M-Cache-up-to-1_80-GHz" data-href="https://ark.intel.com/products/96486/Intel-Atom-x5-E3930-Processor-2M-Cache-up-to-1_80-GHz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">dual-core Atom E3930</strong></a>.</p><pre name="d596" id="d596" class="graf graf--pre graf-after--p">$ dmesg |grep Intel<br>[    0.108336] smpboot: CPU0: Intel(R) Atom(TM) Processor E3930 @ 1.30GHz (family: 0x6, model: 0x5c, stepping: 0x9)</pre><p name="1349" id="1349" class="graf graf--p graf-after--pre">This baby comes with an <strong class="markup--strong markup--p-strong">Intel HD Graphics 500</strong> chip, so we have a GPU in there too. This one has 12 “execution units” capable of running 7 threads each (SIMD architecture). 84 “cores”, then: not a monster, but surely better than running inference on the Atom itself.</p><p name="bf2c" id="bf2c" class="graf graf--p graf-after--p">Now it’s starting to make sense. The Inference Engine is certainly able to leverage <strong class="markup--strong markup--p-strong">specific instructions on the Atom</strong> (with <a href="https://software.intel.com/en-us/mkl" data-href="https://software.intel.com/en-us/mkl" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Intel MKL</a>, no doubt) as well as the <strong class="markup--strong markup--p-strong">GPU architecture</strong> (with <a href="https://software.intel.com/en-us/iocl_rt_ref" data-href="https://software.intel.com/en-us/iocl_rt_ref" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenCL</a>).</p><p name="df6c" id="df6c" class="graf graf--p graf-after--p">Now what about the model optimizin’ thing?</p><h4 name="fd75" id="fd75" class="graf graf--h4 graf-after--p">Model optimization</h4><p name="d456" id="d456" class="graf graf--p graf-after--h4">Says the Intel doc: “(the model optimizer) <em class="markup--em markup--p-em">performs static model analysis and automatically adjusts deep learning models for optimal execution on end-point target device</em>”.</p><p name="866d" id="866d" class="graf graf--p graf-after--p">OK. It optimizes. Nice job explaining it :-/ Let’s figure it out.</p><p name="dc40" id="dc40" class="graf graf--p graf-after--p">After a bit of installing (and cursing at python), we’re able to run the optimizer.</p><figure name="460a" id="460a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/99b3af914a13048b9096c02f8186fa2c.js"></script></figure><p name="97e4" id="97e4" class="graf graf--p graf-after--figure">Most parameters make sense, but two are a bit intriguing.</p><ul class="postList"><li name="12fd" id="12fd" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">--precision</code> - A precision of the output model. Valid values: <strong class="markup--strong markup--li-strong">FP32</strong> (by default) or <strong class="markup--strong markup--li-strong">FP16</strong>. Depending on the selected precision, weights would be aligned accordingly.</li><li name="edf9" id="edf9" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">--fuse</code> - flag which enables <strong class="markup--strong markup--li-strong">fusion</strong> (combination) of layers to boost topology execution. The idea is to join layers to reduce calculations during inference. Valid values: ON (by default) or OFF.</li></ul><p name="3789" id="3789" class="graf graf--p graf-after--li">OK, this explains the model name we saw earlier.</p><pre name="1dd0" id="1dd0" class="graf graf--pre graf-after--p">mxnet_deploy_ssd_FP16_FUSED.bin</pre><p name="8828" id="8828" class="graf graf--p graf-after--pre">This model uses <strong class="markup--strong markup--p-strong">16-bit floating values for weights </strong>(and probably activation functions too). Obviously, 16-bit arithmetic is both <strong class="markup--strong markup--p-strong">faster</strong> and more <strong class="markup--strong markup--p-strong">energy-efficient</strong> than 32-bit arithmetic, so this makes sense.</p><blockquote name="27fc" id="27fc" class="graf graf--blockquote graf-after--p">As a side note, it’s possible to train MXNet directly with FP16 precision. My gut feeling tells me that this will probably yield more accurate models than training with FP32 and then converting to FP16, but who knows. More information on this <a href="http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" data-href="http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">NVIDIA page</a>.</blockquote><p name="8d38" id="8d38" class="graf graf--p graf-after--blockquote">OK, now let’s run this thing on <strong class="markup--strong markup--p-strong">existing models</strong>. First, we’ll download Inception v3 and VGG-16 from the <a href="https://mxnet.incubator.apache.org/model_zoo/" data-href="https://mxnet.incubator.apache.org/model_zoo/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MXNet model zoo</a>. Then, we’ll convert them.</p><figure name="fad6" id="fad6" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/5e6db2afcb418c5c7a744c5022138107.js"></script></figure><p name="7019" id="7019" class="graf graf--p graf-after--figure">Let’s now compare the original models to their optimized version.</p><figure name="45ed" id="45ed" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/f77d44d90267caa105384e542bfdd207.js"></script></figure><p name="410a" id="410a" class="graf graf--p graf-after--figure">As you can see, moving to <strong class="markup--strong markup--p-strong">FP16 definitely halves model size</strong>. Less storage, less RAM, less compute!</p><p name="8e51" id="8e51" class="graf graf--p graf-after--p">Now there’s only question? Do these models work? Should we give this Inference Engine a spin? Of course we should.</p><h4 name="9fdc" id="9fdc" class="graf graf--h4 graf-after--p">Predicting with IR models</h4><p name="18ab" id="18ab" class="graf graf--p graf-after--h4">The toolkit comes with a bunch of <strong class="markup--strong markup--p-strong">samples</strong>, let’s build them.</p><figure name="a488" id="a488" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/7dd4ee1a9e091e0f1332a18fd3464af5.js"></script></figure><p name="acb4" id="acb4" class="graf graf--p graf-after--figure">Our models are <strong class="markup--strong markup--p-strong">classification models</strong>, so let’s try this.</p><figure name="e00b" id="e00b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/a32a44147ff78deac59edb5d907cb99b.js"></script></figure><p name="96b0" id="96b0" class="graf graf--p graf-after--figure">Let’s grab an image and <strong class="markup--strong markup--p-strong">resize</strong> it to 224x224.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="7216" id="7216" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><img class="graf-image" data-image-id="1*UGUM4o2vffsndAymYFSIew.png" data-width="224" data-height="224" src="https://cdn-images-1.medium.com/max/1200/1*UGUM4o2vffsndAymYFSIew.png"></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="989c" id="989c" class="graf graf--p graf-after--figure">How well do our networks do on this one?</p><blockquote name="c382" id="c382" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">At the moment, I haven’t figured out how to get this code to run on the GPU (but I will). My best guess is that the GPU is already busy running the actual DeepLens stuff. Since I’m not going to delete it, I’m running on the CPU instead with FP32 precision. Grumble grumble.</blockquote><figure name="9ff9" id="9ff9" class="graf graf--figure graf--iframe graf-after--blockquote"><script src="https://gist.github.com/juliensimon/ab3f718050e2f47d06594b99c830086e.js"></script></figure><p name="e87c" id="e87c" class="graf graf--p graf-after--figure">Both networks report <strong class="markup--strong markup--p-strong">category #292</strong> as the top one. Let’s check the <strong class="markup--strong markup--p-strong">ImageNet categories</strong> to find out whether this prediction is correct. Line numbers start at one, so category #292 is on line 293 ;)</p><figure name="a6cc" id="a6cc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0H9YcCX1GN5rzrce9PH05w.png" data-width="838" data-height="206" src="https://cdn-images-1.medium.com/max/800/1*0H9YcCX1GN5rzrce9PH05w.png"></figure><p name="9963" id="9963" class="graf graf--p graf-after--figure">Good call :)</p><p name="4a2d" id="4a2d" class="graf graf--p graf-after--p">So there you go. We answered a lot of questions:</p><ul class="postList"><li name="6b35" id="6b35" class="graf graf--li graf-after--p">We now know that DeepLens is not running MNXet itself, but the <strong class="markup--strong markup--li-strong">Intel Inference Engine</strong> on an optimized model.</li><li name="6ba6" id="6ba6" class="graf graf--li graf-after--li">We know how to convert models using the <strong class="markup--strong markup--li-strong">Model Optimizer</strong>.</li><li name="1b1a" id="1b1a" class="graf graf--li graf-after--li">We know how to run image classification models on the <strong class="markup--strong markup--li-strong">Inference Engine</strong>.</li></ul><p name="192e" id="192e" class="graf graf--p graf-after--li">Next step? How about deploying these tools on a SageMaker instance, training an MXNet model, converting it and deploying it to DeepLens? This should keep me busy during the holidays (#NoLifeTillDeath).</p><p name="20ec" id="20ec" class="graf graf--p graf-after--p graf--trailing">I hope you liked this crazy post. Thanks for reading!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/fcad551886ef"><time class="dt-published" datetime="2017-12-19T18:22:56.937Z">December 19, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/exploring-ahem-aws-deeplens-fcad551886ef" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
