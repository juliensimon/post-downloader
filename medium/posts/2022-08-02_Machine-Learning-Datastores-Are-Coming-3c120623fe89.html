<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Machine Learning Datastores Are Coming</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Machine Learning Datastores Are Coming</h1>
</header>
<section data-field="subtitle" class="p-summary">
They are. And it’s about damn time. Read on!
</section>
<section data-field="body" class="e-content">
<section name="77c3" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="552b" id="552b" class="graf graf--h3 graf--leading graf--title">Machine Learning Datastores Are Coming</h3><p name="2223" id="2223" class="graf graf--p graf-after--h3">They are. And it’s about damn time. Read on!</p><figure name="e92d" id="e92d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*jID0ar5TIArQiL6j.jpg" data-width="1092" data-height="614" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*jID0ar5TIArQiL6j.jpg"><figcaption class="imageCaption"><a href="https://commons.wikimedia.org/wiki/File:Asteroid-Earth.jpg" data-href="https://commons.wikimedia.org/wiki/File:Asteroid-Earth.jpg" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Creative Commons</a></figcaption></figure><h3 name="3c0b" id="3c0b" class="graf graf--h3 graf-after--figure">Relationalaurus Rex</h3><p name="f64b" id="f64b" class="graf graf--p graf-after--h3">Once upon a time, enterprise data was tabular, largely static, and low volume. Relational databases ruled the Earth. IT teams relied on them to ingest and index data that their business applications could query. Applications workflows looked something like this:</p><ol class="postList"><li name="7907" id="7907" class="graf graf--li graf-after--p">A business application receives data through its user interface, or Electronic Data Interchange (EDI).</li><li name="6884" id="6884" class="graf graf--li graf-after--li">Said business application writes that data in a relational database, using a well-defined schema.</li><li name="3ba8" id="3ba8" class="graf graf--li graf-after--li">Other applications retrieve that data with queries written in the ubiquitous Structured Query Language (SQL), either for transactional or analytical purposes.</li><li name="ff0e" id="ff0e" class="graf graf--li graf-after--li">More often than not, applications post-process results with “business rules”, i.e. custom code applying particular business practices, and proceed with doing whatever it is they do.</li></ol><p name="006c" id="006c" class="graf graf--p graf-after--li">Over time, we added caching, connection pools, object-relational mappers (ORM, urgh), and other “middleware” that would supposedly simplify our life. Mostly, that stuff ended up making database vendors richer. Still, life was reasonably good and predictable, and the IT world hummed along.</p><p name="ccdb" id="ccdb" class="graf graf--p graf-after--p">Out of the blue, the user-generated content asteroid hit. A 1,000-feet tsunami swept across IT platforms. Massive, relentless, unstructured, ever-changing. Marketers worldwide went crazy with new monetization ideas based on extracting insights and signals from that content.</p><p name="229a" id="229a" class="graf graf--p graf-after--p">However, clicks, likes, emails, tweets, product reviews, images, and videos broke all existing assumptions. Their diversity and volume made it clear that the traditional way of storing and processing data would quickly cease to function. IT teams rushed to design and implement solutions that could help them cope, using new tools like object storage, Extract-Transform-Load (ETL), NoSQL, Map Reduce, and more.</p><p name="bc8e" id="bc8e" class="graf graf--p graf-after--p">This approach was reasonably successful for analytics on unstructured text data (say, web server logs containing product or banner clicks). Soon enough, we were back in charted waters, where we could write and run SQL-like queries on billions of rows to build nice dashboards and reports.</p><p name="be53" id="be53" class="graf graf--p graf-after--p">That was only a partial victory. Images, videos and speech didn’t easily fit in. And whatever the data modality, delivering real-time insights to business applications was insanely difficult.</p><h3 name="5213" id="5213" class="graf graf--h3 graf-after--p">Enter Machine Learning</h3><p name="053e" id="053e" class="graf graf--p graf-after--h3">Machine Learning (ML) lets us replace a big pile of data with a much-smaller statistical model, making it much faster to get answers. It also grants us the gift of second sight: we can now gaze into the crystal ball and predict in milliseconds that our loyal customer Bob wants to buy a new barbecue grill, so let’s recommend him that.</p><p name="03a0" id="03a0" class="graf graf--p graf-after--p">Thanks to Deep Learning (DL), the uncanny child of neural networks and Graphics Processing Units (GPU), we can extend that predictive power to pretty much any form of unstructured data: images, videos, audio, speech, protein sequences, and more.</p><p name="7ea0" id="7ea0" class="graf graf--p graf-after--p">ML and DL are truly amazing, but how do they impact the way we design and build business applications? Quite a lot, as we now have to build models! A typical ML training workflow goes something like this:</p><ol class="postList"><li name="4f4c" id="4f4c" class="graf graf--li graf-after--p">Ingest user and company data from different sources: relational databases, web server logs, social networks, anything goes!</li><li name="8af0" id="8af0" class="graf graf--li graf-after--li">Clean it, organize it and dump it all in a “data lake” (ooooooh).</li><li name="852c" id="852c" class="graf graf--li graf-after--li">Send your bright data scientists to sail on that lake in search of a new Eldorado, hoping that they come back with a full cargo of high-value data they can solve real-life business problems with.</li><li name="46ce" id="46ce" class="graf graf--li graf-after--li">Train models on that data.</li><li name="49e6" id="49e6" class="graf graf--li graf-after--li">Deploy these models.</li></ol><p name="0a2c" id="0a2c" class="graf graf--p graf-after--li">Or course, the next step is to let applications predict with these models, either in real-time or in batch mode. Let’s start with the former:</p><ol class="postList"><li name="9500" id="9500" class="graf graf--li graf-after--p">An application receives data through a user interface or an API and logs it for further use (backtesting, training, etc.).</li><li name="c3f4" id="c3f4" class="graf graf--li graf-after--li">It reads additional data from a low-latency datastore: user preferences, extra features required for prediction, etc.</li><li name="d565" id="d565" class="graf graf--li graf-after--li">It pre-processes the data and sends it for prediction to one or more models.</li><li name="7644" id="7644" class="graf graf--li graf-after--li">It post-processes results, writes them in a datastore, and displays (or sends) them to the caller.</li><li name="bde9" id="bde9" class="graf graf--li graf-after--li">If available, it captures and stores user feedback for further use (“was this useful?”, “did this answer your question?”, etc.).</li></ol><p name="946e" id="946e" class="graf graf--p graf-after--li">Batch prediction is a bit different:</p><ol class="postList"><li name="8891" id="8891" class="graf graf--li graf-after--p">A scheduler starts a batch processing job.</li><li name="b584" id="b584" class="graf graf--li graf-after--li">The job reads lots of data from one or more datastores. It could be as simple as pulling rows from a single relational database or as complex as an ETL process loading and joining data from different datastores.</li><li name="30a8" id="30a8" class="graf graf--li graf-after--li">The job predicts each item with one or more models.</li><li name="be9b" id="be9b" class="graf graf--li graf-after--li">The job writes the results in a datastore.</li><li name="dec8" id="dec8" class="graf graf--li graf-after--li">Business applications read results and use them in their workflows.</li></ol><p name="a3dc" id="a3dc" class="graf graf--p graf-after--li">That’s where we are today. This new way of building applications is spawning a new industry, which we decided to call MLOps: data science platforms, feature stores, training and deployment services, orchestration tools, etc. Indeed, from data preparation to training and predicting, workflows are getting increasingly complex, with many moving parts and way too much IT plumbing. Meanwhile, vendors make money, and Lamborghini dealerships rejoice. Same old same old.</p><p name="7960" id="7960" class="graf graf--p graf-after--p">For all their greatness, ML and MLOps are already becoming too complicated. Can we pause for a minute, step back and consider how we could simplify things again?</p><p name="581c" id="581c" class="graf graf--p graf-after--p">Here are a few whacky (?) ideas on how adding ML capabilities to datastores could improve development experience and agility. By datastores, I mean any place we store data today: relational or NoSQL databases, data lakes, object stores, etc. All of these should have a role to play.</p><p name="7d17" id="7d17" class="graf graf--p graf-after--p">If anything below turns out to be a billion-dollar idea, please consider sending me a t-shirt. Or a Lamborghini. Much obliged!</p><h3 name="846c" id="846c" class="graf graf--h3 graf-after--p">1 — Data? We have lots! Which one do you need?</h3><p name="68eb" id="68eb" class="graf graf--p graf-after--h3">In the pre-ML world, relational databases were the single source of truth. You knew where to go to grab the original copy of your data. Transactions guaranteed integrity and traceability.</p><p name="e326" id="e326" class="graf graf--p graf-after--p">The situation is much fuzzier now. The same data lives in different places and formats. Data stored in a SQL table could be exported to a data lake for staging, then processed by an ETL system and stored in the data lake again, potentially in different formats to account for different downstream workflows. There, it could be picked up by an ML team and joined to a dataset. Feature selection, feature engineering, and data augmentation would take place. Maybe the data would then be pushed into a feature store. At training time, data would be shuffled, sampled, split, and stored on the training infrastructure.</p><p name="3647" id="3647" class="graf graf--p graf-after--p">After all this, good luck tracing a particular feature in your training dataset back to its original row in your relational database, which may have been updated or deleted since then…With an amazing data engineering team, you can pull it off and keep everything neatly organized and versioned in your high-gloss data lake. That still sounds like a ton of work, and the further training data lives from its source, the more likely it is that something will go wrong.</p><p name="6332" id="6332" class="graf graf--p graf-after--p">How about ML-native datastores that could automatically build engineered data in place, and keep track of versioning and lineage (inserts, updates, deletes)?</p><p name="ca4e" id="ca4e" class="graf graf--p graf-after--p">I’d love to be able to attach a Python processing script to a table (or equivalent) and let the datastore automatically build and manage the corresponding engineered data. I could configure when to run the code: periodically, when a certain percentage of the raw data has changed, or on demand. Then, I could access the engineered data in a view (or equivalent), alongside versioning and lineage metadata. To further simplify the data preparation process, the datastore could implement built-in transforms (say normalization, imputation, tokenization, etc.) that I could run automatically without writing any code.</p><blockquote name="64b6" id="64b6" class="graf graf--blockquote graf-after--p">While I was literally writing this post, <a href="https://featurebyte.com/" data-href="https://featurebyte.com/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">FeatureByte</a> launched :) You can read more on <a href="https://venturebeat.com/2022/07/28/featurebyte-launched-by-datarobot-vets-to-advance-ai-feature-engineering/" data-href="https://venturebeat.com/2022/07/28/featurebyte-launched-by-datarobot-vets-to-advance-ai-feature-engineering/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">VentureBeat</a>. Good timing!</blockquote><p name="e3af" id="e3af" class="graf graf--p graf-after--blockquote">Benefits:</p><ul class="postList"><li name="5c14" id="5c14" class="graf graf--li graf-after--p">Raw data and engineered data in the same place.</li><li name="f8ce" id="f8ce" class="graf graf--li graf-after--li">Built-in lineage and versioning.</li><li name="8a3a" id="8a3a" class="graf graf--li graf-after--li">Simpler ETL.</li><li name="0346" id="0346" class="graf graf--li graf-after--li">Less data movement.</li></ul><h3 name="aa65" id="aa65" class="graf graf--h3 graf-after--li">2 — Want your predictions for here or to go?</h3><p name="d030" id="d030" class="graf graf--p graf-after--h3">Today, ML predictions require dedicated code and infrastructure. Typically, an application reads data and sends it to a prediction service, which runs more code to invoke a model and send results back to the application. This requires a bit of data movement, IT plumbing, and boilerplate code that we should try to eliminate.</p><p name="f527" id="f527" class="graf graf--p graf-after--p">I can see a lot of use cases where predictions could be run directly inside the datastore when data is added/updated and used to automatically populate new attributes/metadata. Here are some examples.</p><p name="7def" id="7def" class="graf graf--p graf-after--p">Product catalogs:</p><ul class="postList"><li name="7584" id="7584" class="graf graf--li graf-after--p">Translate product descriptions automatically to account for multi-lingual websites and applications.</li><li name="1f25" id="1f25" class="graf graf--li graf-after--li">Classify images to extract labels to improve search and quality assurance (does the picture show the correct object?)</li></ul><p name="b05e" id="b05e" class="graf graf--p graf-after--li">Voice of the customer (support emails, online reviews, social media, etc.):</p><ul class="postList"><li name="ea56" id="ea56" class="graf graf--li graf-after--p">Extract entities to understand what product or service the customer is mentioning.</li><li name="4db7" id="4db7" class="graf graf--li graf-after--li">Analyze sentiment and emotion.</li></ul><p name="15c5" id="15c5" class="graf graf--p graf-after--li">Knowledge bases:</p><ul class="postList"><li name="d3d3" id="d3d3" class="graf graf--li graf-after--p">Extract entities (company names, product names, etc.) and classify documents to improve search with reliable metadata.</li><li name="d716" id="d716" class="graf graf--li graf-after--li">And also: translation, summarization, etc.</li></ul><p name="b549" id="b549" class="graf graf--p graf-after--li">Semantic search:</p><ul class="postList"><li name="24db" id="24db" class="graf graf--li graf-after--p">Extract features from text documents (I’m looking at you, vector databases).</li><li name="83d2" id="83d2" class="graf graf--li graf-after--li">Generate image-to-text descriptions.</li><li name="b417" id="b417" class="graf graf--li graf-after--li">Transcribe audio with speech-to-text.</li></ul><p name="a84a" id="a84a" class="graf graf--p graf-after--li">The list goes on. You can probably do this today with User-Defined Functions (UDF) and external APIs, but that involves additional code, IT plumbing, and data movement. Meh.</p><p name="d32e" id="d32e" class="graf graf--p graf-after--p">Instead, I’d love to be able to attach metadata to a particular column/attribute/object describing the model to use, when to run prediction (synchronously, asynchronously, scheduled), and where to store results. The model would be fetched automatically from a model repository, loaded in a container, and run inside the datastore.</p><p name="801a" id="801a" class="graf graf--p graf-after--p">At prediction time, the datastore automatically converts the data to the appropriate inference format and invokes the model. Predictions would be stored in the same place as the original data, and I could retrieve it using whatever query language the datastore supports. Why does it have to be more complicated than this?</p><p name="ba8e" id="ba8e" class="graf graf--p graf-after--p">Benefits:</p><ul class="postList"><li name="9e23" id="9e23" class="graf graf--li graf-after--p">Raw data and predictions data in the same place.</li><li name="d021" id="d021" class="graf graf--li graf-after--li">No data movement.</li><li name="ec50" id="ec50" class="graf graf--li graf-after--li">Less plumbing.</li><li name="f256" id="f256" class="graf graf--li graf-after--li">Simpler application code.</li></ul><h3 name="ad7d" id="ad7d" class="graf graf--h3 graf-after--li">3 — You say tomato, I say tomato</h3><p name="05ef" id="05ef" class="graf graf--p graf-after--h3">ML algorithms expect training data in a well-defined technical format: CSV, libsvm, JSON, protobuf, TFRecord, Parquet, you name it. Most of the time, datastores cannot provide data in these formats. This impedance mismatch forces us to write conversion code, which we should eliminate.</p><p name="de4e" id="de4e" class="graf graf--p graf-after--p">I’d love to pull data in the exact format that an algorithm expects. datastores provide seamless export options for the most popular ML formats. A good example is the little-known <a href="https://github.com/aws/sagemaker-spark" data-href="https://github.com/aws/sagemaker-spark" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker Spark SDK</a>, which automatically converts training data from DataFrame to protobuf (the format that many SageMaker built-in algorithms expect) and uploads it to S3. Perfect.</p><p name="9499" id="9499" class="graf graf--p graf-after--p">For more flexibility, I should be able to run custom code for feature engineering, feature selection, and advanced formatting. The place to run this is in the datastore, and nowhere else. Snowflake’s <a href="https://docs.snowflake.com/en/developer-guide/snowpark/index.html" data-href="https://docs.snowflake.com/en/developer-guide/snowpark/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Snowpark</a> is an interesting step in that direction.</p><p name="c416" id="c416" class="graf graf--p graf-after--p">Benefits:</p><ul class="postList"><li name="b55e" id="b55e" class="graf graf--li graf-after--p">One-click/one-line export to the appropriate format.</li><li name="29ff" id="29ff" class="graf graf--li graf-after--li">Simpler ETL.</li><li name="1155" id="1155" class="graf graf--li graf-after--li">No data movement.</li></ul><h3 name="3aae" id="3aae" class="graf graf--h3 graf-after--li">4 — Closing the loop</h3><p name="a033" id="a033" class="graf graf--p graf-after--h3">If a datastore is storing pre-processed data and can export it to the right training format, it’s a step away from being able to train models. So why shouldn’t it?</p><p name="aa66" id="aa66" class="graf graf--p graf-after--p">Many enterprise ML projects use traditional algorithms like linear regression and classification (scikit-learn, XGBoost, etc.). Most of the time, training will amount to minutes on a CPU. These jobs are screaming for commoditization.</p><p name="47bd" id="47bd" class="graf graf--p graf-after--p">I’d love to be able to run training in place:</p><ul class="postList"><li name="82b7" id="82b7" class="graf graf--li graf-after--p">Write a query to build the training set,</li><li name="5d93" id="5d93" class="graf graf--li graf-after--li">Export it to the appropriate training format,</li><li name="349a" id="349a" class="graf graf--li graf-after--li">Pick an algorithm and set hyperparameters, or better yet, use AutoML,</li><li name="a5af" id="a5af" class="graf graf--li graf-after--li">Train inside the datastore (and deploy in place!).</li></ul><p name="18e3" id="18e3" class="graf graf--p graf-after--li">Retraining could be configured automatically on a schedule or when a certain percentage of data has been added or updated.</p><p name="e48f" id="e48f" class="graf graf--p graf-after--p">Could we do the same for Deep Learning models? With pre-trained Transformers and few-shot learning, the prediction cost for unstructured data could be reasonable enough to run in place. For tabular data, we could also consider using a TabTransformer model pre-trained on a big pile of unlabeled tabular data and fine-tune it on a bit of our labeled data (this nice <a href="https://blog.ml6.eu/how-a-pretrained-tabtransformer-performs-in-the-real-world-eccb12362950" data-href="https://blog.ml6.eu/how-a-pretrained-tabtransformer-performs-in-the-real-world-eccb12362950" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">blog post</a> explains the whole process). The compute cost would be higher, but who said we couldn’t have hardware accelerators in datastore systems?</p><p name="00d5" id="00d5" class="graf graf--p graf-after--p">Benefits:</p><ul class="postList"><li name="ac69" id="ac69" class="graf graf--li graf-after--p">Simpler workflow.</li><li name="7745" id="7745" class="graf graf--li graf-after--li">Fresher models.</li><li name="4074" id="4074" class="graf graf--li graf-after--li">No data movement.</li></ul><h3 name="7a34" id="7a34" class="graf graf--h3 graf-after--li">Conclusion</h3><p name="e5a1" id="e5a1" class="graf graf--p graf-after--h3">There you have it. Everybody is claiming to simplify ML, but it’s never been so complicated… Hopefully, we’re at the peak or close to it.</p><p name="1238" id="1238" class="graf graf--p graf-after--p">For everyone’s sake, we need to start simplifying, standardizing, and commoditizing workflows. My crystal ball tells me that datastores have a significant role to play. That next wave of innovation hasn’t really started,</p><p name="ee83" id="ee83" class="graf graf--p graf-after--p graf--trailing">Watch the skies. That asteroid is coming.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/3c120623fe89"><time class="dt-published" datetime="2022-08-02T09:10:17.778Z">August 2, 2022</time></a>.</p><p><a href="https://medium.com/@julsimon/machine-learning-datastores-are-coming-3c120623fe89" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>