<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>3 new videos: Intel Sapphire Rapids inference, few-shot learning with SetFit, and semantic search…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">3 new videos: Intel Sapphire Rapids inference, few-shot learning with SetFit, and semantic search…</h1>
</header>
<section data-field="subtitle" class="p-summary">
Not one, but three new Hugging Face videos today :) Learn and share!
</section>
<section data-field="body" class="e-content">
<section name="15a8" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1ce1" id="1ce1" class="graf graf--h3 graf--leading graf--title">3 new videos: Intel Sapphire Rapids inference, few-shot learning with SetFit, and semantic search on images and videos with BridgeTower</h3><p name="b30b" id="b30b" class="graf graf--p graf-after--h3">Not one, but three new <a href="https://www.linkedin.com/feed/#" data-href="https://www.linkedin.com/feed/#" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face</a> videos today :) Learn and share!</p><figure name="ab64" id="ab64" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/-q9rmF6fK_w?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="e1f6" id="e1f6" class="graf graf--p graf-after--figure graf--trailing">In this video, you will learn how to accelerate PyTorch inference with an Intel Sapphire Rapids server running on AWS. Working with popular Hugging Face transformers implemented with PyTorch, we’ll first measure their performance on an Ice Lake server for short and long NLP token sequences. Then, we’ll do the same with a Sapphire Rapids server and the latest version of Hugging Face Optimum Intel, an open-source library dedicated to hardware acceleration for Intel platforms.</p></div></div></section><section name="4294" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="bd57" id="bd57" class="graf graf--figure graf--iframe graf--leading"><iframe src="https://www.youtube.com/embed/3uOXG01mcUQ" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="9db9" id="9db9" class="graf graf--p graf-after--figure graf--trailing">In this video, I’m using SetFit, an open source library for few-shot learning to train a text classification model from only 16 labeled samples. In less than 2.5 minutes of CPU training, I get an accuracy of 94.5% :)</p></div></div></section><section name="5a02" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="d212" id="d212" class="graf graf--figure graf--iframe graf--leading"><iframe src="https://www.youtube.com/embed/1Qzkzk3iQFw?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="1fc4" id="1fc4" class="graf graf--p graf-after--figure graf--trailing">BridgeTower is a new vision-language multimodal model by Intel and Microsoft. In this video, I show you two quick demos for semantic search on images and videos.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/e3f3cc9e1f6b"><time class="dt-published" datetime="2023-02-07T13:22:29.129Z">February 7, 2023</time></a>.</p><p><a href="https://medium.com/@julsimon/3-new-videos-intel-sapphire-rapids-inference-few-shot-learning-with-setfit-and-semantic-search-e3f3cc9e1f6b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
