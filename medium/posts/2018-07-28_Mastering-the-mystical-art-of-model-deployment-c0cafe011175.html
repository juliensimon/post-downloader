<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Mastering the mystical art of model deployment</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Mastering the mystical art of model deployment</h1>
</header>
<section data-field="subtitle" class="p-summary">
With all the talk about algorithm selection, hyper parameter optimization and so on, you could think that training models is the hardest…
</section>
<section data-field="body" class="e-content">
<section name="13d9" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ab79" id="ab79" class="graf graf--h3 graf--leading graf--title">Mastering the mystical art of model deployment</h3><p name="0dc8" id="0dc8" class="graf graf--p graf-after--h3">With all the talk about algorithm selection, hyper parameter optimization and so on, you could think that training models is the hardest part of the Machine Learning process. However, in my experience, the really tricky step is to deploy these models safely in a web production environment.</p><p name="d74d" id="d74d" class="graf graf--p graf-after--p">In this post, I’ll first talk about the typical tasks required to <strong class="markup--strong markup--p-strong">deploy and validate</strong> models in production. Then, I’ll present several <strong class="markup--strong markup--p-strong">model deployment techniques </strong>and how to implement them with <a href="http://aws.amazon.com/sagemaker" data-href="http://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a>. In particular, I’ll show you in detail how to <strong class="markup--strong markup--p-strong">host multiple models on the same prediction endpoint</strong>, an important technique to minimize deployment risks.</p><figure name="fce0" id="fce0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AZHhCobmks9vT2vWYN0kww.gif" data-width="500" data-height="282" src="https://cdn-images-1.medium.com/max/800/1*AZHhCobmks9vT2vWYN0kww.gif"><figcaption class="imageCaption">That guy played Alan Turing and Dr Strange. ‘nuff said.</figcaption></figure><h3 name="ac5e" id="ac5e" class="graf graf--h3 graf-after--figure">Validating a model</h3><p name="89ae" id="89ae" class="graf graf--p graf-after--h3">Even if you’ve carefully trained and evaluated a model in your Data Science sandbox, additional work is required to check that it will work correctly in your production environment. This usually involve tasks like:</p><ul class="postList"><li name="37b9" id="37b9" class="graf graf--li graf-after--p">setting up a <strong class="markup--strong markup--li-strong">monitoring system</strong> to store and visualize model metrics,</li><li name="002a" id="002a" class="graf graf--li graf-after--li">building a <strong class="markup--strong markup--li-strong">test web application</strong> bundling the model and running <strong class="markup--strong markup--li-strong">technical tests</strong> (is the model fast? how much RAM does it require? etc.) as well as <strong class="markup--strong markup--li-strong">prediction tests</strong> (is my model still predicting as expected?).</li><li name="4bd2" id="4bd2" class="graf graf--li graf-after--li">integrating the model with your <strong class="markup--strong markup--li-strong">business application</strong> and running end to end tests,</li><li name="d002" id="d002" class="graf graf--li graf-after--li">deploying the application in production using techniques like <strong class="markup--strong markup--li-strong">blue-green deployment</strong> or <strong class="markup--strong markup--li-strong">canary testing</strong> (more on this in a minute),</li><li name="6f7e" id="6f7e" class="graf graf--li graf-after--li">running <strong class="markup--strong markup--li-strong">different versions of the same model in parallel</strong> for longer periods of time, in order to measure their long-term effectiveness with respect to business metrics (aka <strong class="markup--strong markup--li-strong">A/B testing</strong>).</li></ul><p name="a229" id="a229" class="graf graf--p graf-after--li">Quite a bit of work, then. Let’s first look at the different ways we could deploy models.</p><h3 name="84a1" id="84a1" class="graf graf--h3 graf-after--p">Deployment options</h3><h4 name="af33" id="af33" class="graf graf--h4 graf-after--h3">Standard deployment</h4><p name="b421" id="b421" class="graf graf--p graf-after--h4">In its simplest form, deploying a model usually involves building a bespoke <strong class="markup--strong markup--p-strong">web application hosting your model</strong> and receiving prediction requests. Testing is what you would expect: sending HTTP requests, checking logs and checking metrics.</p><p name="032f" id="032f" class="graf graf--p graf-after--p">SageMaker <strong class="markup--strong markup--p-strong">greatly</strong> simplifies this process. With just a few lines of code, the <a href="http://sagemaker.readthedocs.io/en/latest/estimators.html" data-href="http://sagemaker.readthedocs.io/en/latest/estimators.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Estimator</a> object in the <a href="https://github.com/aws/sagemaker-python-sdk" data-href="https://github.com/aws/sagemaker-python-sdk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker SDK</a> (or its subclasses for built-in algos, TensorFlow, etc.) lets you deploy a model to an HTTPS <strong class="markup--strong markup--p-strong">endpoint</strong> and run prediction tests. No need to write any app. In addition, technical metrics are available out of the box in <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CloudWatch</a>.</p><blockquote name="c50e" id="c50e" class="graf graf--blockquote graf-after--p">If you’re interested in load testing and sizing endpoints, this nice <a href="https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/" data-href="https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">AWS blog post</a> will show you how to do it.</blockquote><p name="a4b3" id="a4b3" class="graf graf--p graf-after--blockquote">I won’t dwell on this: I’ve covered it several times in previous posts and you’ll also find plenty of examples in the SageMaker <a href="https://github.com/awslabs/amazon-sagemaker-examples$" data-href="https://github.com/awslabs/amazon-sagemaker-examples$" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">notebook collection</a>.</p><h4 name="1303" id="1303" class="graf graf--h4 graf-after--p">Blue-green deployment</h4><p name="033a" id="033a" class="graf graf--p graf-after--h4">This proven <a href="https://martinfowler.com/bliki/BlueGreenDeployment.html" data-href="https://martinfowler.com/bliki/BlueGreenDeployment.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">deployment technique</a> requires <strong class="markup--strong markup--p-strong">two identical environments</strong>:</p><ul class="postList"><li name="3c3e" id="3c3e" class="graf graf--li graf-after--p">the live production environment (“blue”) running <strong class="markup--strong markup--li-strong">version n</strong>,</li><li name="d63c" id="d63c" class="graf graf--li graf-after--li">an exact copy of this environment (“green”) running <strong class="markup--strong markup--li-strong">version n+1</strong>.</li></ul><p name="a6f3" id="a6f3" class="graf graf--p graf-after--li">First, you <strong class="markup--strong markup--p-strong">run tests</strong> on the green environment, <strong class="markup--strong markup--p-strong">monitor</strong> technical and business metrics and check that everything is correct. If it is, you can then <strong class="markup--strong markup--p-strong">switch traffic</strong> to the green environment… and check again. If something goes wrong, you can immediately <strong class="markup--strong markup--p-strong">switch back</strong> to the blue environment and investigate. If everything is fine, you can <strong class="markup--strong markup--p-strong">delete</strong> the blue environment.</p><p name="ed57" id="ed57" class="graf graf--p graf-after--p">To make this process completely transparent to client applications, a middleman — located between the clients and the environments — is in charge of <strong class="markup--strong markup--p-strong">implementing the switch</strong>: popular choices include load balancers, DNS, etc. This is what it looks like.</p><figure name="d9f8" id="d9f8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WGLxrDadcyWmSzC608wKhg.png" data-width="1496" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*WGLxrDadcyWmSzC608wKhg.png"><figcaption class="imageCaption">Blue-green deployment</figcaption></figure><h4 name="2829" id="2829" class="graf graf--h4 graf-after--figure">Blue-green deployment, the SageMaker way</h4><p name="48b9" id="48b9" class="graf graf--p graf-after--h4">The <a href="https://boto3.readthedocs.io/en/latest/reference/services/sagemaker.html" data-href="https://boto3.readthedocs.io/en/latest/reference/services/sagemaker.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS SDK for SageMaker</a> provides the middleman that we need in the form of the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">endpoint configuration</strong></a>. This resource lets us attach <strong class="markup--strong markup--p-strong">several models</strong> to the same endpoint, with different weights and different instance configurations (aka <strong class="markup--strong markup--p-strong">production variants</strong>). The setup may be updated at any time during the life of the endpoint.</p><p name="cdaf" id="cdaf" class="graf graf--p graf-after--p">In fact, one could picture an endpoint as a special type of load balancer, using weighted round robin to send prediction requests to instance pools hosting different models. Here’s the information required to set one up with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">CreateEndpointConfig</em></a> API.</p><pre name="d27c" id="d27c" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0">&quot;<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html#SageMaker-CreateEndpointConfig-request-ProductionVariants" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html#SageMaker-CreateEndpointConfig-request-ProductionVariants" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">ProductionVariants</a>&quot;: [ <br>      { <br>         &quot;<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InitialInstanceCount" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InitialInstanceCount" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">InitialInstanceCount</a>&quot;: <em class="markup--em markup--pre-em">number</em>,<br>         &quot;<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InitialVariantWeight" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InitialVariantWeight" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">InitialVariantWeight</a>&quot;: <em class="markup--em markup--pre-em">number</em>,<br>         &quot;<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InstanceType" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InstanceType" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">InstanceType</a>&quot;: &quot;<em class="markup--em markup--pre-em">string</em>&quot;,<br>         &quot;<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-ModelName" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-ModelName" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">ModelName</a>&quot;: &quot;<em class="markup--em markup--pre-em">string</em>&quot;,<br>         &quot;<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-VariantName" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-VariantName" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">VariantName</a>&quot;: &quot;<em class="markup--em markup--pre-em">string</em>&quot;<br>      }<br>   ]</code></pre><p name="3e43" id="3e43" class="graf graf--p graf-after--pre">Implementing blue-green deployment now goes like this:</p><ul class="postList"><li name="e587" id="e587" class="graf graf--li graf-after--p">create a <strong class="markup--strong markup--li-strong">new endpoint configuration</strong>, holding the <strong class="markup--strong markup--li-strong">production variants</strong> for the existing live model and for the new model.</li><li name="5a7a" id="5a7a" class="graf graf--li graf-after--li">update the <strong class="markup--strong markup--li-strong">existing live endpoint</strong> with the new endpoint configuration (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">UpdateEndpoint</em></a> API): SageMaker creates the required infrastructure for the new production variant and update weights <strong class="markup--strong markup--li-strong">without any downtime.</strong></li><li name="004f" id="004f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">switch traffic</strong> to the new model (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">UpdateEndpointWeightAndCapacities</em></a> API),</li><li name="b383" id="b383" class="graf graf--li graf-after--li">create a new endpoint configuration holding <strong class="markup--strong markup--li-strong">only the new production variant</strong> and apply it to the endpoint: SageMaker terminates the infrastructure for the previous production variant.</li></ul><p name="9946" id="9946" class="graf graf--p graf-after--li">This is what it looks like.</p><figure name="c17e" id="c17e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OmCCeJXV7KD5R5LC7qfQUQ.png" data-width="1506" data-height="618" src="https://cdn-images-1.medium.com/max/800/1*OmCCeJXV7KD5R5LC7qfQUQ.png"><figcaption class="imageCaption">Blue-green deployment with a single SageMaker endpoint</figcaption></figure><h4 name="4cdd" id="4cdd" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Canary testing</strong></h4><p name="20a4" id="20a4" class="graf graf--p graf-after--h4">Canary testing lets you validate a new release with <strong class="markup--strong markup--p-strong">minimal risk</strong> by deploying it first for a <strong class="markup--strong markup--p-strong">fraction of your users</strong>: everyone else keeps using the previous version. This user split can be done in many ways: random, geolocation, specific user lists, etc. Once you’re satisfied with the release, you can gradually roll it out to <strong class="markup--strong markup--p-strong">all users</strong>.</p><p name="ae08" id="ae08" class="graf graf--p graf-after--p">This requires “<strong class="markup--strong markup--p-strong">stickiness</strong>”: for the duration of the test, designated users must be routed to servers running the <strong class="markup--strong markup--p-strong">new release</strong>. This could be achieved by setting a specific cookie for these users, allowing the web application to identify them and send their traffic to the proper servers.</p><p name="858f" id="858f" class="graf graf--p graf-after--p">You could implement this logic either in the application itself or in a dedicated web service. The latter would be in charge of receiving prediction requests and invoking the appropriate endpoint.</p><p name="e800" id="e800" class="graf graf--p graf-after--p">This feels like extra work, but chances are you’ll need a web service anyway for data pre-processing (normalization, injecting extra data in the prediction request, etc.) and post-processing (filtering prediction results, logging, etc.). Lambda feels like a good way to do this: easy to deploy, easy to scale, built-in high-availability, etc.: here’s an <a href="https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033" data-href="https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033" class="markup--anchor markup--p-anchor" target="_blank">example</a> implemented with AWS Chalice.</p><p name="2599" id="2599" class="graf graf--p graf-after--p">This what it would look like with two endpoints.</p><figure name="4c1a" id="4c1a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xCtJrB5KeBb9akLDRCjoug.png" data-width="1210" data-height="614" src="https://cdn-images-1.medium.com/max/800/1*xCtJrB5KeBb9akLDRCjoug.png"><figcaption class="imageCaption">Using a “switch” web service and two single-model endpoints for canary testing.</figcaption></figure><p name="05d5" id="05d5" class="graf graf--p graf-after--figure">Once we’re happy that the new model works, we can gradually roll it out to all users, scaling endpoints up and down accordingly.</p><figure name="92c9" id="92c9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*y896vuwRBZI4ccJOVCU4DA.png" data-width="1260" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*y896vuwRBZI4ccJOVCU4DA.png"><figcaption class="imageCaption">Gradually switching all users to the new models.</figcaption></figure><h4 name="5eef" id="5eef" class="graf graf--h4 graf-after--figure">A/B testing</h4><p name="197f" id="197f" class="graf graf--p graf-after--h4">A/B testing is about <strong class="markup--strong markup--p-strong">comparing the performance of different versions of the same feature</strong> while monitoring a high-level <strong class="markup--strong markup--p-strong">metric</strong> (e.g. click-through rate, conversion rate, etc.). In this context, this would mean predicting with different models for different users and analysing results.</p><p name="b0d1" id="b0d1" class="graf graf--p graf-after--p">Technically speaking, A/B testing is similar to canary testing with <strong class="markup--strong markup--p-strong">larger user groups</strong> and a <strong class="markup--strong markup--p-strong">longer time-scale</strong> (days or even weeks). Stickiness is essential and the technique mentioned above would certainly work: building user buckets, sticking them to different endpoints and logging results.</p><p name="c1d8" id="c1d8" class="graf graf--p graf-after--p">As you can see, the ability to deploy multiple models to the same endpoint is an important requirement for validation and testing. Let’s see how this works.</p><h3 name="9844" id="9844" class="graf graf--h3 graf-after--p">Deploying multiple models to the same endpoint</h3><p name="9ada" id="9ada" class="graf graf--p graf-after--h3">Imagine we’d like to compare different models trained with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">built-in algorithm for image classification</a> using different hyper-parameters.</p><p name="c5d5" id="c5d5" class="graf graf--p graf-after--p">These are the steps we need to take (<a href="https://gitlab.com/juliensimon/dlnotebooks/blob/master/sagemaker/05-Image-classification-two-models.ipynb" data-href="https://gitlab.com/juliensimon/dlnotebooks/blob/master/sagemaker/05-Image-classification-two-models.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">full notebook</a> available on Gitlab):</p><ol class="postList"><li name="bbd2" id="bbd2" class="graf graf--li graf-after--p">train model A.</li><li name="97f5" id="97f5" class="graf graf--li graf-after--li">train model B.</li><li name="046c" id="046c" class="graf graf--li graf-after--li">create models A and B, i.e. registering them in SageMaker.</li><li name="87d1" id="87d1" class="graf graf--li graf-after--li">create an endpoint configuration with the two production variants.</li><li name="cd50" id="cd50" class="graf graf--li graf-after--li">create the endpoint.</li><li name="e128" id="e128" class="graf graf--li graf-after--li">send traffic and look at CloudWatch metrics.</li></ol><p name="d5b7" id="d5b7" class="graf graf--p graf-after--li">We’ve trained this algo in a <a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" data-href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" class="markup--anchor markup--p-anchor" target="_blank">previous post</a>, so I won’t go into details: in a nutshell, we’re simply training two models with different learning rates.</p><h4 name="9e80" id="9e80" class="graf graf--h4 graf-after--p">Creating the endpoint configuration</h4><p name="4f99" id="4f99" class="graf graf--p graf-after--h4">This is where we define our <strong class="markup--strong markup--p-strong">two production variants</strong>: one for model A and one for model B. To begin with, we’ll assign them <strong class="markup--strong markup--p-strong">equal weights</strong>, in order to balance traffic 50/50. We’ll also use <strong class="markup--strong markup--p-strong">identical instance configurations</strong>.</p><figure name="f3ba" id="f3ba" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/9d571ed98862c038e91d5f4a0e1cbcbc.js"></script></figure><h4 name="8a32" id="8a32" class="graf graf--h4 graf-after--figure">Creating the endpoint</h4><p name="6005" id="6005" class="graf graf--p graf-after--h4">Pretty straightforward: all it takes is calling the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">CreateEndpoint</em></a> API, which builds all infrastructure required to support the production variants defined in the endpoint configuration.</p><figure name="c24e" id="c24e" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/juliensimon/75b72a45ce5a219c34cbcb7e46ddbd79.js"></script></figure><p name="1fe4" id="1fe4" class="graf graf--p graf-after--figure">After a few minutes, we can see the endpoint settings in the SageMaker console.</p><figure name="6735" id="6735" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EF19xYbvaunDGh9AAtshfg.png" data-width="1788" data-height="414" src="https://cdn-images-1.medium.com/max/800/1*EF19xYbvaunDGh9AAtshfg.png"></figure><h4 name="df5b" id="df5b" class="graf graf--h4 graf-after--figure">Monitoring traffic</h4><p name="f6fc" id="f6fc" class="graf graf--p graf-after--h4">Let’s send some traffic and monitor the endpoint in <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CloudWatch</a>. After a few more minutes, we can see that traffic is nicely balanced between the two production variants.</p><figure name="b067" id="b067" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*6TsltmOvd5GC2i1Q61FTVw.png" data-width="714" data-height="426" src="https://cdn-images-1.medium.com/max/800/1*6TsltmOvd5GC2i1Q61FTVw.png"></figure><p name="4e13" id="4e13" class="graf graf--p graf-after--figure">Let’s update the weights in the AWS console: model A now gets <strong class="markup--strong markup--p-strong">10%</strong> of traffic and model B gets <strong class="markup--strong markup--p-strong">90%</strong>. As mentioned above, you could also do this programmaticall with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">UpdateEndpointWeightAndCapacities</em></a> API.</p><figure name="659d" id="659d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tCdaLSHUQ1kK5OJXeKhxsg.png" data-width="1774" data-height="394" src="https://cdn-images-1.medium.com/max/800/1*tCdaLSHUQ1kK5OJXeKhxsg.png"></figure><p name="3172" id="3172" class="graf graf--p graf-after--figure">Almost immediately, we see most of the traffic now going model B.</p><figure name="9a9a" id="9a9a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*uZFUNAgGmn22hKrjJTkmFQ.png" data-width="1036" data-height="392" src="https://cdn-images-1.medium.com/max/800/1*uZFUNAgGmn22hKrjJTkmFQ.png"></figure><h3 name="09d6" id="09d6" class="graf graf--h3 graf-after--figure">Wrapping up</h3><p name="fdc7" id="fdc7" class="graf graf--p graf-after--h3">As you can see, it’s pretty easy to manage multiple models on the same prediction endpoint. This lets use different techniques to safely test new models before deploying them with <strong class="markup--strong markup--p-strong">minimal risk</strong> to client applications :)</p><p name="4ae8" id="4ae8" class="graf graf--p graf-after--p graf--trailing">That’s it for today. Thank you for reading. As always, please feel free to ask your questions here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Twitter</a>.</p></div></div></section><section name="5ed6" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e9f0" id="e9f0" class="graf graf--p graf--leading"><a href="https://medium.com/@julsimon/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579" data-href="https://medium.com/@julsimon/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">Part 2</strong></a><strong class="markup--strong markup--p-strong"> is available: deploying Amazon SageMaker endpoints with AWS CloudFormation</strong></p><figure name="ee2e" id="ee2e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*qwnJSfEDCax1z-r9" data-width="727" data-height="18" src="https://cdn-images-1.medium.com/max/800/0*qwnJSfEDCax1z-r9"></figure><p name="32cf" id="32cf" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Join our community Slack and read our weekly Faun topics ⬇</strong></p><div name="a9fa" id="a9fa" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium" data-href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium"><strong class="markup--strong markup--mixtapeEmbed-strong">Join a Community of Aspiring Developers.Get must-read articles, learn new technologies for free…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Join thousands of developers and IT experts, get must-read articles, chat with like-minded people, get job offers and…</em>www.faun.dev</a><a href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="196e30881c9915d1a1d318225381bca9" data-thumbnail-img-id="0*AWWZdqNDfqshVEPJ" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*AWWZdqNDfqshVEPJ);"></a></div><h4 name="cb07" id="cb07" class="graf graf--h4 graf-after--mixtapeEmbed graf--trailing">If this post was helpful, please click the clap 👏 button below a few times to show your support for the author! ⬇</h4></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/c0cafe011175"><time class="dt-published" datetime="2018-07-28T16:07:35.640Z">July 28, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/mastering-the-mystical-art-of-model-deployment-c0cafe011175" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
