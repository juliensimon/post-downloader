<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Yet another 10 Deep Learning projects based on Apache MXNet</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Yet another 10 Deep Learning projects based on Apache MXNet</h1>
</header>
<section data-field="subtitle" class="p-summary">
In previous articles, I listed 10 Deep Learning projects based on Apache MXNet…. and then 10 more… and what do you know, here is another…
</section>
<section data-field="body" class="e-content">
<section name="51c1" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="5d94" id="5d94" class="graf graf--h3 graf--leading graf--title">Yet another 10 Deep Learning projects based on Apache MXNet</h3><p name="6afd" id="6afd" class="graf graf--p graf-after--h3">In previous articles, I listed <a href="https://medium.com/@julsimon/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64" data-href="https://medium.com/@julsimon/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64" class="markup--anchor markup--p-anchor" target="_blank">10 Deep Learning projects</a> based on <a href="https://mxnet.incubator.apache.org" data-href="https://mxnet.incubator.apache.org" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">Apache MXNet</a>…. and then <a href="https://medium.com/@julsimon/10-more-deep-learning-projects-based-on-apache-mxnet-a2dababe455f" data-href="https://medium.com/@julsimon/10-more-deep-learning-projects-based-on-apache-mxnet-a2dababe455f" class="markup--anchor markup--p-anchor" target="_blank">10 more</a>… and what do you know, here is another batch!</p><figure name="bdd0" id="bdd0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*h3bzFiFNKHqaxap574xtvg.png" data-width="710" data-height="473" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*h3bzFiFNKHqaxap574xtvg.png"><figcaption class="imageCaption">Oh, we’ll get there… eventually.</figcaption></figure><h3 name="fa2b" id="fa2b" class="graf graf--h3 graf-after--figure">Models</h3><h4 name="f30b" id="f30b" class="graf graf--h4 graf-after--h3">#1 — Dual Path Networks</h4><p name="2496" id="2496" class="graf graf--p graf-after--h4">This is an implementation of the architecture described on <a href="https://arxiv.org/abs/1707.01629" data-href="https://arxiv.org/abs/1707.01629" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the self-titled paper</a> by Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan and Jiashi Feng.</p><blockquote name="bfb6" id="bfb6" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">This architecture won the <a href="http://image-net.org/challenges/LSVRC/2017/results" data-href="http://image-net.org/challenges/LSVRC/2017/results" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">ImageNet 2017 object localization competition</a> with a top-5 error of <strong class="markup--strong markup--blockquote-strong">6.22%</strong>.</blockquote><p name="03be" id="03be" class="graf graf--p graf-after--blockquote">Quoting from the paper: “<em class="markup--em markup--p-em">On the ImageNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed</em>”.</p><div name="4e16" id="4e16" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/cypw/DPNs" data-href="https://github.com/cypw/DPNs" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/cypw/DPNs"><strong class="markup--strong markup--mixtapeEmbed-strong">cypw/DPNs</strong><br><em class="markup--em markup--mixtapeEmbed-em">DPNs - Dual Path Networks</em>github.com</a><a href="https://github.com/cypw/DPNs" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8f572be880d03a39a381cd35497cd4a5" data-thumbnail-img-id="0*hIBcjAuBeO34FE1u." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*hIBcjAuBeO34FE1u.);"></a></div><h4 name="89d9" id="89d9" class="graf graf--h4 graf-after--mixtapeEmbed">#2— Squeeze-and-Excitation Networks</h4><p name="8470" id="8470" class="graf graf--p graf-after--h4">This is an implementation of the architecture described on <a href="https://arxiv.org/abs/1709.01507" data-href="https://arxiv.org/abs/1709.01507" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the self-titled paper</a> by Jie Hu, Li Shen and Gang Sun.</p><blockquote name="096d" id="096d" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">This architecture won the <a href="http://image-net.org/challenges/LSVRC/2017/results" data-href="http://image-net.org/challenges/LSVRC/2017/results" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">ImageNet 2017 classification competition</a> with a top-5 error of <strong class="markup--strong markup--blockquote-strong">2.251%</strong>.</blockquote><div name="1b98" id="1b98" class="graf graf--mixtapeEmbed graf-after--blockquote"><a href="https://github.com/bruinxiong/SENet.mxnet" data-href="https://github.com/bruinxiong/SENet.mxnet" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/bruinxiong/SENet.mxnet"><strong class="markup--strong markup--mixtapeEmbed-strong">bruinxiong/SENet.mxnet</strong><br><em class="markup--em markup--mixtapeEmbed-em">SENet.mxnet — :fire::fire: A MXNet implementation of Squeeze-and-Excitation Networks (SE-ResNext, SE-Resnet…</em>github.com</a><a href="https://github.com/bruinxiong/SENet.mxnet" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="055b985f09e7a6c9aa6b6e85c7cddd43" data-thumbnail-img-id="0*sA048PAKJ0aMiMLj." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*sA048PAKJ0aMiMLj.);"></a></div><h4 name="ff53" id="ff53" class="graf graf--h4 graf-after--mixtapeEmbed">#3 — Capsule Networks (Symbolic API)</h4><p name="369f" id="369f" class="graf graf--p graf-after--h4">This project implements the CapsNet architecture presented in the “<a href="https://arxiv.org/abs/1710.09829" data-href="https://arxiv.org/abs/1710.09829" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Dynamic Routing Between Capsules</strong></a>” paper by Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. In a nutshell, capsule networks are an exciting new development designed to <strong class="markup--strong markup--p-strong">overcome the limitations of convolutional neural networks</strong>.</p><div name="fc27" id="fc27" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/Soonhwan-Kwon/capsnet.mxnet" data-href="https://github.com/Soonhwan-Kwon/capsnet.mxnet" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/Soonhwan-Kwon/capsnet.mxnet"><strong class="markup--strong markup--mixtapeEmbed-strong">Soonhwan-Kwon/capsnet.mxnet</strong><br><em class="markup--em markup--mixtapeEmbed-em">capsnet.mxnet - MXNet implementation of CapsNet</em>github.com</a><a href="https://github.com/Soonhwan-Kwon/capsnet.mxnet" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6c7bc2f35d611f4d471ac174385bf95e" data-thumbnail-img-id="0*x5QAIfyzNC6bKo0u." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*x5QAIfyzNC6bKo0u.);"></a></div><p name="432d" id="432d" class="graf graf--p graf-after--mixtapeEmbed">This code achieves <strong class="markup--strong markup--p-strong">99.71%</strong> accuracy on the <a href="http://yann.lecun.com/exdb/mnist/" data-href="http://yann.lecun.com/exdb/mnist/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MNIST</a> dataset, which is in line with the scores reported in the paper.</p><figure name="069d" id="069d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eJ4u8aUib0ptWBp9aCPpfA.png" data-width="647" data-height="397" src="https://cdn-images-1.medium.com/max/800/1*eJ4u8aUib0ptWBp9aCPpfA.png"></figure><h4 name="fdef" id="fdef" class="graf graf--h4 graf-after--figure">#4 — Capsule Networks (Gluon API)</h4><p name="adf6" id="adf6" class="graf graf--p graf-after--h4">This project also implements the CapsNet architecture, but it does so using the imperative Gluon API (here’s an <a href="https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55" data-href="https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55" class="markup--anchor markup--p-anchor" target="_blank">introduction</a> to Gluon if you’re not familiar with it).</p><div name="c145" id="c145" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/GarrickLin/Capsnet.Gluon" data-href="https://github.com/GarrickLin/Capsnet.Gluon" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/GarrickLin/Capsnet.Gluon"><strong class="markup--strong markup--mixtapeEmbed-strong">GarrickLin/Capsnet.Gluon</strong><br><em class="markup--em markup--mixtapeEmbed-em">Capsnet.Gluon - Capsule Net implementation in Gluon</em>github.com</a><a href="https://github.com/GarrickLin/Capsnet.Gluon" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="10f76d1c47fcf6780fcde75061cf382f" data-thumbnail-img-id="0*-gAuqMBIEs1HySHJ." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*-gAuqMBIEs1HySHJ.);"></a></div><p name="a0e2" id="a0e2" class="graf graf--p graf-after--mixtapeEmbed">This implementation achieves <strong class="markup--strong markup--p-strong">99.53%</strong> accuracy on MNIST, which the author suggests could be improved by adding more data augmentation.</p><figure name="f029" id="f029" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*CzWmdXt52h-z3xZx3WyOMQ.png" data-width="700" data-height="450" src="https://cdn-images-1.medium.com/max/800/1*CzWmdXt52h-z3xZx3WyOMQ.png"></figure><h4 name="99ab" id="99ab" class="graf graf--h4 graf-after--figure">#5 — MobileNets</h4><p name="3472" id="3472" class="graf graf--p graf-after--h4">This is an implementation of the architecture described in “<a href="https://arxiv.org/abs/1704.04861" data-href="https://arxiv.org/abs/1704.04861" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</strong></a>” by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto and Hartwig Adam.</p><p name="f08a" id="f08a" class="graf graf--p graf-after--p">Quoting from the paper: MobileNets are “<em class="markup--em markup--p-em">a class of efficient models (…) for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks”.</em></p><p name="05f4" id="05f4" class="graf graf--p graf-after--p">A model pre-trained on ImageNet is provided, with a top-5 accuracy of <strong class="markup--strong markup--p-strong">90.15%</strong>.</p><div name="368a" id="368a" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/KeyKy/mobilenet-mxnet" data-href="https://github.com/KeyKy/mobilenet-mxnet" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/KeyKy/mobilenet-mxnet"><strong class="markup--strong markup--mixtapeEmbed-strong">KeyKy/<em class="markup--em markup--mixtapeEmbed-em">mobilenet-mxnet</em></strong><br>mobilenet-mxnetgithub.com</a><a href="https://github.com/KeyKy/mobilenet-mxnet" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="3771bcd89206fb5a4537d2c3cbcfe59e" data-thumbnail-img-id="0*Ld142kgCPFl-Ui4A." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Ld142kgCPFl-Ui4A.);"></a></div><h3 name="f225" id="f225" class="graf graf--h3 graf-after--mixtapeEmbed">Applications</h3><h4 name="bece" id="bece" class="graf graf--h4 graf-after--h3">#6— Face Recognition</h4><p name="bd47" id="bd47" class="graf graf--p graf-after--h4">This is an implementation of the architecture described in “<a href="https://arxiv.org/abs/1801.07698" data-href="https://arxiv.org/abs/1801.07698" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</strong></a>” by Jiankang Deng, Jia Guo, and Stefanos Zafeiriou.</p><p name="5453" id="5453" class="graf graf--p graf-after--p">InsightFace is a new face recognition method, which achieves state-of-the art scores of <strong class="markup--strong markup--p-strong">99.80%</strong>+ on <a href="http://vis-www.cs.umass.edu/lfw/" data-href="http://vis-www.cs.umass.edu/lfw/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LFW</a> and <strong class="markup--strong markup--p-strong">98%</strong>+ on <a href="http://megaface.cs.washington.edu/" data-href="http://megaface.cs.washington.edu/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Megaface</a>.</p><div name="cd9e" id="cd9e" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/deepinsight/insightface" data-href="https://github.com/deepinsight/insightface" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/deepinsight/insightface"><strong class="markup--strong markup--mixtapeEmbed-strong">deepinsight/insightface</strong><br><em class="markup--em markup--mixtapeEmbed-em">insightface - Face Recognition Project on MXNet</em>github.com</a><a href="https://github.com/deepinsight/insightface" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="76b0674adaee07e3e24509753b4e5c1c" data-thumbnail-img-id="0*3uOAKjCdBCM3Z1qf." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*3uOAKjCdBCM3Z1qf.);"></a></div><h4 name="6491" id="6491" class="graf graf--h4 graf-after--mixtapeEmbed">#7 — Speech to Text</h4><p name="35db" id="35db" class="graf graf--p graf-after--h4">This is an implementation of the architecture described in “<a href="https://arxiv.org/abs/1512.02595" data-href="https://arxiv.org/abs/1512.02595" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</strong></a>” by Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan and Zhenyao Zhu (pfew!).</p><p name="ac10" id="ac10" class="graf graf--p graf-after--p">This is a great project if you want to build a speech-to-text model. Please bear in mind that you will need a <strong class="markup--strong markup--p-strong">very large dataset</strong>. Quoting from the original paper: “<em class="markup--em markup--p-em">our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours. We use data synthesis to further augment the data during training</em>”.</p><div name="a6da" id="a6da" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/samsungsds-rnd/deepspeech.mxnet" data-href="https://github.com/samsungsds-rnd/deepspeech.mxnet" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/samsungsds-rnd/deepspeech.mxnet"><strong class="markup--strong markup--mixtapeEmbed-strong">samsungsds-rnd/deepspeech.mxnet</strong><br><em class="markup--em markup--mixtapeEmbed-em">deepspeech.mxnet - A MXNet implementation of Baidu&#39;s DeepSpeech architecture</em>github.com</a><a href="https://github.com/samsungsds-rnd/deepspeech.mxnet" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="c5073d0e712a982054b72397063f4643" data-thumbnail-img-id="0*N-HrDEp_Nig8Whmh." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*N-HrDEp_Nig8Whmh.);"></a></div><h4 name="6985" id="6985" class="graf graf--h4 graf-after--mixtapeEmbed">#8— 3D face reconstruction</h4><p name="b312" id="b312" class="graf graf--p graf-after--h4">This is an implementation of the architecture described in “<strong class="markup--strong markup--p-strong">End-to-end 3D face reconstruction with deep neural networks</strong>” by Pengfei Dou, Shishir K. Shah and Ioannis A. Kakadiaris.</p><p name="1644" id="1644" class="graf graf--p graf-after--p">Thanks to this project, you can build a 3D model of a a face using only a single 2D image. Quite impressive!</p><div name="f6c2" id="f6c2" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShownX/mxnet-E2FAR" data-href="https://github.com/ShownX/mxnet-E2FAR" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShownX/mxnet-E2FAR"><strong class="markup--strong markup--mixtapeEmbed-strong">ShownX/mxnet-E2FAR</strong><br><em class="markup--em markup--mixtapeEmbed-em">mxnet-E2FAR - MXNET/Gluon Implementation of End-to-end 3D Face Reconstruction with Deep Neural Networks</em>github.com</a><a href="https://github.com/ShownX/mxnet-E2FAR" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1a63d0926f01666c287e723412d0266c" data-thumbnail-img-id="0*_EgYe6FUVrTm-7VN." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*_EgYe6FUVrTm-7VN.);"></a></div><figure name="aabc" id="aabc" class="graf graf--figure graf-after--mixtapeEmbed"><img class="graf-image" data-image-id="1*eA1G_YupnjaCdxaAjVrvLA.png" data-width="374" data-height="165" src="https://cdn-images-1.medium.com/max/800/1*eA1G_YupnjaCdxaAjVrvLA.png"><figcaption class="imageCaption">Examples taken from the original paper</figcaption></figure><h3 name="1213" id="1213" class="graf graf--h3 graf-after--figure">Tools</h3><h4 name="a453" id="a453" class="graf graf--h4 graf-after--h3">#9 — Deepo</h4><p name="6e4b" id="6e4b" class="graf graf--p graf-after--h4">Deepo is a set of pre-built containers for Deep Learning. It supports MXNet as well as other frameworks. Containers will run on Linux (CPU/GPU), Windows (CPU) and MacOS (CPU) with either Python 2.7 or Python 3.6.</p><div name="9a86" id="9a86" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ufoym/deepo" data-href="https://github.com/ufoym/deepo" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ufoym/deepo"><strong class="markup--strong markup--mixtapeEmbed-strong">ufoym/deepo</strong><br><em class="markup--em markup--mixtapeEmbed-em">deepo - A series of Docker images (and their generator) that allows you to quickly set up your deep learning research…</em>github.com</a><a href="https://github.com/ufoym/deepo" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8c90ade65399c50c200e12b7a79bb0f3" data-thumbnail-img-id="0*oYFsnsjVoVZl0tYO." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*oYFsnsjVoVZl0tYO.);"></a></div><p name="01df" id="01df" class="graf graf--p graf-after--mixtapeEmbed">This is pretty handy if you want to work locally, and of course on AWS with one of our Docker services: <a href="http://aws.amazon.com/ecs" data-href="http://aws.amazon.com/ecs" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ECS</a>, <a href="http://aws.amazon.com/eks" data-href="http://aws.amazon.com/eks" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">EKS</a> or <a href="http://aws.amazon.com/fargate" data-href="http://aws.amazon.com/fargate" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Fargate</a>.</p><h4 name="166e" id="166e" class="graf graf--h4 graf-after--p">#10 — MXNet finetuner</h4><p name="d5e9" id="d5e9" class="graf graf--p graf-after--h4">This tool simplifies the process of fine-tuning an image classification dataset on your own dataset (here’s an <a href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" data-href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" class="markup--anchor markup--p-anchor" target="_blank">introduction to fine-tuning</a> if you’re unfamiliar with this technique).</p><p name="3526" id="3526" class="graf graf--p graf-after--p">It wil automatically build RecordIO files from a tree of images, download pre-trained models, replace the last layer according to the number of classes in your dataset, add data augmentation, run fine-tuning, visualize results, etc.</p><p name="2484" id="2484" class="graf graf--p graf-after--p">Good stuff!</p><div name="c454" id="c454" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://github.com/knjcode/mxnet-finetuner" data-href="https://github.com/knjcode/mxnet-finetuner" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/knjcode/mxnet-finetuner"><strong class="markup--strong markup--mixtapeEmbed-strong">knjcode/mxnet-finetuner</strong><br><em class="markup--em markup--mixtapeEmbed-em">mxnet-finetuner - An all-in-one Deep Learning toolkit for image classification to fine-tuning pretrained models using…</em>github.com</a><a href="https://github.com/knjcode/mxnet-finetuner" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="83d94166eb7bd151ff7e6fb5e1cd27b0" data-thumbnail-img-id="0*otZg4sS6BoMonC31." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*otZg4sS6BoMonC31.);"></a></div></div></div></section><section name="8928" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9e9d" id="9e9d" class="graf graf--p graf--leading">That’s it for today. Kudos to all project authors for their fascinating work. I hope they will inspire you to <a href="https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459" data-href="https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459" class="markup--anchor markup--p-anchor" target="_blank">get started with Deep Learning</a>.</p><p name="f0dc" id="f0dc" class="graf graf--p graf-after--p graf--trailing">As always, thanks a lot for reading!</p></div></div></section><section name="36f6" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="c47c" id="c47c" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">One of the most addictive albums I’ve heard in years. Listen once, sing it forever \m/</em></p><figure name="786d" id="786d" class="graf graf--figure graf--iframe graf-after--p graf--trailing"><iframe src="https://open.spotify.com/embed/album/3xlz8I3SbJc8UsZy7dyBUD" width="300" height="380" frameborder="0" scrolling="no"></iframe></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/da09f30cca38"><time class="dt-published" datetime="2018-02-25T18:23:16.353Z">February 25, 2018</time></a>.</p><p><a href="https://medium.com/@julsimon/yet-another-10-deep-learning-projects-based-on-apache-mxnet-da09f30cca38" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
