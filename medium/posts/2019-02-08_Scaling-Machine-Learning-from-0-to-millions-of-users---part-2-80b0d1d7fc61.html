<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Scaling Machine Learning from 0 to millions of users — part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Scaling Machine Learning from 0 to millions of users — part 2</h1>
</header>
<section data-field="subtitle" class="p-summary">
Training on EC2, EMR, ECS, EKS or SageMaker?
</section>
<section data-field="body" class="e-content">
<section name="2fa5" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="330c" id="330c" class="graf graf--h3 graf--leading graf--title">Scaling Machine Learning from 0 to millions of users — part 2</h3><h4 name="ada6" id="ada6" class="graf graf--h4 graf-after--h3 graf--subtitle">Training: EC2, EMR, ECS, EKS or SageMaker?</h4><p name="82dd" id="82dd" class="graf graf--p graf-after--h4">In <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" data-href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" class="markup--anchor markup--p-anchor" target="_blank">part 1</a>, we broke out of the laptop, and decided to deploy our prediction service on a virtual machine. By doing so, we discussed a few simple techniques that helped with initial scalability… and hopefully with reducing manual ops. Since then, despite a few production hiccups due the lack of high availability, life has been pretty good.</p><p name="bb3b" id="bb3b" class="graf graf--p graf-after--p">However, traffic soon starts to increase, data piles up, more models need to be trained, etc. Technical and business stakes are getting higher, and let’s face it, the current architecture will go underwater soon. Time’s up: in this post, we’ll focus on <strong class="markup--strong markup--p-strong">scaling training to a large number of machines</strong>.</p><figure name="b039" id="b039" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*0OcxdRHpzI3An9BQ" data-width="560" data-height="425" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*0OcxdRHpzI3An9BQ"><figcaption class="imageCaption">Two hundred… and fifty… six… GPUs!!! Ja!</figcaption></figure><blockquote name="decd" id="decd" class="graf graf--blockquote graf--hasDropCapModel graf-after--figure">This is an opinionated series, remember? It’s also based on what I hear when talking to real-life customers, not ideal ones. Reality is often ugly, and only tech hipsters (and top management) think that it looks exactly like that fancy article your read on &lt;insert_name_here&gt; ;)</blockquote><blockquote name="4f15" id="4f15" class="graf graf--blockquote graf--hasDropCapModel graf-after--blockquote">Special thanks to rockin’ Evangelists Abby Fuller, Jerry Hargrove, Adrian Hornsby, Brent Langston and Ian Massingham for their tips and ideas.</blockquote><h3 name="3dcc" id="3dcc" class="graf graf--h3 graf-after--blockquote"><strong class="markup--strong markup--h3-strong">Scaling up will fix it… right?</strong></h3><p name="4f1f" id="4f1f" class="graf graf--p graf-after--h3">Yes and no. Yes, it can be a <strong class="markup--strong markup--p-strong">short-term solution</strong> to use a large server for training and prediction. Amazon EC2 has a ton of <a href="https://aws.amazon.com/ec2/instance-types/" data-href="https://aws.amazon.com/ec2/instance-types/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">instance types</a> to pick from, and all it takes is <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/stop-instances.html" data-href="https://docs.aws.amazon.com/cli/latest/reference/ec2/stop-instances.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">stopping</a> your instance, <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/modify-instance-attribute.html" data-href="https://docs.aws.amazon.com/cli/latest/reference/ec2/modify-instance-attribute.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">changing the instance type</a>, and <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/start-instances.html" data-href="https://docs.aws.amazon.com/cli/latest/reference/ec2/start-instances.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">starting</a> it again.</p><blockquote name="b704" id="b704" class="graf graf--pullquote graf-after--p">You should only be using APIs from now on. When it comes to scaling tech &amp; ops, manual work in any GUI is the Antichrist.</blockquote><p name="ba67" id="ba67" class="graf graf--p graf-after--pullquote">No, because it’s only a <strong class="markup--strong markup--p-strong">temporary solution</strong>:</p><ul class="postList"><li name="3b94" id="3b94" class="graf graf--li graf-after--p">It’s fine to use bigger instances, until <strong class="markup--strong markup--li-strong">there is no bigger instance</strong>. Then what?</li><li name="bd7f" id="bd7f" class="graf graf--li graf-after--li">It’s also quite possible that <strong class="markup--strong markup--li-strong">your workload won’t scale nicely</strong>, and won’t make full use of the additional hardware (RAM, CPU cores, I/O, etc.). A marginal performance gain isn’t worth the extra spend.</li><li name="fcff" id="fcff" class="graf graf--li graf-after--li">Most of all, <strong class="markup--strong markup--li-strong">scaling up will simply delay the inevitable</strong>. Keep doing it, and the only thing you’ll end up with is a bigger problem to solve.</li></ul><p name="9023" id="9023" class="graf graf--p graf-after--li">In the spirit of avoiding over-engineering, it’s OK to scale up a couple of times, but if monitoring keeps pointing at the Impassable Wall of Scalability Doom, I’d advise you to <strong class="markup--strong markup--p-strong">act a little too early rather than a little too late</strong>: things scale linearly until they don’t, and you don’t want to find out what happens when the exponential starts rising!</p><h3 name="bfac" id="bfac" class="graf graf--h3 graf-after--p">Scaling out</h3><p name="bf7d" id="bf7d" class="graf graf--p graf-after--h3">When it comes to training Machine Learning (ML) models, the top requirements are actually pretty simple:</p><ul class="postList"><li name="517c" id="517c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Reliable, scalable storage </strong>for your data sets.</li><li name="c33a" id="c33a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Elastic compute clusters</strong>, that can be started on-demand in lots of different configurations (hardware, frameworks, etc.).</li><li name="9332" id="9332" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">As little ops as possible: </strong>ML is what you should focus on, because ML is what turns your raw data into revenue / profit / improver customer experience.</li></ul><blockquote name="20c6" id="20c6" class="graf graf--blockquote graf--hasDropCapModel graf--startsWithDoubleQuote graf-after--li">“That’s not the full list! We want total control, no lock-in, low bills, top performance… and everything else too, whatever it is”. Yes, yes, we’ll get there :)</blockquote><h3 name="0068" id="0068" class="graf graf--h3 graf-after--blockquote">Storage</h3><p name="2ed6" id="2ed6" class="graf graf--p graf-after--h3">Let’s get that one out of the way: <strong class="markup--strong markup--p-strong">your data goes to Amazon S3</strong>. Any other choice would need a bullet-proof justification (try me!). Throughput? Well, you now get up to <strong class="markup--strong markup--p-strong">25 Gigabit per second between EC2 and S3</strong>. That should enough for now. Scalability? High availability? Security? No ops? Cost? Check.</p><div name="e0b5" id="e0b5" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/" data-href="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/"><strong class="markup--strong markup--mixtapeEmbed-strong">The Floodgates Are Open - Increased Network Bandwidth for EC2 Instances | Amazon Web Services</strong><br><em class="markup--em markup--mixtapeEmbed-em">I hope that you have configured your AMIs and your current-generation EC2 instances to use the Elastic Network Adapter…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="93be5551fa6453f2c6979b4857a7bb20"></a></div><blockquote name="f181" id="f181" class="graf graf--blockquote graf--hasDropCapModel graf-after--mixtapeEmbed">At this point, anyone in your team coming up with a “high performance storage cluster based on this super cool open source project” should be hit on the head with a heavy object until they stop moving. No mercy for wasting time, putting projects at risk, and over-engineering.</blockquote><figure name="c88c" id="c88c" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="0*RsLrN3NR8QTSY7g0.jpg" data-width="600" data-height="483" src="https://cdn-images-1.medium.com/max/800/0*RsLrN3NR8QTSY7g0.jpg"><figcaption class="imageCaption">Because “the doc wasn’t clear” and “bugs happen to everyone”.</figcaption></figure><p name="c5c7" id="c5c7" class="graf graf--p graf-after--figure">OK, now what about compute options? <strong class="markup--strong markup--p-strong">Amazon EC2? Amazon EMR? Container services? Amazon SageMaker?</strong></p><blockquote name="c09a" id="c09a" class="graf graf--pullquote graf-after--p">Begun the Scaling War has.</blockquote><h3 name="4732" id="4732" class="graf graf--h3 graf-after--pullquote">Amazon EC2</h3><p name="7192" id="7192" class="graf graf--p graf-after--h3">Our journey started on EC2, so it can be quite tempting to continue there. Is that a good enough reason?</p><p name="dc08" id="dc08" class="graf graf--p graf-after--p">As discussed in <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" data-href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" class="markup--anchor markup--p-anchor" target="_blank">part 1</a>, the <a href="https://aws.amazon.com/machine-learning/amis/" data-href="https://aws.amazon.com/machine-learning/amis/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Deep Learning AMI</strong></a> makes your life much simpler. It’s packed with open source tools and libraries optimized by AWS (<a href="https://aws.amazon.com/about-aws/whats-new/2018/10/chainer4-4_theano_1-0-2_launch_deep_learning_ami/?nc1=h_ls" data-href="https://aws.amazon.com/about-aws/whats-new/2018/10/chainer4-4_theano_1-0-2_launch_deep_learning_ami/?nc1=h_ls" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">11x speedup</a> on TensorFlow 1.11, anyone? Or maybe <a href="https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/" data-href="https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">linear scaling up to 256 GPUs</a>?). <a href="https://github.com/uber/horovod" data-href="https://github.com/uber/horovod" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Horovod</strong></a>, a popular library for distributed training, is also <a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-tensorflow.html" data-href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-tensorflow.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">included</a>. All of this will make it much easier to setup efficient, distributed training clusters.</p><p name="fb38" id="fb38" class="graf graf--p graf-after--p">Some constraints may prevent you from using that <strong class="markup--strong markup--p-strong">AWS-maintained AMI</strong>: need for a specific OS vendor, licensing restrictions, having to run identical builds across different providers, etc. Unfortunately, you’ll have to set everything up yourself. Don’t underestimate that task: you’ll have to do it again and again as new versions are released. Even with automation, that’ll never be a lights-out operation.</p><p name="d7a1" id="d7a1" class="graf graf--p graf-after--p">Whichever AMI you use, setting up a training cluster means:</p><ol class="postList"><li name="832c" id="832c" class="graf graf--li graf-after--p">Firing up a bunch of instances,</li><li name="3b9f" id="3b9f" class="graf graf--li graf-after--li">Picking one as the leader, and setting up distributed training. That usually involves listing hostnames / IP addresses of other machines in the cluster.</li><li name="b07e" id="b07e" class="graf graf--li graf-after--li">Start training,</li><li name="0189" id="0189" class="graf graf--li graf-after--li">Once training is complete, grab the trained model and save it in Amazon S3.</li><li name="cfba" id="cfba" class="graf graf--li graf-after--li">Shut the training cluster down.</li></ol><p name="26a6" id="26a6" class="graf graf--p graf-after--li">Quite a bit of work, then, which is probably why most of you go through steps 1 and 2 once, <strong class="markup--strong markup--p-strong">run the cluster 24/7</strong>, and never make it to step 5… And this, ladies and gentlemen, is my main concern for using EC2 here. <strong class="markup--strong markup--p-strong">Unless you automate all of this</strong> (with <a href="http://aws.amazon.com/cloudformation" data-href="http://aws.amazon.com/cloudformation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS CloudFormation</a>, <a href="https://www.terraform.io" data-href="https://www.terraform.io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Terraform</a>, CLI scripts, etc.), <strong class="markup--strong markup--p-strong">you will waste a ton of money</strong>. Someone up above will quickly put a cap on your budget, meaning that you’ll probably end up with a fixed-size cluster that needs to be time-shared by multiple developers / teams… and of course, someone will develop a nice intranet page to book time slots on the cluster. Congratulations, you’re reinvented the mainframe!</p><figure name="e256" id="e256" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*mkO2AvthJStvlArN" data-width="590" data-height="479" src="https://cdn-images-1.medium.com/max/800/0*mkO2AvthJStvlArN"><figcaption class="imageCaption">Resnet-50 training in 3… 2… 1…</figcaption></figure><blockquote name="4ebf" id="4ebf" class="graf graf--blockquote graf--hasDropCapModel graf-after--figure">Don’t laugh. I’ve met a very large — and otherwise brilliant — AI company managing hundreds of physical GPU servers just like this. Unfortunately, I’m sure some customers do the same on EC2… Get in touch if you’re stuck there, we can help!</blockquote><p name="a20c" id="a20c" class="graf graf--p graf-after--blockquote">Of course, maybe your DevOps team was kind enough to provide <strong class="markup--strong markup--p-strong">an all-singing, all-dancing cluster provisioning script</strong> that each developer can run to get their own training cluster (buy them lots of beer: that rarely ever happens). Would that be a good enough reason to stick to EC2 for training?</p><p name="a9fb" id="a9fb" class="graf graf--p graf-after--p">Maybe. Techniques for cost optimization on EC2 are well-known: <a href="https://aws.amazon.com/ec2/pricing/reserved-instances/" data-href="https://aws.amazon.com/ec2/pricing/reserved-instances/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">reserved instances</a>, <a href="https://aws.amazon.com/ec2/spot/" data-href="https://aws.amazon.com/ec2/spot/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">spot instances</a>, etc. Doing that right may offset the extra DevOps costs. Only you can find out, and you definitely should.</p><blockquote name="d77e" id="d77e" class="graf graf--pullquote graf--hasDropCapModel graf-after--p"><strong class="markup--strong markup--pullquote-strong">If you don’t have time or skills to get automation and cost optimization right, I’d think twice about running training jobs at scale on EC2.</strong></blockquote><h3 name="ba95" id="ba95" class="graf graf--h3 graf-after--pullquote"><strong class="markup--strong markup--h3-strong">Amazon EMR</strong></h3><p name="224b" id="224b" class="graf graf--p graf-after--h3"><a href="http://aws.amazon.com/emr" data-href="http://aws.amazon.com/emr" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon EMR</a> in a ML discussion? Well, yes: <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-tensorflow.html" data-href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-tensorflow.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> and <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-mxnet.html" data-href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-mxnet.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apache MXNet</a> are part of the EMR distribution, and of course, <a href="https://spark.apache.org/mllib/" data-href="https://spark.apache.org/mllib/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Spark MLlib</a> is also included. EMR supports <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html" data-href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">compute-optimized instances</a> (c5 and p3), so it looks like we have everything we need.</p><p name="ca99" id="ca99" class="graf graf--p graf-after--p">Here are <strong class="markup--strong markup--p-strong">a few good reasons to run training jobs on EMR</strong>:</p><ul class="postList"><li name="acba" id="acba" class="graf graf--li graf-after--p">You already use EMR for other tasks, with solid automation (on-demand clusters, <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html" data-href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">steps</a>, etc.) and cost optimization (<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html" data-href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">spot instances</a>!).</li><li name="7adf" id="7adf" class="graf graf--li graf-after--li">Your data requires a lot of ETL, and Hive / Spark would work great for that. Why not run everything in one place?</li><li name="d0e0" id="d0e0" class="graf graf--li graf-after--li">Spark MLlib has the algos you need.</li><li name="acb4" id="acb4" class="graf graf--li graf-after--li">You read somewhere that there a <a href="https://github.com/aws/sagemaker-spark" data-href="https://github.com/aws/sagemaker-spark" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker SDK for Spark</a>, so that could be another option for the future.</li></ul><p name="841d" id="841d" class="graf graf--p graf-after--li">And <strong class="markup--strong markup--p-strong">equally good reasons NOT to do it</strong>:</p><ul class="postList"><li name="3b6a" id="3b6a" class="graf graf--li graf-after--p">You don’t have time or skills to automate and optimize costs. GPU-based EMR clusters running 24/7 at on-demand price with zero load <em class="markup--em markup--li-em">&lt;rolls a D100 for sanity check… &gt;</em></li><li name="c5c3" id="c5c3" class="graf graf--li graf-after--li">You’re using neither TensorFlow, Apache MXNet nor Spark MLlib. Yes, you could install additional packages to your clusters, but that’s extra work.</li><li name="4a5f" id="4a5f" class="graf graf--li graf-after--li">Your ETL and ML jobs have <strong class="markup--strong markup--li-strong">conflicting instance requirements</strong>. Let’s say that they would respectively run best on 8 <em class="markup--em markup--li-em">r5.4xlarge</em> and 2 <em class="markup--em markup--li-em">p3.8xlarge</em> for training. How do you compromise? That’s a hard call, and you may end up picking an instance type that’s suboptimal for both ETL and training… or creating a dedicated GPU cluster (another one to manage and worry about).</li></ul><p name="253f" id="253f" class="graf graf--p graf-after--li">I have <strong class="markup--strong markup--p-strong">mixed feelings</strong> about this: I’d be fine with piling some amount of ML on top on an existing cluster, but <strong class="markup--strong markup--p-strong">unless it was 100% based on Spark MLlib, scaling it simply wouldn’t feel right</strong>.</p><h3 name="62f4" id="62f4" class="graf graf--h3 graf-after--p">Container services</h3><p name="f0ec" id="f0ec" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" data-href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" class="markup--anchor markup--p-anchor" target="_blank">part 1</a>, I suggested early one that you containerize your code in order to solve deployment issues. Obviously, this would also pay dividends when deploying to Docker clusters, whether on <a href="http://aws.amazon.com/ecs" data-href="http://aws.amazon.com/ecs" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon ECS</strong></a> or <a href="http://aws.amazon.com/eks" data-href="http://aws.amazon.com/eks" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon EKS</strong></a>.</p><p name="33a0" id="33a0" class="graf graf--p graf-after--p">You can <strong class="markup--strong markup--p-strong">run any workload</strong> in a Docker container, and you can <strong class="markup--strong markup--p-strong">move it around </strong>without any restriction, from your laptop to your production environment (or so I’m told). When running on AWS, costs can be squeezed with auto scaling, reserved instances, spot instances, etc. Woohoo.</p><p name="eaaa" id="eaaa" class="graf graf--p graf-after--p">From a training perspective, containers give you <strong class="markup--strong markup--p-strong">full flexibility</strong> to use any open source library, or even your own custom code. All popular ML/DL libraries provide <strong class="markup--strong markup--p-strong">base images</strong>, which you can either run directly or customize, and these will save you a lot of time.</p><p name="5ecd" id="5ecd" class="graf graf--p graf-after--p">Thanks to auto scaling now supporting <a href="https://aws.amazon.com/blogs/aws/new-ec2-auto-scaling-groups-with-multiple-instance-types-purchase-options/" data-href="https://aws.amazon.com/blogs/aws/new-ec2-auto-scaling-groups-with-multiple-instance-types-purchase-options/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">mixed instances types</a>, <strong class="markup--strong markup--p-strong">different instance types can coexist within the same cluster. </strong>Thus, you can easily add compute-optimized instances to any cluster and schedule your ML/DL trainings there. Of course, you can also create a dedicated cluster for training if you think that makes more sense.</p><p name="3e2b" id="3e2b" class="graf graf--p graf-after--p">Last but not least, <strong class="markup--strong markup--p-strong">GPU instances</strong> are supported on both services, with <strong class="markup--strong markup--p-strong">GPU-optimized AMIs</strong> to boot (nvidia-docker, NVIDIA drivers, etc).</p><h4 name="e1b4" id="e1b4" class="graf graf--h4 graf-after--p">Training on Amazon ECS</h4><p name="f0a4" id="f0a4" class="graf graf--p graf-after--h4">To minimize training time and cost, you need to make sure that training jobs run on the <strong class="markup--strong markup--p-strong">most appropriate instance type</strong> (say <em class="markup--em markup--p-em">c5</em> or <em class="markup--em markup--p-em">p3</em>). Amazon ECS lets you add <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html" data-href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">placement constraints</a> in <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html" data-href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">task definitions</a>. Here’s how we would ask ECS to schedule this task only on <em class="markup--em markup--p-em">p3</em> instances.</p><pre name="7126" id="7126" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">&quot;placementConstraints&quot;: [<br>    {<br>        &quot;expression&quot;: &quot;attribute:ecs.instance-type =~ p3.*&quot;,<br>        &quot;type&quot;: &quot;memberOf&quot;<br>    }<br>]</code></pre><p name="bbbb" id="bbbb" class="graf graf--p graf-after--pre">We can also one step further, thanks to a new feature that lets <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-gpu.html" data-href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-gpu.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">pin a specific number of GPUs</strong></a> to a given task, e.g:</p><pre name="52fa" id="52fa" class="graf graf--pre graf-after--p">{ &quot;containerDefinitions&quot;: [ <br>    { &quot;resourceRequirements&quot; : [ <br>       { &quot;type&quot; : &quot;GPU&quot;, &quot;value&quot; : <code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0"><em class="markup--em markup--pre-em">&quot;</em>2</code>&quot; }<br>      ],<br>    },<br>   ... <br>}</pre><p name="8e20" id="8e20" class="graf graf--p graf-after--pre">Here’s a nice blog post with additional details.</p><div name="5af8" id="5af8" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/" data-href="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/"><strong class="markup--strong markup--mixtapeEmbed-strong">Scheduling GPUs for deep learning tasks on Amazon ECS | Amazon Web Services</strong><br><em class="markup--em markup--mixtapeEmbed-em">This post is contributed by Brent Langston - Sr. Developer Advocate, Amazon Container Services Last week, AWS announced…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6fc0f603fdb3115007783858c6dc9a0b" data-thumbnail-img-id="0*nOtoUfH3SK6htXVs" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*nOtoUfH3SK6htXVs);"></a></div><h4 name="604c" id="604c" class="graf graf--h4 graf-after--mixtapeEmbed">Training on Amazon EKS</h4><p name="6688" id="6688" class="graf graf--p graf-after--h4">You can pretty much do the same thing on EKS. This nice blog post will walk you through the whole process of adding <em class="markup--em markup--p-em">p3</em> worker nodes to an existing cluster, defining a GPU-powered pod, and launching it on the cluster.</p><div name="5422" id="5422" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/" data-href="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/"><strong class="markup--strong markup--mixtapeEmbed-strong">Running GPU-Accelerated Kubernetes Workloads on P3 and P2 EC2 Instances with Amazon EKS | Amazon…</strong><br><em class="markup--em markup--mixtapeEmbed-em">This post contributed by Scott Malkie, AWS Solutions Architect Amazon EC2 P3 and P2 instances, featuring NVIDIA GPUs…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="dce9b4039f609b693e2419d1419ff5ae" data-thumbnail-img-id="0*gAJ-l2__7uvA8Bsc" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*gAJ-l2__7uvA8Bsc);"></a></div><h4 name="0513" id="0513" class="graf graf--h4 graf-after--mixtapeEmbed">Container services for ML/DL training, yes or no?</h4><blockquote name="bef0" id="bef0" class="graf graf--blockquote graf--hasDropCapModel graf-after--h4">If you belong to the Wild Hyperborean Horde who’d rather eat frozen Yack poo than NOT use home-made containers for absolutely everything, you’ve already answered the question, haven’t you? ;)</blockquote><figure name="234e" id="234e" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="0*tDaTZrUsPZkBpkpE" data-width="474" data-height="296" src="https://cdn-images-1.medium.com/max/800/0*tDaTZrUsPZkBpkpE"><figcaption class="imageCaption">Yes, for a very short while. And then you got your guts ripped out. Hmm.</figcaption></figure><p name="9b91" id="9b91" class="graf graf--p graf-after--figure">If the need ever arose, <strong class="markup--strong markup--p-strong">I wouldn’t worry about scaling container services to a large number of nodes</strong> (ECS was designed to <a href="https://www.allthingsdistributed.com/2015/07/under-the-hood-of-the-amazon-ec2-container-service.html" data-href="https://www.allthingsdistributed.com/2015/07/under-the-hood-of-the-amazon-ec2-container-service.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">scale linearly to 1,000+ nodes</a>). However, once again, you should very much worry about your ability to <strong class="markup--strong markup--p-strong">scale ops</strong> (containers, clusters, etc.) and <strong class="markup--strong markup--p-strong">manage cost</strong>.</p><p name="5410" id="5410" class="graf graf--p graf-after--p">If you work in a Docker shop where another team is managing clusters, providing you with <strong class="markup--strong markup--p-strong">agile, automated and cost-effective ways to provision</strong> them, then sure. It could be as easy as committing a TensorFlow script, and then letting a CI/CD pipeline deploy it automatically to a cluster. Not a lot of extra work for ML developers and data scientists.</p><p name="c077" id="c077" class="graf graf--p graf-after--p">Now, if you live a world where you have to <strong class="markup--strong markup--p-strong">build and operate clusters on top of your actual ML job</strong>, that’s not such a exciting proposition any more. Plumbing, large bills, fire, brimstone… you know the story.</p><p name="7083" id="7083" class="graf graf--p graf-after--p">Let’s look at setting up and managing large, distributed training jobs with Horovod: here’s the <a href="https://github.com/uber/horovod/blob/master/docs/running.md" data-href="https://github.com/uber/horovod/blob/master/docs/running.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentation</a>. Once you’ve got everything figured out (<a href="https://github.com/uber/horovod/blob/master/docs/docker.md" data-href="https://github.com/uber/horovod/blob/master/docs/docker.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Docker</a>, <a href="https://github.com/kubeflow/kubeflow/blob/master/kubeflow/openmpi/" data-href="https://github.com/kubeflow/kubeflow/blob/master/kubeflow/openmpi/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Kubeflow</a>, <a href="https://github.com/kubeflow/mpi-operator/" data-href="https://github.com/kubeflow/mpi-operator/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MPI Operator</a>, <a href="https://github.com/kubernetes/charts/tree/master/stable/horovod/" data-href="https://github.com/kubernetes/charts/tree/master/stable/horovod/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Helm Chart</a>, and <a href="https://github.com/IBM/FfDL/tree/master/etc/examples/horovod/" data-href="https://github.com/IBM/FfDL/tree/master/etc/examples/horovod/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">FfDL</a>), here’s how to run a training job on 4 machines with 4 GPUs each:</p><pre name="760b" id="760b" class="graf graf--pre graf-after--p">$ mpirun -np 16 \<br>    -H server1:4,server2:4,server3:4,server4:4 \<br>    -bind-to none -map-by slot \<br>    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \<br>    -mca pml ob1 -mca btl ^openib \<br>    python train.py</pre><p name="760c" id="760c" class="graf graf--p graf-after--pre">Don’t get me wrong: Docker, Kubernetes, Horovod and so on are impressive<strong class="markup--strong markup--p-strong"> </strong>projects, but if you insist on building and maintaining everything yourself (or if Hyperborean Harald sneers that it’s “<em class="markup--em markup--p-em">the only proper way to do it</em>”), you should know what you’re stepping into, as you will be using this all day long.</p><blockquote name="f535" id="f535" class="graf graf--pullquote graf-after--p"><strong class="markup--strong markup--pullquote-strong">Is this what you really need? </strong>Maybe, maybe not. <strong class="markup--strong markup--pullquote-strong">Please make up your own mind.</strong></blockquote><h3 name="e0b6" id="e0b6" class="graf graf--h3 graf-after--pullquote">Amazon SageMaker</h3><p name="bace" id="bace" class="graf graf--p graf-after--h3">One more option to go: <a href="http://aws.amazon.com/sagemaker" data-href="http://aws.amazon.com/sagemaker" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a>. I’ve discussed it at lengths in previous posts and talks (start <a href="https://medium.com/@julsimon/talk-from-notebook-to-production-with-amazon-sagemaker-ee2a2036c0fe" data-href="https://medium.com/@julsimon/talk-from-notebook-to-production-with-amazon-sagemaker-ee2a2036c0fe" class="markup--anchor markup--p-anchor" target="_blank">here</a> for an recent overview), and as the most recent service of the bunch, I’ve saved it for last to see where it improves on previous options with respect to training large jobs.</p><p name="e701" id="e701" class="graf graf--p graf-after--p">A quick reminder:</p><ul class="postList"><li name="9402" id="9402" class="graf graf--li graf-after--p">All activity in SageMaker is driven by a <strong class="markup--strong markup--li-strong">high-level </strong><a href="https://github.com/aws/sagemaker-python-sdk" data-href="https://github.com/aws/sagemaker-python-sdk" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Python SDK</strong></a>.</li><li name="c505" id="c505" class="graf graf--li graf-after--li">Training is based on on-demand, fully-managed instances. <strong class="markup--strong markup--li-strong">Zero</strong> infrastructure work. Spot instances are not available.</li><li name="b3c7" id="b3c7" class="graf graf--li graf-after--li">Models may be trained using <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">AWS-maintained built-in algorithms</strong></a> and <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">optimized frameworks</strong></a> (same ones as in the Deep Learning AMI), as well as <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">custom algorithms</strong></a>.</li><li name="70ce" id="70ce" class="graf graf--li graf-after--li">Distributed training is built-in. <strong class="markup--strong markup--li-strong">Zero</strong> setup.</li><li name="6493" id="6493" class="graf graf--li graf-after--li">Plenty of <strong class="markup--strong markup--li-strong">examples</strong> are available <a href="https://github.com/awslabs/amazon-sagemaker-examples" data-href="https://github.com/awslabs/amazon-sagemaker-examples" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">here</a>.</li></ul><p name="73e5" id="73e5" class="graf graf--p graf-after--li">For instance, here’s how you would <strong class="markup--strong markup--p-strong">train a TensorFlow model</strong>. First, put your data in Amazon S3 (it’s hopefully already there). Then, configure your training job: pass your code, the instance type, the number of instances. Finally, call <em class="markup--em markup--p-em">fit()</em>.</p><pre name="36ad" id="36ad" class="graf graf--pre graf-after--p">from sagemaker.tensorflow import TensorFlow<br><br>tf_model = TensorFlow(entry_point=&#39;model.py&#39;,<br>                             role=role,<br>                             framework_version=&#39;1.12.0&#39;,<br>                             training_steps=1000, <br>                             evaluation_steps=100,<br>                             train_instance_count=2,<br>                             train_instance_type=&#39;ml.c4.xlarge&#39;)<br><br>tf_model.fit(inputs)</pre><p name="2bbc" id="2bbc" class="graf graf--p graf-after--pre">That’s it. <strong class="markup--strong markup--p-strong">This is at least 10x (100x ?) less code than any automation you’d be using with EC2 or containers</strong>.</p><p name="abe0" id="abe0" class="graf graf--p graf-after--p">And lock-in? Well, none: you’re free to <strong class="markup--strong markup--p-strong">take your TensorFlow code and run it anywhere</strong> else.</p><h4 name="9a6f" id="9a6f" class="graf graf--h4 graf-after--p">EC2 or SageMaker?</h4><p name="52e2" id="52e2" class="graf graf--p graf-after--h4">Compared to EC2, <strong class="markup--strong markup--p-strong">SageMaker saves you from managing any infrastructure</strong>, and probably a lot of framework containers too. That might not be a big deal when you’re working with a couple of instances, but it sure it when you start scaling to tens or hundreds, running all kinds of different jobs.</p><p name="062c" id="062c" class="graf graf--p graf-after--p">SageMaker also terminates training clusters automatically once training is complete.</p><blockquote name="6df4" id="6df4" class="graf graf--pullquote graf-after--p"><strong class="markup--strong markup--pullquote-strong">You will never overpay for training</strong>.</blockquote><p name="6106" id="6106" class="graf graf--p graf-after--pullquote">Yes, SageMaker instances are more expensive than EC2 instances. However, if you factor in less ops and automatic termination, I’d be really surprised if the gap wasn’t significantly reduced.</p><blockquote name="7c45" id="7c45" class="graf graf--pullquote graf-after--p">Total cost of ownership is what matters.</blockquote><h4 name="b167" id="b167" class="graf graf--h4 graf-after--pullquote">EMR or SageMaker?</h4><p name="14c5" id="14c5" class="graf graf--p graf-after--h4">As mentioned earlier, I don’t see any compelling reason to use EMR at scale for training unless you stick to Spark MLlib. Still, if you’re asking yourself the question, you’re probably already using EMR... so how about both? As it happens, SageMaker also includes a <a href="https://github.com/aws/sagemaker-spark" data-href="https://github.com/aws/sagemaker-spark" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Spark SDK</a>. I’ve covered this topic before.</p><div name="09b8" id="09b8" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7" data-href="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7"><strong class="markup--strong markup--mixtapeEmbed-strong">Mixing Spark with Sagemaker ?</strong><br><em class="markup--em markup--mixtapeEmbed-em">This short post comes from a question asked by Manel Maragal (thanks!) on my YouTube channel. It’s a really good…</em>medium.com</a><a href="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="e4bb9a19d5b9a5107b19b5eb5c4d1aa2"></a></div><div name="7ae0" id="7ae0" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243" data-href="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243"><strong class="markup--strong markup--mixtapeEmbed-strong">Apache Spark and Amazon SageMaker, the Infinity Gems of analytics</strong><br><em class="markup--em markup--mixtapeEmbed-em">In a previous post, I showed you how to build a spam classifier by running PySpark on an Amazon SageMaker notebook…</em>medium.com</a><a href="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="be04f50314b23f24ff9403ba470e7300" data-thumbnail-img-id="0*IMN1uGhTOkn7ccgv." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*IMN1uGhTOkn7ccgv.);"></a></div><p name="f9f5" id="f9f5" class="graf graf--p graf-after--mixtapeEmbed">The short version is: separate concerns.</p><blockquote name="6f61" id="6f61" class="graf graf--pullquote graf-after--p"><strong class="markup--strong markup--pullquote-strong">Spark for large-scale ETL</strong>, <strong class="markup--strong markup--pullquote-strong">SageMaker for large-scale training</strong>.</blockquote><h4 name="3441" id="3441" class="graf graf--h4 graf-after--pullquote">Container services or SageMaker?</h4><p name="1125" id="1125" class="graf graf--p graf-after--h4">There is a myriad of technical details that separate these two approaches, but at the end of the day, I think the choice does come down to <strong class="markup--strong markup--p-strong">engineering culture </strong>and <strong class="markup--strong markup--p-strong">focus</strong>.</p><p name="28d7" id="28d7" class="graf graf--p graf-after--p">Some teams are convinced <strong class="markup--strong markup--p-strong">doing everything themselves creates value for the company</strong> ( in some cases, it does), and some other teams would rather <strong class="markup--strong markup--p-strong">rely on managed services in order to iterate as fast as possible</strong>. Some teams feel better about <strong class="markup--strong markup--p-strong">putting all their eggs in a single basket</strong> (“we run everything on Docker clusters”), some other teams are happier with using <strong class="markup--strong markup--p-strong">different services for different things</strong>. No one but them can judge what’s best for their particular use case.</p><blockquote name="8b3e" id="8b3e" class="graf graf--blockquote graf-after--p">My personal choice would still go to SageMaker, because unlike ECS and EKS, <strong class="markup--strong markup--blockquote-strong">SageMaker is built for Machine Learning only: the team is obsessed with simplifying and optimizing the service for ML users, and ML users only</strong>. No offence to the ECS and EKS teams, but their focus is different, as they have to accommodate literally every possible workload.</blockquote><p name="562a" id="562a" class="graf graf--p graf-after--blockquote">These services are all based on containers anyway, and if you’re able to run distributed TensorFlow with Horovod on Kubernetes, the SageMaker SDK will feel like a breeze! Give it a try and let me know what you think.</p><p name="55bc" id="55bc" class="graf graf--p graf-after--p">That’s the end of the second part. In the next post, we’ll talk about optimizing training from a framework perspective. Plenty more to come, we haven’t even talked about prediction yet!</p><p name="e56c" id="e56c" class="graf graf--p graf-after--p">As always, thanks for reading. Agree? Disagree? Great! Happy to discuss here or on <a href="https://twitter.com/julsimon" data-href="https://twitter.com/julsimon" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener" target="_blank">Twitter</a>.</p><figure name="1ef4" id="1ef4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Piks8Tu6xUYpF4DU" data-width="727" data-height="18" src="https://cdn-images-1.medium.com/max/800/0*Piks8Tu6xUYpF4DU"></figure><p name="9b5e" id="9b5e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Follow us on </strong><a href="https://twitter.com/joinfaun" data-href="https://twitter.com/joinfaun" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Twitter</strong></a><strong class="markup--strong markup--p-strong"> </strong>🐦<strong class="markup--strong markup--p-strong"> and </strong><a href="https://www.facebook.com/faun.dev/" data-href="https://www.facebook.com/faun.dev/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Facebook</strong></a><strong class="markup--strong markup--p-strong"> </strong>👥<strong class="markup--strong markup--p-strong"> and join our </strong><a href="https://www.facebook.com/groups/364904580892967/" data-href="https://www.facebook.com/groups/364904580892967/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Facebook Group</strong></a><strong class="markup--strong markup--p-strong"> </strong>💬<strong class="markup--strong markup--p-strong">.</strong></p><p name="c972" id="c972" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">To join our community Slack </strong>🗣️ <strong class="markup--strong markup--p-strong">and read our weekly Faun topics </strong>🗞️,<strong class="markup--strong markup--p-strong"> click here⬇</strong></p></div><div class="section-inner sectionLayout--fullWidth"><figure name="83d1" id="83d1" class="graf graf--figure graf--layoutFillWidth graf-after--p"><a href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmediumbanner" data-href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmediumbanner" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true"rel="noopener"target="_blank"><img class="graf-image" data-image-id="0*oSdFkACJxs5iy1oR" data-width="1600" data-height="275" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2560/0*oSdFkACJxs5iy1oR"></a></figure></div><div class="section-inner sectionLayout--insetColumn"><h4 name="3062" id="3062" class="graf graf--h4 graf-after--figure graf--trailing">If this post was helpful, please click the clap 👏 button below a few times to show your support for the author! ⬇</h4></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/80b0d1d7fc61"><time class="dt-published" datetime="2019-02-08T15:02:03.005Z">February 8, 2019</time></a>.</p><p><a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-2-80b0d1d7fc61" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>