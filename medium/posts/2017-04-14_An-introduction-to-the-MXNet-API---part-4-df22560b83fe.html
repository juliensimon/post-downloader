<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>An introduction to the MXNet API — part 4</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">An introduction to the MXNet API — part 4</h1>
</header>
<section data-field="subtitle" class="p-summary">
In part 3, we built and trained our first neural network. We now know enough to take on more advanced examples.
</section>
<section data-field="body" class="e-content">
<section name="3ed9" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="bc2f" id="bc2f" class="graf graf--h3 graf--leading graf--title">An introduction to the MXNet API — part 4</h3><p name="8749" id="8749" class="graf graf--p graf-after--h3">In <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" class="markup--anchor markup--p-anchor" target="_blank">part 3</a>, we built and trained our first neural network. We now know enough to take on more advanced examples.</p><p name="0d86" id="0d86" class="graf graf--p graf-after--p">State of the art Deep Learning models are insanely complex. They have <strong class="markup--strong markup--p-strong">hundreds of layers</strong> and take days — if not weeks — to train on vast amounts of data. Building and tuning these models requires a lot of expertise.</p><p name="7cce" id="7cce" class="graf graf--p graf-after--p">Fortunately, using these models is much simpler and only requires <strong class="markup--strong markup--p-strong">a few lines of code</strong>. In this article, we’re going to work with a pre-trained model for image classification called <strong class="markup--strong markup--p-strong">Inception v3</strong>.</p><h4 name="61f5" id="61f5" class="graf graf--h4 graf-after--p">Inception v3</h4><p name="fc21" id="fc21" class="graf graf--p graf-after--h4">Published in December 2015, <a href="https://arxiv.org/abs/1512.00567" data-href="https://arxiv.org/abs/1512.00567" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Inception v3</a> is an evolution of the <a href="https://arxiv.org/abs/1409.4842" data-href="https://arxiv.org/abs/1409.4842" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GoogleNet</a> model (which won the <a href="http://image-net.org/challenges/LSVRC/2014/" data-href="http://image-net.org/challenges/LSVRC/2014/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2014 ImageNet challenge</a>). We won’t go into the details of the research paper, but paraphrasing its conclusion, Inception v3 is <strong class="markup--strong markup--p-strong">15–25% more accurate</strong> than the best models available at the time, while being <strong class="markup--strong markup--p-strong">six times cheaper computationally</strong> and using at least <strong class="markup--strong markup--p-strong">five times less parameters</strong> (i.e. less RAM is required to use the model).</p><p name="14df" id="14df" class="graf graf--p graf-after--p">Quite a beast, then. So how do we put it to work?</p><h4 name="8683" id="8683" class="graf graf--h4 graf-after--p">The MXNet model zoo</h4><p name="b13f" id="b13f" class="graf graf--p graf-after--h4">The <a href="http://mxnet.io/model_zoo/" data-href="http://mxnet.io/model_zoo/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model zoo</a> is a collection of <strong class="markup--strong markup--p-strong">pre-trained models</strong> ready for use. You’ll find the <strong class="markup--strong markup--p-strong">model definition</strong>, the <strong class="markup--strong markup--p-strong">model parameters</strong> (i.e. the neuron weights) and instructions (maybe).</p><p name="10bd" id="10bd" class="graf graf--p graf-after--p">Let’s download the definition and the parameters (you may have to change the filename). Feel free to open the first file: you’ll see the definition of all the layers. The second one is a binary file, leave it alone ;)</p><pre name="b850" id="b850" class="graf graf--pre graf-after--p">$ wget <a href="http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json" data-href="http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json" class="markup--anchor markup--pre-anchor" rel="nofollow noopener noopener" target="_blank">http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json</a></pre><pre name="4578" id="4578" class="graf graf--pre graf-after--pre">$ wget <a href="http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params" data-href="http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params</a></pre><pre name="3286" id="3286" class="graf graf--pre graf-after--pre">$ mv Inception-BN-0126.params Inception-BN-0000.params</pre><p name="51c6" id="51c6" class="graf graf--p graf-after--pre">Since this model has been trained on the <a href="http://www.image-net.org/" data-href="http://www.image-net.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> data set, we also need to download the corresponding list of image <strong class="markup--strong markup--p-strong">categories</strong> (1000 of them).</p><pre name="c1c8" id="c1c8" class="graf graf--pre graf-after--p">$ wget <a href="http://data.dmlc.ml/models/imagenet/synset.txt" data-href="http://data.dmlc.ml/models/imagenet/synset.txt" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://data.dmlc.ml/models/imagenet/synset.txt</a></pre><pre name="17df" id="17df" class="graf graf--pre graf-after--pre">$ wc -l synset.txt<br>    1000 synset.txt</pre><pre name="ba49" id="ba49" class="graf graf--pre graf-after--pre">$ head -5 synset.txt<br>n01440764 tench, Tinca tinca<br>n01443537 goldfish, Carassius auratus<br>n01484850 great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias<br>n01491361 tiger shark, Galeocerdo cuvieri<br>n01494475 hammerhead, hammerhead shark</pre><p name="cf86" id="cf86" class="graf graf--p graf-after--pre">Ok, done. Now let’s get to work.</p><h4 name="ba0b" id="ba0b" class="graf graf--h4 graf-after--p">Loading the model</h4><p name="33c2" id="33c2" class="graf graf--p graf-after--h4">Here’s what we need to do:</p><ul class="postList"><li name="7c6f" id="7c6f" class="graf graf--li graf-after--p">load the model from its saved state: MXNet calls this a <strong class="markup--strong markup--li-strong">checkpoint</strong>. In return, we get the input <em class="markup--em markup--li-em">Symbol</em> and the model parameters.</li></ul><pre name="4276" id="4276" class="graf graf--pre graf-after--li">import mxnet as mx<br><br>sym, arg_params, aux_params = mx.model.load_checkpoint(&#39;Inception-BN&#39;, 0)</pre><ul class="postList"><li name="8a1c" id="8a1c" class="graf graf--li graf-after--pre">create a new <em class="markup--em markup--li-em">Module</em> and assign it the input <em class="markup--em markup--li-em">Symbol</em>. We could also a <em class="markup--em markup--li-em">context</em> parameter indicating where we want to run the model: the default value is <em class="markup--em markup--li-em">cpu(0)</em>, but we’d use <em class="markup--em markup--li-em">gpu(0)</em> to run this on a GPU.</li></ul><pre name="9319" id="9319" class="graf graf--pre graf-after--li">mod = mx.mod.Module(symbol=sym)</pre><ul class="postList"><li name="1ee7" id="1ee7" class="graf graf--li graf-after--pre">bind the input <em class="markup--em markup--li-em">Symbol</em> to input data. We’ll call it ‘data’ because that’s its name in the <strong class="markup--strong markup--li-strong">input layer</strong> of the network (look at the first few lines of the JSON file).</li><li name="767a" id="767a" class="graf graf--li graf-after--li">define the <strong class="markup--strong markup--li-strong">shape</strong> of ‘data’ as 1 x 3 x 224 x 224. Don’t panic ;) ‘224 x 224’ is the image resolution, that’s how the model was trained. ‘3’ is the number of channels : red, green and blue (in this order). ‘1’ is the batch size: we’ll predict one image at a time.</li></ul><pre name="0911" id="0911" class="graf graf--pre graf-after--li">mod.bind(for_training=False, data_shapes=[(&#39;data&#39;, (1,3,224,224))])</pre><ul class="postList"><li name="0ddf" id="0ddf" class="graf graf--li graf-after--pre">set the model parameters.</li></ul><pre name="35b2" id="35b2" class="graf graf--pre graf-after--li">mod.set_params(arg_params, aux_params)</pre><p name="7287" id="7287" class="graf graf--p graf-after--pre">That’s all it takes. Four lines of code! Now it’s take to push some data in there and see what happens. Well… not quite yet.</p><h4 name="61d4" id="61d4" class="graf graf--h4 graf-after--p">Preparing our data</h4><p name="82d0" id="82d0" class="graf graf--p graf-after--h4">Data preparation: making our life miserable since the Seventies… From relational databases to Machine Learning to Deep Learning, nothing has really changed in that respect. It’s boring but necessary. Let’s get it done.</p><p name="b0c2" id="b0c2" class="graf graf--p graf-after--p">Remember that the model expects a 4-dimension <em class="markup--em markup--p-em">NDArray</em> holding the red, green and blue channels of a single 224 x 224 image. We’re going to use the popular <a href="http://www.opencv.org" data-href="http://www.opencv.org" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenCV</a> library to build this <em class="markup--em markup--p-em">NDArray</em> from our input image. If you don’t have OpenCV installed, running “<em class="markup--em markup--p-em">pip install opencv-python</em>” should be enough in most cases :)</p><p name="c7e0" id="c7e0" class="graf graf--p graf-after--p">Here are the steps:</p><ul class="postList"><li name="0c55" id="0c55" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">read</strong> the image: this will return a <em class="markup--em markup--li-em">numpy</em> array shaped as (image height, image width, 3), with the three channels in <strong class="markup--strong markup--li-strong">BGR</strong> order (blue, green and red).</li></ul><pre name="d3ac" id="d3ac" class="graf graf--pre graf-after--li">img = cv2.imread(filename)</pre><ul class="postList"><li name="a7b7" id="a7b7" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">convert</strong> the image to <strong class="markup--strong markup--li-strong">RGB</strong>.</li></ul><pre name="801c" id="801c" class="graf graf--pre graf-after--li">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</pre><ul class="postList"><li name="eb69" id="eb69" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">resize</strong> the image to <strong class="markup--strong markup--li-strong">224 x 224</strong>.</li></ul><pre name="b1c4" id="b1c4" class="graf graf--pre graf-after--li">img = cv2.resize(img, (224, 224,))</pre><ul class="postList"><li name="95a4" id="95a4" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">reshape</strong> the array from (image height, image width, 3) to (3, image height, image width).</li></ul><pre name="abd9" id="abd9" class="graf graf--pre graf-after--li">img = np.swapaxes(img, 0, 2)<br>img = np.swapaxes(img, 1, 2)</pre><ul class="postList"><li name="d6b8" id="d6b8" class="graf graf--li graf-after--pre">add a <strong class="markup--strong markup--li-strong">fourth dimension</strong> and build the <em class="markup--em markup--li-em">NDArray</em></li></ul><pre name="3f91" id="3f91" class="graf graf--pre graf-after--li">img = img[np.newaxis, :]<br>array = mx.nd.array(img)</pre><pre name="ddcc" id="ddcc" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; print array.shape<br>(1L, 3L, 224L, 224L)</pre><p name="4291" id="4291" class="graf graf--p graf-after--pre">Dizzy? Let’s look at an example. Here’s our input picture.</p><figure name="845c" id="845c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sPdrfGtDd_6RQfYvD5qcyg.jpeg" data-width="448" data-height="336" src="https://cdn-images-1.medium.com/max/800/1*sPdrfGtDd_6RQfYvD5qcyg.jpeg"><figcaption class="imageCaption">Input picture 448x336 (Source: metaltraveller.com)</figcaption></figure><p name="f1c2" id="f1c2" class="graf graf--p graf-after--figure">Once processed, this picture has been resized and split into RGB channels stored in <em class="markup--em markup--p-em">array[0] </em>(<a href="https://gist.github.com/juliensimon/c62742b200396b4eadd8229a22c4cf0b" data-href="https://gist.github.com/juliensimon/c62742b200396b4eadd8229a22c4cf0b" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a> is the code used to generate the images below).</p><figure name="c90c" id="c90c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yqdl78KIugYepzJ4-lMY8g.jpeg" data-width="224" data-height="224" src="https://cdn-images-1.medium.com/max/800/1*yqdl78KIugYepzJ4-lMY8g.jpeg"><figcaption class="imageCaption">array[0][0] : 224x224 red channel</figcaption></figure><figure name="654d" id="654d" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*sitDwoAzPDLrav0dXQrcbA.jpeg" data-width="224" data-height="224" src="https://cdn-images-1.medium.com/max/800/1*sitDwoAzPDLrav0dXQrcbA.jpeg"><figcaption class="imageCaption">array[0][1] : 224x224 green channel</figcaption></figure><figure name="ba70" id="ba70" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*h1RyEPvd2fqIgd2jkfES-w.jpeg" data-width="224" data-height="224" src="https://cdn-images-1.medium.com/max/800/1*h1RyEPvd2fqIgd2jkfES-w.jpeg"><figcaption class="imageCaption">array[0][2] : 224x224 blue channel</figcaption></figure><p name="47d8" id="47d8" class="graf graf--p graf-after--figure">If batch size was higher than 1, then we would have a second image in <em class="markup--em markup--p-em">array[1]</em>, a third in <em class="markup--em markup--p-em">array[2]</em> and so on.</p><p name="b7ed" id="b7ed" class="graf graf--p graf-after--p">Was this fun or what? Now let’s predict!</p><h4 name="c119" id="c119" class="graf graf--h4 graf-after--p">Predicting</h4><p name="1ac8" id="1ac8" class="graf graf--p graf-after--h4">You may remember from <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" class="markup--anchor markup--p-anchor" target="_blank">part 3</a> that a <em class="markup--em markup--p-em">Module</em> object must feed data to a model in <strong class="markup--strong markup--p-strong">batches</strong>: the common way to do this is to use a <strong class="markup--strong markup--p-strong">data iterator</strong> (specifically, we used an <em class="markup--em markup--p-em">NDArrayIter</em> object).</p><p name="948a" id="948a" class="graf graf--p graf-after--p">Here, we’d like to predict a <strong class="markup--strong markup--p-strong">single</strong> image, so although we could use data iterator, it’d probably be overkill. Instead, we’re going to create a <strong class="markup--strong markup--p-strong">named tuple</strong>, called <em class="markup--em markup--p-em">Batch</em>, which will act as a fake iterator by returning our input <em class="markup--em markup--p-em">NDArray </em>when its <em class="markup--em markup--p-em">data</em> attribute is referenced.</p><pre name="125c" id="125c" class="graf graf--pre graf-after--p">from collections import namedtuple<br>Batch = namedtuple(&#39;Batch&#39;, [&#39;data&#39;])</pre><p name="9493" id="9493" class="graf graf--p graf-after--pre">Now we can pass this “batch” to the model and let it predict.</p><pre name="b399" id="b399" class="graf graf--pre graf-after--p">mod.forward(Batch([array]))</pre><p name="c8c8" id="c8c8" class="graf graf--p graf-after--pre">The model will output an <em class="markup--em markup--p-em">NDArray</em> holding the <strong class="markup--strong markup--p-strong">1000 probabilities</strong>, corresponding to the 1000 categories. It has only one line since batch size is equal to 1.</p><pre name="0fe4" id="0fe4" class="graf graf--pre graf-after--p">prob = mod.get_outputs()[0].asnumpy()</pre><pre name="6402" id="6402" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; prob.shape<br>(1, 1000)</pre><p name="8631" id="8631" class="graf graf--p graf-after--pre">Let’s turn this into an array with <em class="markup--em markup--p-em">squeeze</em>(). Then, using <em class="markup--em markup--p-em">argsort</em>(), we’re creating a second array holding the <strong class="markup--strong markup--p-strong">index</strong> of these probabilities sorted in <strong class="markup--strong markup--p-strong">descending order</strong>.</p><pre name="40c6" id="40c6" class="graf graf--pre graf-after--p">prob = np.squeeze(prob)</pre><pre name="d118" id="d118" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; prob.shape<br>(1000,)<br>&gt;&gt; prob<br>[  4.14978594e-08   1.31608676e-05   2.51907986e-05   2.24045834e-05<br>   2.30327873e-06   3.40798979e-05   7.41563645e-06   3.04062659e-08 <em class="markup--em markup--pre-em">etc.</em></pre><pre name="1669" id="1669" class="graf graf--pre graf-after--pre">sortedprob = np.argsort(prob)[::-1]</pre><pre name="6ee2" id="6ee2" class="graf graf--pre graf-after--pre">&gt;&gt; sortedprob.shape<br>(1000,)</pre><p name="8b91" id="8b91" class="graf graf--p graf-after--pre">According to the model, the most likely category for this picture is <strong class="markup--strong markup--p-strong">#546</strong> , with a probability of <strong class="markup--strong markup--p-strong">58%</strong>.</p><pre name="82c1" id="82c1" class="graf graf--pre graf-after--p">&gt;&gt; sortedprob<br>[546 819 862 818 542 402 650 420 983 632 733 644 513 875 776 917 795<br><em class="markup--em markup--pre-em">etc.<br></em>&gt;&gt; prob[546]<br>0.58039135</pre><p name="0915" id="0915" class="graf graf--p graf-after--pre">Let’s find the name of this category. Using the <em class="markup--em markup--p-em">synset.txt</em> file, we can build a <strong class="markup--strong markup--p-strong">list of categories</strong> and find the one at index 546.</p><pre name="26fa" id="26fa" class="graf graf--pre graf-after--p">synsetfile = open(&#39;synset.txt&#39;, &#39;r&#39;)<br>categorylist = []<br>for line in synsetfile:<br>  categorylist.append(line.rstrip())</pre><pre name="708d" id="708d" class="graf graf--pre graf-after--pre">&gt;&gt;&gt; categorylist[546]<br>&#39;n03272010 electric guitar&#39;</pre><p name="6531" id="6531" class="graf graf--p graf-after--pre">What about the second highest category?</p><pre name="1610" id="1610" class="graf graf--pre graf-after--p">&gt;&gt;&gt; prob[819]<br>0.27168664<br>&gt;&gt;&gt; categorylist[819]<br>&#39;n04296562 stage</pre><p name="32ce" id="32ce" class="graf graf--p graf-after--pre">That’s pretty good, don’t you think?</p><p name="391a" id="391a" class="graf graf--p graf-after--p">So there you go. Now you know how to use a <strong class="markup--strong markup--p-strong">pre-trained, state of the art model</strong> for image classification. All it took was <strong class="markup--strong markup--p-strong">4 lines of code</strong>… and the rest was just data preparation.</p><p name="ff06" id="ff06" class="graf graf--p graf-after--p graf--trailing">You’ll find the full code below. Have fun and stay tuned :D</p></div></div></section><section name="58d8" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="d037" id="d037" class="graf graf--p graf--leading">Next:</p><ul class="postList"><li name="2b71" id="2b71" class="graf graf--li graf-after--p"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" class="markup--anchor markup--li-anchor" target="_blank">Part 5</a>: More pre-trained models (VGG16 and ResNet-152)</li><li name="9a63" id="9a63" class="graf graf--li graf-after--li"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" data-href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" class="markup--anchor markup--li-anchor" target="_blank">Part 6</a>: Real-time object detection on a Raspberry Pi (and it speaks, too!)</li></ul><figure name="c2f1" id="c2f1" class="graf graf--figure graf--iframe graf-after--li graf--trailing"><script src="https://gist.github.com/juliensimon/4a5e999d9c851f0b036ab3870eccd59d.js"></script></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/df22560b83fe"><time class="dt-published" datetime="2017-04-14T23:14:25.249Z">April 14, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
