<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Building FPGA applications on AWS — and yes, for Deep Learning too</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Building FPGA applications on AWS — and yes, for Deep Learning too</h1>
</header>
<section data-field="subtitle" class="p-summary">
Field Programmable Gate Arrays (FPGA) are not shiny new technology: indeed, the first commercial product dates back to 1985. So how could…
</section>
<section data-field="body" class="e-content">
<section name="28be" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="70bc" id="70bc" class="graf graf--h3 graf--leading graf--title">Building FPGA applications on AWS — and yes, for Deep Learning too</h3><p name="1365" id="1365" class="graf graf--p graf-after--h3"><a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array" data-href="https://en.wikipedia.org/wiki/Field-programmable_gate_array" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Field Programmable Gate Arrays</a> (FPGA) are not shiny new technology: indeed, the first commercial product dates back to 1985. So how could they be relevant to a bleeding edge topic like Deep Learning? Then again, neural networks themselves go back to the late Forties, so... There might be something afoot, then. Read on :)</p><blockquote name="0708" id="0708" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p">“Grab onto my arm now. Hold tight. We are going into a number of dark places, but I think I know the way. Just don’t let go of my arm” — Stephen King</blockquote><h4 name="98d9" id="98d9" class="graf graf--h4 graf-after--pullquote">The case for non-CPU architectures</h4><p name="520b" id="520b" class="graf graf--p graf-after--h4">Until quite recently, the world of computing has been <strong class="markup--strong markup--p-strong">unequivocally</strong> ruled by CPUs. However, for a while now, there have been <a href="http://www.economist.com/technology-quarterly/2016-03-12/after-moores-law" data-href="http://www.economist.com/technology-quarterly/2016-03-12/after-moores-law" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ever-growing doubts</a> on how sustainable <a href="https://en.wikipedia.org/wiki/Moore%27s_law" data-href="https://en.wikipedia.org/wiki/Moore%27s_law" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Moore’s Law</a> really is.</p><figure name="fe90" id="fe90" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*za7UKYMSL8UqNUljul8l-A.png" data-width="511" data-height="351" src="https://cdn-images-1.medium.com/max/800/1*za7UKYMSL8UqNUljul8l-A.png"><figcaption class="imageCaption">Source: Intel</figcaption></figure><p name="3753" id="3753" class="graf graf--p graf-after--figure">To prevent chips from melting, clock speeds have been stagnating for years. In addition, even though lithography processes still manage to carve smaller and smaller features, we’re bound to hit <strong class="markup--strong markup--p-strong">technology limits</strong> rather sooner than later: in the latest Intel Skylake architecture, a transistor is 100 atoms wide.</p><blockquote name="1e0f" id="1e0f" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">The great man himself </em><a href="https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress" data-href="https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank"><em class="markup--em markup--blockquote-em">publicly declared</em></a><em class="markup--em markup--blockquote-em"> in 2015</em>: « I guess I see Moore’s Law dying here in the next decade or so, but that’s not surprising ».</blockquote><p name="0fe0" id="0fe0" class="graf graf--p graf-after--blockquote">Wait, there’s more.</p><h4 name="6e1e" id="6e1e" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">With new workloads come new requirements</strong></h4><p name="431a" id="431a" class="graf graf--p graf-after--h4">Another factor is helping foment the coming coup against King CPU: the emergence of <strong class="markup--strong markup--p-strong">new workloads</strong>, such as genomics, financial computing or Deep Learning. As it happens, these involve staggering amounts of <strong class="markup--strong markup--p-strong">mathematical computation</strong> which can greatly benefit from <strong class="markup--strong markup--p-strong">massive parallelism</strong> (think tens of thousands of cores). Sure, it’s definitely not impossible to achieve this with CPU-based architectures — here’s a <a href="https://aws.amazon.com/blogs/aws/natural-language-processing-at-clemson-university-1-1-million-vcpus-ec2-spot-instances/" data-href="https://aws.amazon.com/blogs/aws/natural-language-processing-at-clemson-university-1-1-million-vcpus-ec2-spot-instances/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">mind-boggling example</a> —but in recent years, a very serious contender has emerged: the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" data-href="https://en.wikipedia.org/wiki/Graphics_processing_unit" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Graphics Processing Unit</a> (GPU), spearheaded by Nvidia.</p><h4 name="71b1" id="71b1" class="graf graf--h4 graf-after--p">The King is dead, long live the King (?)</h4><p name="1325" id="1325" class="graf graf--p graf-after--h4">Equipped with <strong class="markup--strong markup--p-strong">thousands of floating-point cores</strong>, a typical GPU is indeed a formidable crunching machine, able to deliver proper hardware parallelism at scale. Soon enough, researchers have understood how these chips could be applied to <strong class="markup--strong markup--p-strong">Machine Learning</strong> and <strong class="markup--strong markup--p-strong">Deep Learning</strong> at scale.</p><blockquote name="36f8" id="36f8" class="graf graf--blockquote graf-after--p">Patrice Y. Simard, Dave Steinkrau, Ian Buck, “<a href="https://www.computer.org/csdl/proceedings/icdar/2005/2420/00/24201115-abs.html" data-href="https://www.computer.org/csdl/proceedings/icdar/2005/2420/00/24201115-abs.html" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Using GPUs for Machine Learning Algorithms</a>”, 2005</blockquote><blockquote name="baf3" id="baf3" class="graf graf--blockquote graf-after--blockquote">Dan C. Cireşan, Ueli Meier, Jonathan Masci, Luca M. Gambardella, Jürgen Schmidhuber, “<a href="https://arxiv.org/abs/1102.0183" data-href="https://arxiv.org/abs/1102.0183" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">High-Performance Neural Networks for Visual Object Classification</a>”, 2011</blockquote><p name="3281" id="3281" class="graf graf--p graf-after--blockquote">And thus began the Age of the GPU, leading up to the design of computing monsters such as the <a href="https://devblogs.nvidia.com/parallelforall/inside-volta/" data-href="https://devblogs.nvidia.com/parallelforall/inside-volta/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Nvidia V100</a>: 21.1 billion transistors, 815 square millimeters (1.36 square inch for my US friends), 5120 <strong class="markup--strong markup--p-strong">CUDA cores</strong>, 640 <strong class="markup--strong markup--p-strong">tensor cores</strong>. Surely, this should be enough for anyone… right?</p><h4 name="ba30" id="ba30" class="graf graf--h4 graf-after--p">A chink in the GPU armor?</h4><p name="4f80" id="4f80" class="graf graf--p graf-after--h4">When it comes to brute force computing powers, GPUs are unmatched.</p><figure name="3043" id="3043" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YleOt5F6fkFykx87FroKeQ.png" data-width="551" data-height="397" src="https://cdn-images-1.medium.com/max/800/1*YleOt5F6fkFykx87FroKeQ.png"><figcaption class="imageCaption">I’m pretty sure this guy is actually the Nvidia CEO.</figcaption></figure><p name="ba4a" id="ba4a" class="graf graf--p graf-after--figure">However, for some applications, they don’t deliver the most bang for your buck. Here are some reasons why you might not want to use a GPU:</p><ul class="postList"><li name="9e34" id="9e34" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Power consumption</strong> and maybe more importantly, <strong class="markup--strong markup--li-strong">power efficienc</strong>y (aka TeraOPS per Watt). This does matter a lot in the embedded and IoT worlds.</li><li name="d28a" id="d28a" class="graf graf--li graf-after--li">The need to process <strong class="markup--strong markup--li-strong">custom data types</strong> (not everything is a float)</li><li name="1d85" id="1d85" class="graf graf--li graf-after--li">Applications exhibiting <strong class="markup--strong markup--li-strong">irregular parallelism</strong> (alternating phases of sequential and parallel processing) or <strong class="markup--strong markup--li-strong">divergence</strong> (not all cores executing the same code at the same time).</li></ul><p name="21f4" id="21f4" class="graf graf--p graf-after--li">What about Deep Learning specifically? Of course, we know that GPUs are great for training: their massive parallelism allows them to crunch large data sets in reasonable time. To optimize throughput and put all these cores to good use, we don’t forward single samples through the model: we use <strong class="markup--strong markup--p-strong">batches</strong> of samples instead.</p><p name="0419" id="0419" class="graf graf--p graf-after--p">However, training is only half the story: what about <strong class="markup--strong markup--p-strong">inference</strong>? Well, it depends. If your application can live with the latency required to collect enough samples to forward a full batch, then you should be fine. If not, then you’ll have to run inference on single samples and it’s likely that throughput will suffer.</p><p name="dda5" id="dda5" class="graf graf--p graf-after--p">In order to get the best inference performance, the logical step would be to use a custom chip. For decades, the choice has been pretty simple: either build an Application Specific Integrated Circuit (<strong class="markup--strong markup--p-strong">ASIC</strong>) or use an <strong class="markup--strong markup--p-strong">FPGA</strong>.</p><h4 name="f5de" id="f5de" class="graf graf--h4 graf-after--p">The ASIC way</h4><p name="f3e9" id="f3e9" class="graf graf--p graf-after--h4">An ASIC is a <strong class="markup--strong markup--p-strong">fully custom</strong> design, which is mass-produced and deployed in devices. Obviously, you get to tweak it and optimise it in the way that works best for your application: best performance, best power efficiency, etc. However, designing, producing and deploying an ASIC is a long, expensive and risky process. You’ll be lucky to complete it in less than 18 months.</p><p name="cb39" id="cb39" class="graf graf--p graf-after--p">This is the route that Google took for their TPU chip. They did it in <strong class="markup--strong markup--p-strong">15 months</strong>, which is impressive indeed. Just wonder how long it would take you.</p><blockquote name="681c" id="681c" class="graf graf--blockquote graf-after--p">Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, “<a href="https://arxiv.org/abs/1704.04760" data-href="https://arxiv.org/abs/1704.04760" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">In Datacenter Performance Analysis of a Tensor Processing Unit</a>”, 2017</blockquote><p name="a92b" id="a92b" class="graf graf--p graf-after--blockquote">Of course, ASICs are inflexible: if your application requirements change significantly, you have to start all over again.</p><h4 name="1383" id="1383" class="graf graf--h4 graf-after--p">The FPGA way</h4><p name="0898" id="0898" class="graf graf--p graf-after--h4">As their name implies, FPGAs are (re)programmable logic circuits. A typical FPGA includes hundreds of thousands and sometimes millions of <strong class="markup--strong markup--p-strong">logic cells</strong>, thousands of Digital Signal Processing (<strong class="markup--strong markup--p-strong">DSP</strong>)“slices” as well as very fast <strong class="markup--strong markup--p-strong">on-chip memory</strong>.</p><figure name="fd44" id="fd44" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qYBIbuuhH8I1VuDQ2-Fi2Q.png" data-width="586" data-height="346" src="https://cdn-images-1.medium.com/max/800/1*qYBIbuuhH8I1VuDQ2-Fi2Q.png"><figcaption class="imageCaption">Source: embedded-vision.com</figcaption></figure><p name="0759" id="0759" class="graf graf--p graf-after--figure">Not everyone enjoys digital circuits and Boolean algebra, so let’s keep this simple: FPGAs are <strong class="markup--strong markup--p-strong">Lego digital architecture</strong>. By mixing and matching the right logic blocks, a system designer armed with the right tools can pretty much implement <strong class="markup--strong markup--p-strong">anything</strong>… even an <a href="http://www.cs.columbia.edu/~sedwards/apple2fpga/" data-href="http://www.cs.columbia.edu/~sedwards/apple2fpga/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Apple ][</a> :)</p><figure name="99b5" id="99b5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*KZMwL5tU-PHgtx7nTHHoGQ.png" data-width="440" data-height="378" src="https://cdn-images-1.medium.com/max/800/1*KZMwL5tU-PHgtx7nTHHoGQ.png"><figcaption class="imageCaption">Source: bober-optosensorik.de</figcaption></figure><h4 name="4a9f" id="4a9f" class="graf graf--h4 graf-after--figure">Building FPGA applications</h4><p name="4175" id="4175" class="graf graf--p graf-after--h4">Historically, building FPGA applications has required the purchase of costly <strong class="markup--strong markup--p-strong">software and hardware tools</strong> in order to design, simulate, debug, synthesize and route custom logic designs. Let’s face it, they made it hard to scale engineering efforts.</p><p name="c501" id="c501" class="graf graf--p graf-after--p">Designing custom logic for FPGAs also required the mastery of esoteric languages like <strong class="markup--strong markup--p-strong">VHDL</strong> or <strong class="markup--strong markup--p-strong">Verilog</strong>… and your computer desktop would look something like this. Definitely not for everyone (including myself).</p><figure name="92b0" id="92b0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HU6bPUYLjVNUCwuZ758PXg.png" data-width="545" data-height="362" src="https://cdn-images-1.medium.com/max/800/1*HU6bPUYLjVNUCwuZ758PXg.png"><figcaption class="imageCaption">Source: AWS</figcaption></figure><p name="c1e6" id="c1e6" class="graf graf--p graf-after--figure">Fortunately, developers now have the option to build FPGA applications in <strong class="markup--strong markup--p-strong">C/C++</strong> thanks to the <a href="https://www.xilinx.com/products/design-tools/software-zone/sdaccel.html" data-href="https://www.xilinx.com/products/design-tools/software-zone/sdaccel.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SDAccel</a> environment and <strong class="markup--strong markup--p-strong">OpenCL</strong>. The programming model won’t be unfamiliar to CUDA developers :)</p><figure name="eb14" id="eb14" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-A2gBeSfVSDmDRFO5Tkd-g.png" data-width="700" data-height="412" src="https://cdn-images-1.medium.com/max/800/1*-A2gBeSfVSDmDRFO5Tkd-g.png"><figcaption class="imageCaption">Source: Xilinx</figcaption></figure><h4 name="539a" id="539a" class="graf graf--h4 graf-after--figure">Deploying FPGA applications on AWS</h4><p name="25ca" id="25ca" class="graf graf--p graf-after--h4">About a year ago, AWS introduced <a href="https://aws.amazon.com/ec2/instance-types/f1/" data-href="https://aws.amazon.com/ec2/instance-types/f1/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon EC2 F1 instances</a>.</p><figure name="c1dc" id="c1dc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-XETskDJkRq8QBkLV_B0uA.png" data-width="383" data-height="165" src="https://cdn-images-1.medium.com/max/800/1*-XETskDJkRq8QBkLV_B0uA.png"><figcaption class="imageCaption">Source: AWS</figcaption></figure><p name="9f62" id="9f62" class="graf graf--p graf-after--figure">They rely on the <strong class="markup--strong markup--p-strong">Xilinx Ultrascale+ VU9P chip</strong>. Here are some of the specs (<a href="https://www.xilinx.com/support/documentation/selection-guides/ultrascale-plus-fpga-product-selection-guide.pdf" data-href="https://www.xilinx.com/support/documentation/selection-guides/ultrascale-plus-fpga-product-selection-guide.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">PDF</a>): over 2.5 million System Logic Cells (<a href="https://www.xilinx.com/support/documentation/user_guides/ug574-ultrascale-clb.pdf" data-href="https://www.xilinx.com/support/documentation/user_guides/ug574-ultrascale-clb.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">specs</a> — PDF) and 6,840 DSP slices (<a href="https://www.xilinx.com/support/documentation/user_guides/ug579-ultrascale-dsp.pdf" data-href="https://www.xilinx.com/support/documentation/user_guides/ug579-ultrascale-dsp.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">specs</a> — PDF). Yes, it’s a beast!</p><p name="0adc" id="0adc" class="graf graf--p graf-after--p">In order to simplify FPGA development, AWS also provides an <a href="https://aws.amazon.com/marketplace/pp/B06VVYBLZZ" data-href="https://aws.amazon.com/marketplace/pp/B06VVYBLZZ" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">FPGA Developer AMI</a> coming with the full Xilinx SDx 2017.1 <strong class="markup--strong markup--p-strong">tool suite</strong>… and a <strong class="markup--strong markup--p-strong">free license</strong> :) The AMI also includes the AWS <strong class="markup--strong markup--p-strong">SDK</strong> and <strong class="markup--strong markup--p-strong">HDK</strong> to help you build and manage your FPGA images: both are Open Source and available on <a href="https://github.com/aws/aws-fpga" data-href="https://github.com/aws/aws-fpga" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a>.</p><p name="3e72" id="3e72" class="graf graf--p graf-after--p">The overall process would look something like this:</p><ul class="postList"><li name="7557" id="7557" class="graf graf--li graf-after--p">Using the FPGA Developer AMI on a compute-optimized instance (such as a c4), design, simulate and build the <strong class="markup--strong markup--li-strong">Amazon FPGA Image</strong> (AFI).</li><li name="471f" id="471f" class="graf graf--li graf-after--li">On an <a href="https://aws.amazon.com/ec2/instance-types/f1/" data-href="https://aws.amazon.com/ec2/instance-types/f1/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">EC2 F1 instance</a>, use the AWS FPGA SDK to load the AFI and access it from a <strong class="markup--strong markup--li-strong">host application</strong> running on the CPU.</li></ul><figure name="2165" id="2165" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*IXYJOEu7RAdQQwKx3lNckg.png" data-width="1236" data-height="668" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*IXYJOEu7RAdQQwKx3lNckg.png"><figcaption class="imageCaption">Source: AWS</figcaption></figure><h4 name="ee5a" id="ee5a" class="graf graf--h4 graf-after--figure">Building Neural Networks with FPGAs</h4><p name="8f5b" id="8f5b" class="graf graf--p graf-after--h4">At the core of Neural Networks lies the “<strong class="markup--strong markup--p-strong">Multiply and Accumulate</strong>” operation, where we multiply inputs by their respective weights and add all the results together. This can be easily implemented using a DSP slice. Yes, I know its a very simple example, but more complex operations like convolution or pooling could be implemented as well.</p><figure name="1684" id="1684" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*M_5t0hvHtSS_mOOjTbK5uw.png" data-width="1258" data-height="568" src="https://cdn-images-1.medium.com/max/800/1*M_5t0hvHtSS_mOOjTbK5uw.png"><figcaption class="imageCaption">Source: “FPGA Implementations of Neural Networks”, Springer, 2006</figcaption></figure><p name="15b1" id="15b1" class="graf graf--p graf-after--figure">Of course, modern FPGAs have tons of gates and they’re able to support very large models. However, in the interest of speed, latency and power consumption, it would make sense to try to <strong class="markup--strong markup--p-strong">minimize the number of gates</strong>.</p><h4 name="c030" id="c030" class="graf graf--h4 graf-after--p">Optimizing Deep Learning models for FPGAs</h4><p name="8a60" id="8a60" class="graf graf--p graf-after--h4">There’s a lot of ongoing research to <strong class="markup--strong markup--p-strong">simplify</strong> and <strong class="markup--strong markup--p-strong">shrink</strong> Deep Learning models with <strong class="markup--strong markup--p-strong">minimal</strong> loss of accuray. The three most popular techniques are:</p><p name="a5bd" id="a5bd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Quantization</strong>, i.e. using <strong class="markup--strong markup--p-strong">integer weights</strong> (8, 4 or even 2-bit) instead of 32-bit floats. Less power is required: less gates are required to implement the model, and integer operations are cheaper than floating-point operations. Less memory is also required, as we save memory and shrink model size.</p><p name="6b97" id="6b97" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Pruning</strong>, i.e. <strong class="markup--strong markup--p-strong">removing connections</strong> that play little or no role in predicting successfully. Computation speed goes up, latency goes down. Less memory is required, as we save memory and reduce model size.</p><p name="cf69" id="cf69" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Compression</strong>, i.e. <strong class="markup--strong markup--p-strong">encoding weights</strong>, as they’re now integer-based and exhibit a smaller set of possible values. Less memory is required, as we save memory and reduce model size even further.</p><p name="5066" id="5066" class="graf graf--p graf-after--p">As a bonus, models may shrink so much that <strong class="markup--strong markup--p-strong">on-chip SRAM</strong> could become a viable option. This would help in saving even more power (as SRAM is much <strong class="markup--strong markup--p-strong">more efficient</strong> than DRAM) as well as speeding up computation (as on-chip RAM is always <strong class="markup--strong markup--p-strong">faster</strong> to access than off-chip RAM).</p><p name="ed99" id="ed99" class="graf graf--p graf-after--p">Using these techniques and more, researchers have obtained <strong class="markup--strong markup--p-strong">spectacular</strong> results.</p><blockquote name="8423" id="8423" class="graf graf--blockquote graf-after--p">Song Han et al, “<a href="https://arxiv.org/abs/1510.00149" data-href="https://arxiv.org/abs/1510.00149" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a>”, 2016</blockquote><ul class="postList"><li name="cc29" id="cc29" class="graf graf--li graf-after--blockquote">Optimizing CNNs on CPU and GPU</li><li name="b5ba" id="b5ba" class="graf graf--li graf-after--li">AlexNet <strong class="markup--strong markup--li-strong">35x</strong> smaller, VGG-16 <strong class="markup--strong markup--li-strong">49x</strong> smaller</li><li name="27aa" id="27aa" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">3x to 4x</strong> speedup, <strong class="markup--strong markup--li-strong">3x to 7x</strong> more energy-efficient</li><li name="ede1" id="ede1" class="graf graf--li graf-after--li">No loss of accuracy</li></ul><blockquote name="2306" id="2306" class="graf graf--blockquote graf-after--li">Song Han et al, “<a href="https://arxiv.org/abs/1612.00694" data-href="https://arxiv.org/abs/1612.00694" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA</a>”, 2017</blockquote><ul class="postList"><li name="67a0" id="67a0" class="graf graf--li graf-after--blockquote">Optimizing LSTM networks on Xilinx FPGA</li><li name="2871" id="2871" class="graf graf--li graf-after--li">FPGA vs CPU: <strong class="markup--strong markup--li-strong">43x</strong> faster, <strong class="markup--strong markup--li-strong">40x</strong> more energy-efficient</li><li name="fd0c" id="fd0c" class="graf graf--li graf-after--li">FPGA vs GPU: <strong class="markup--strong markup--li-strong">3x</strong> faster, <strong class="markup--strong markup--li-strong">11.5x</strong> more energy-efficient</li></ul><blockquote name="bb07" id="bb07" class="graf graf--blockquote graf-after--li">Nurvitadhi et al, “<a href="http://jaewoong.org/pubs/fpga17-next-generation-dnns.pdf" data-href="http://jaewoong.org/pubs/fpga17-next-generation-dnns.pdf" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?</a>”, 2017</blockquote><ul class="postList"><li name="ddfb" id="ddfb" class="graf graf--li graf-after--blockquote">Optimizing CNNs on Intel FPGA</li><li name="2a66" id="2a66" class="graf graf--li graf-after--li">FPGA vs GPU: <strong class="markup--strong markup--li-strong">60%</strong> faster, <strong class="markup--strong markup--li-strong">2.3x</strong> more energy-efficient</li><li name="2d23" id="2d23" class="graf graf--li graf-after--li">&lt;1% loss of accuracy</li></ul><h4 name="26d2" id="26d2" class="graf graf--h4 graf-after--li">The next step: Deep Learning hardware</h4><p name="9bc1" id="9bc1" class="graf graf--p graf-after--h4">Some of you may still remember the ill-fated <a href="https://en.wikipedia.org/wiki/Lisp_machine" data-href="https://en.wikipedia.org/wiki/Lisp_machine" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LISP machines</a> and shiver at the thought of AI hardware. However, times have changed and researchers are moving fast here as well.</p><blockquote name="626a" id="626a" class="graf graf--blockquote graf-after--p">Song Han, “<a href="http://isfpga.org/slides/D1_S1_Tutorial.pdf" data-href="http://isfpga.org/slides/D1_S1_Tutorial.pdf" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Deep Learning Tutorial and Recent Trends</a>” (PDF), 2017</blockquote><p name="75b2" id="75b2" class="graf graf--p graf-after--blockquote">The topic picked up even more speed when Nvidia recently announced a new initiative, <a href="http://nvdla.org/" data-href="http://nvdla.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Nvidia Hardware for Deep Learning</a>. In a nutshell, this includes <a href="https://github.com/nvdla/" data-href="https://github.com/nvdla/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Open Source hardware blocks</a> implemented in Verilog that may be used to build Deep Learning accelerators for IoT applications:</p><ul class="postList"><li name="5e51" id="5e51" class="graf graf--li graf-after--p">Convolution Core — optimized high-performance <strong class="markup--strong markup--li-strong">convolution</strong> engine</li><li name="a33a" id="a33a" class="graf graf--li graf-after--li">Single Data Processor — single-point lookup engine for <strong class="markup--strong markup--li-strong">activation</strong> functions</li><li name="2905" id="2905" class="graf graf--li graf-after--li">Planar Data Processor — planar averaging engine for <strong class="markup--strong markup--li-strong">pooling</strong></li><li name="e0c6" id="e0c6" class="graf graf--li graf-after--li">Channel Data Processor — multi-channel averaging engine for <strong class="markup--strong markup--li-strong">normalization</strong> functions</li><li name="d6c2" id="d6c2" class="graf graf--li graf-after--li">Dedicated Memory and Data Reshape Engines — memory-to-memory <br> transformation acceleration for <strong class="markup--strong markup--li-strong">tensor reshape</strong> and <strong class="markup--strong markup--li-strong">copy</strong> operations.</li></ul><blockquote name="8a68" id="8a68" class="graf graf--blockquote graf-after--li">Although clearly targeted at IoT devices, these building blocks can be simulated and deployed to F1 instances :)</blockquote><p name="35ea" id="35ea" class="graf graf--p graf-after--blockquote">This FPGA-based initiative is coming from the company that brought us GPUs in the first place, which should definitely raise a few eyebrows. In my humble opinion, <strong class="markup--strong markup--p-strong">we should definitely pay attention</strong>: no one would know more than Nvidia about GPUs strengths and weaknesses and about speeding up Deep Learning computations. A exciting and clever move indeed.</p><blockquote name="872e" id="872e" class="graf graf--pullquote graf--startsWithDoubleQuote graf-after--p">“What now? Let me tell you what now”</blockquote><blockquote name="0743" id="0743" class="graf graf--pullquote graf-after--pullquote">— Marcellus (Pulp Fiction)</blockquote><p name="944f" id="944f" class="graf graf--p graf-after--pullquote">I don’t have a crystal ball, but here are a few closing predictions based on extensive analysis of my gut feelings :-P</p><ul class="postList"><li name="21cb" id="21cb" class="graf graf--li graf-after--p">Deep Learning is shaping up to be a <strong class="markup--strong markup--li-strong">major workload</strong> for public clouds and IoT. No single hardware architecture can win both battles.</li><li name="5291" id="5291" class="graf graf--li graf-after--li">Much more infrastructure will be used for <strong class="markup--strong markup--li-strong">inference</strong> than for training (I’d expect multiple orders of magnitude). Again, no single hardware architecture can win both battles.</li><li name="8957" id="8957" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Cloud-based GPUs</strong> will dominate <strong class="markup--strong markup--li-strong">training</strong> for the foreseeable future.</li><li name="15aa" id="15aa" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Cloud-based inference</strong> will be the mother of all battles. ASICs look good, but I just don’t see Nvidia letting go. Grab some popcorn and wait for the showdown.</li><li name="bf82" id="bf82" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">CPU inference</strong> will still be a thing for smaller IoT devices, which is why software acceleration solutions like <a href="https://software.intel.com/en-us/mkl" data-href="https://software.intel.com/en-us/mkl" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Intel MKL</a> or <a href="https://github.com/Maratyszcza/NNPACK" data-href="https://github.com/Maratyszcza/NNPACK" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">NNPACK</a> are important.</li><li name="9b4d" id="9b4d" class="graf graf--li graf-after--li">For larger IoT devices, we may witness an <strong class="markup--strong markup--li-strong">inference-driven FPGA renaissance</strong>. Current GPUs are too power-hungry and ASICs too inflexible.</li></ul><p name="1155" id="1155" class="graf graf--p graf-after--li">Well, we made it. No code this time, but I hope you still enjoyed this :)</p><p name="9bf6" id="9bf6" class="graf graf--p graf-after--p graf--trailing">Thanks for reading.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/643097257192"><time class="dt-published" datetime="2017-10-09T22:10:32.809Z">October 9, 2017</time></a>.</p><p><a href="https://medium.com/@julsimon/building-fpga-applications-on-aws-and-yes-for-deep-learning-too-643097257192" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
