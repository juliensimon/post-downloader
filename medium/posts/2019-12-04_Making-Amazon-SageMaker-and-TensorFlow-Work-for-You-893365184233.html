<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Making Amazon SageMaker and TensorFlow Work for You</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Making Amazon SageMaker and TensorFlow Work for You</h1>
</header>
<section data-field="subtitle" class="p-summary">
This is a guest post by Chaim Rand, Machine Learning Algorithm Developer at Mobileye. It builds upon the AIM410R session at AWS re:Invent…
</section>
<section data-field="body" class="e-content">
<section name="b462" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6866" id="6866" class="graf graf--h3 graf--leading graf--title">Making Amazon SageMaker and TensorFlow Work for You — Mobileye guest post</h3><p name="1373" id="1373" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">This is a guest post by </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Chaim Rand</em></strong><em class="markup--em markup--p-em">, </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Machine Learning Algorithm Developer</em></strong><em class="markup--em markup--p-em"> at </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Mobileye</em></strong><em class="markup--em markup--p-em">. It builds upon the </em><a href="https://www.youtube.com/watch?v=iW0RASdjnOk" data-href="https://www.youtube.com/watch?v=iW0RASdjnOk" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">AIM410R session</em></a><em class="markup--em markup--p-em"> at AWS re:Invent 2019. You can also read </em><a href="https://medium.com/@julsimon/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c" data-href="https://medium.com/@julsimon/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">part 2</em></a><em class="markup--em markup--p-em"> and </em><a href="https://towardsdatascience.com/tensorflow-performance-analysis-314b56dceb59" data-href="https://towardsdatascience.com/tensorflow-performance-analysis-314b56dceb59" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">part 3</em></a><em class="markup--em markup--p-em"> for more!</em></p><figure name="f443" id="f443" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/iW0RASdjnOk?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><h3 name="0cbc" id="0cbc" class="graf graf--h3 graf-after--figure">Abstract</h3><p name="0181" id="0181" class="graf graf--p graf-after--h3">Under the surface of <a href="https://www.mobileye.com" data-href="https://www.mobileye.com" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Mobileye</a>’s (officially known as “Mobileye, an <a href="https://www.intel.com" data-href="https://www.intel.com" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Intel</a> Company”) life-saving driving assistant products are cutting edge AI technologies. At any given time at Mobileye, we may be training scores of Deep Neural Networks (DNN) targeted for the next generation of <a href="https://www.mobileye.com/our-technology/adas/" data-href="https://www.mobileye.com/our-technology/adas/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Advanced Driving Assistant</a>, <a href="https://www.mobileye.com/future-of-mobility/history-autonomous-driving/" data-href="https://www.mobileye.com/future-of-mobility/history-autonomous-driving/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Autonomous Vehicle</a>, and <a href="https://www.mobileye.com/our-technology/rem/" data-href="https://www.mobileye.com/our-technology/rem/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Road Experience Management</a> products.</p><figure name="1908" id="1908" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/qfJbkuDY1xI?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="8d6e" id="8d6e" class="graf graf--p graf-after--figure">This requires vast amounts infrastructure that is fast, flexible, scalable, and secure. Enter <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a>. In this post, I will share some of the details of how we adapted one of our DNNs to SageMaker’s <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode </a>and the surprising ways in which this accelerated the development cycle.</p><h3 name="00ef" id="00ef" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Prelude</strong></h3><p name="c5f5" id="c5f5" class="graf graf--p graf-after--h3">This post is about the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Amazon SageMaker</a> service. It is an exciting story about how my team and I ported one of our Deep Learning (DL) training flows to <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sagemaker</a>, the challenges we encountered along the way, how we overcame them, and the benefits we discovered along the way. It is a story of courage, creativity, and, above all, perseverance.</p><div name="b7aa" id="b7aa" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/aws/sagemaker/" data-href="https://aws.amazon.com/blogs/aws/sagemaker/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/aws/sagemaker/"><strong class="markup--strong markup--mixtapeEmbed-strong">Amazon SageMaker - Accelerating Machine Learning | Amazon Web Services</strong><br><em class="markup--em markup--mixtapeEmbed-em">Machine Learning is a pivotal technology for many startups and enterprises. Despite decades of investment and…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/aws/sagemaker/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="9a3fb5d024e299b2c07810edc02dcb2a" data-thumbnail-img-id="0*71VPT8lJoGP6Z38A" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*71VPT8lJoGP6Z38A);"></a></div><h3 name="93c3" id="93c3" class="graf graf--h3 graf-after--mixtapeEmbed"><strong class="markup--strong markup--h3-strong">Chapter 1: Introduction</strong></h3><p name="2319" id="2319" class="graf graf--p graf-after--h3">The audience I am targeting includes:</p><ol class="postList"><li name="6f93" id="6f93" class="graf graf--li graf-after--p">Developers trying to decide whether <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker</a> is right for them or their company.</li><li name="f25d" id="f25d" class="graf graf--li graf-after--li">Developers who have been tasked with porting their training code to <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker</a> and don’t know where to begin or what to expect.</li><li name="5794" id="5794" class="graf graf--li graf-after--li">Developers who are knee deep in <a href="https://aws.amazon.com/sagemaker" data-href="https://aws.amazon.com/sagemaker" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker</a>.</li></ol><p name="a597" id="a597" class="graf graf--p graf-after--li">The message that I want to deliver to you today, whichever group you may fall into is that… <strong class="markup--strong markup--p-strong">everything is going to be okay</strong>.</p><p name="f0c5" id="f0c5" class="graf graf--p graf-after--p">Let me start by saying that I am NOT an AWS guy. On the one hand, what that means is that I don’t speak on behalf of AWS. Any allusions that I may make regarding performance or cost are based solely on my own experience and should be verified by your own AWS representative. On the other hand, that means that I am one of <strong class="markup--strong markup--p-strong">you</strong>. I am here for <strong class="markup--strong markup--p-strong">you</strong>. Feel free drop me a line sharing your AWS woes, or as I like to call them… your wAWS. I promise to be supportive.</p><p name="20c2" id="20c2" class="graf graf--p graf-after--p">Here are some of the things I like about <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a>:</p><ul class="postList"><li name="8a1b" id="8a1b" class="graf graf--li graf-after--p">It offers a <strong class="markup--strong markup--li-strong">secure and scalable environment</strong> in which one can essentially spin up as many training sessions as they want.</li><li name="26b3" id="26b3" class="graf graf--li graf-after--li">It enables one to freely choose between <strong class="markup--strong markup--li-strong">many different types of </strong><a href="https://aws.amazon.com/sagemaker/pricing/instance-types/" data-href="https://aws.amazon.com/sagemaker/pricing/instance-types/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">training instances</strong></a> with ease.</li><li name="ff49" id="ff49" class="graf graf--li graf-after--li">It enables feeding one’s training data <strong class="markup--strong markup--li-strong">directly from </strong><a href="https://aws.amazon.com/s3/" data-href="https://aws.amazon.com/s3/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">Amazon S3</strong></a>, essentially removing any storage space constraints.</li><li name="9ce0" id="9ce0" class="graf graf--li graf-after--li">It enables one to <strong class="markup--strong markup--li-strong">decouple the storage</strong> of their training data from the actual training execution.</li><li name="7fcd" id="7fcd" class="graf graf--li graf-after--li">It enables one to run their <strong class="markup--strong markup--li-strong">entire development pipeline in the cloud</strong>, from data collection and creation all the way to quantization and deployment.</li></ul><p name="145e" id="145e" class="graf graf--p graf-after--li">However, as with any other new framework, adopting <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> might require some patience, resilience, and effort.</p><blockquote name="4128" id="4128" class="graf graf--pullquote graf-after--p">Make no mistake, the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">SageMaker</a> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">documentation</a> is quite good. The <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_Operations.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_Operations.html" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">APIs</a> are pretty straightforward and there are code samples demonstrating a wide variety of use cases.</blockquote><p name="98c7" id="98c7" class="graf graf--p graf-after--pullquote">At the end of the day, adopting our flow to <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> did not require much heavy lifting. While we did face some challenges along the way, we overcame them and gained more and more confidence that <strong class="markup--strong markup--p-strong">Sagemaker could work for us</strong>. Over the next few minutes I hope to pass this confidence to you. Our story is based on using <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> with the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a><strong class="markup--strong markup--p-strong">, </strong>but I believe that most of what I have to say carries over to any solution based on <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a> usage. When you are using large data sets, using <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a> is the “right” way to train on <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a>. Of course, this is just my opinion… but it’s true. (Legal disclaimer, what I mean is that ‘I think it’s true’, but it really is!).</p><p name="6d3a" id="6d3a" class="graf graf--p graf-after--p graf--trailing">My story is told based on <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> version 1.13.1 and <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> version 1.23. To the best of my knowledge, my comments are correct as of today (November 2019). Naturally, things may have changed since then.</p></div></div></section><section name="0d54" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="838a" id="838a" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 2: Sagemaker Pipe Mode</strong></h3><p name="b884" id="b884" class="graf graf--p graf-after--h3">What is <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a> and what is it good for?</p><h4 name="ae7c" id="ae7c" class="graf graf--h4 graf-after--p">An introduction to Pipe Mode</h4><p name="83fc" id="83fc" class="graf graf--p graf-after--h4">Pipe input mode is one of the main features offered by the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> training environment, and it is said to enable meaningful reductions in both train time and cost. <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a> is a mechanism (based on Linux pipes) for <strong class="markup--strong markup--p-strong">streaming your training data directly from </strong><a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon S3</strong></a><strong class="markup--strong markup--p-strong"> storage to your training instance</strong>.</p><div name="f62a" id="f62a" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/"><strong class="markup--strong markup--mixtapeEmbed-strong">Accelerate model training using faster Pipe mode on Amazon SageMaker | Amazon Web Services</strong><br><em class="markup--em markup--mixtapeEmbed-em">Amazon SageMaker now comes with a faster Pipe mode implementation, significantly accelerating the speeds at which data…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2cf2b7cb61b2c3a6ad2c244e6494c3ed" data-thumbnail-img-id="0*MFCqvq1l_-VV6I86" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*MFCqvq1l_-VV6I86);"></a></div><p name="f4a6" id="f4a6" class="graf graf--p graf-after--mixtapeEmbed">The previous way of doing this was to download all of the data from <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">S3</a> to the training instance. This had to be done each time you wanted to spin up a new training session. When working with large data sets,say 10s or even 100s of Terabytes, this would cause a significant delay to the training start time. You may also incur significant storage costs, again, for each training instance.</p><p name="9857" id="9857" class="graf graf--p graf-after--p"><a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a> avoids this by essentially feeding the data directly to the algorithm as it is needed. This means that <strong class="markup--strong markup--p-strong">training can start as soon as the pipe is opened</strong> and no local storage is required.</p><figure name="bf9e" id="bf9e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Pq_Q8fTHAssGKp1rwVK99g.png" data-width="3110" data-height="1192" src="https://cdn-images-1.medium.com/max/800/1*Pq_Q8fTHAssGKp1rwVK99g.png"></figure><p name="ae9c" id="ae9c" class="graf graf--p graf-after--figure">In particular, this has the effect of <strong class="markup--strong markup--p-strong">removing any limitations on the size of your data set</strong>. You can store all of your data on <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">S3</a>, with its virtually limitless storage capacity, and not have to worry about local storage constraints or costs.</p><blockquote name="6691" id="6691" class="graf graf--pullquote graf-after--p">This means that your <strong class="markup--strong markup--pullquote-strong">data storage and training environment are now decoupled</strong>. You can spin up as many training instances as you’d like and have them all point to same data storage location in <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">S3</a>.</blockquote><pre name="9ceb" id="9ceb" class="graf graf--pre graf-after--pullquote">from sagemaker.tensorflow import TensorFlow</pre><pre name="8070" id="8070" class="graf graf--pre graf-after--pre">tensorflow = TensorFlow(<br> entry_point=’myscript.py’, <br> input_mode=’Pipe’,<br> …)</pre><pre name="9a3e" id="9a3e" class="graf graf--pre graf-after--pre">train_data = ‘s3://sagemaker-path-to-train-data’</pre><pre name="f233" id="f233" class="graf graf--pre graf-after--pre">tensorflow.fit({‘train’:train_data})</pre><p name="9a57" id="9a57" class="graf graf--p graf-after--pre">There is one significant drawback to <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a>. In our pre-<a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> work flow, we were accustomed to random access control over our data set. In other words, we were able to freely access any sample within our data set, and we counted on this ability in order to control how data was fed to the training pipeline; to control appropriate shuffling of the input data, enable boosting of certain subsets of data, and more. In the next chapters I will go into this in more detail and describe how we solved this.</p><p name="beac" id="beac" class="graf graf--p graf-after--p">Lest you should be thinking to yourself, “<em class="markup--em markup--p-em">Linux pipes? Come on… Now I have to manage those?!!</em>”.</p><blockquote name="f76b" id="f76b" class="graf graf--pullquote graf-after--p"><a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">SageMaker</a> comes with an implementation of the <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">TensorFlow</a> <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" data-href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank"><em class="markup--em markup--pullquote-em">Dataset</em></a> interface that essentially hides all the low level from you.</blockquote><p name="78ed" id="78ed" class="graf graf--p graf-after--pullquote">This provides support for all the <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> operations (preprocessing, boosting, shuffling, etc.) that your heart may desire, and feeds directly into the training pipeline. Nirvana…</p><pre name="ae25" id="ae25" class="graf graf--pre graf-after--p">def parse(record):</pre><pre name="5e10" id="5e10" class="graf graf--pre graf-after--pre">feature = {‘label’: tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),</pre><pre name="090b" id="090b" class="graf graf--pre graf--startsWithSingleQuote graf-after--pre">‘image_raw’: tf.FixedLenFeature([], tf.string)}</pre><pre name="83bf" id="83bf" class="graf graf--pre graf-after--pre">features = tf.parse_single_example(record, feature)</pre><pre name="c23e" id="c23e" class="graf graf--pre graf-after--pre">image = tf.decode_raw(features[‘image_raw’], tf.uint8)</pre><pre name="c777" id="c777" class="graf graf--pre graf-after--pre">label = features[‘label’]</pre><pre name="276e" id="276e" class="graf graf--pre graf-after--pre">return {“image”: image}, label # This is what will be fed into your model</pre><pre name="4ed7" id="4ed7" class="graf graf--pre graf-after--pre">ds = PipeModeDataset(“train”, record_format=’TFRecord’)</pre><pre name="c4c8" id="c4c8" class="graf graf--pre graf-after--pre">ds = ds.apply(map_and_batch(parse, batch_size=32, num_parallel_batches=2))</pre><pre name="c03e" id="c03e" class="graf graf--pre graf-after--pre">return ds</pre><h4 name="3021" id="3021" class="graf graf--h4 graf-after--pre">Picking a file format</h4><p name="27d0" id="27d0" class="graf graf--p graf-after--h4">But there was a catch. A fairly significant one. The catch was that we would need to transform all of our training data into one of the data formats supported by <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a>, which include: <a href="https://www.tensorflow.org/tutorials/tensorflow_text/intro" data-href="https://www.tensorflow.org/tutorials/tensorflow_text/intro" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">text</a> records, <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> and <a href="https://github.com/protocolbuffers/protobuf" data-href="https://github.com/protocolbuffers/protobuf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Protobuf</a>. Of course, we could choose to use <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a> with our existing data format, but to enjoy the goodness of <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a>, and save ourselves the headache of implementing the pipe management ourselves, we would need to adopt one of the above formats.</p><p name="f035" id="f035" class="graf graf--p graf-after--p">We chose <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a>, (really not for any other reason than the abundance of sample code available). For those wary of adopting a new data format, I will mention that <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> is <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a>’s binary storage format and that converting your data to <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> should be fairly simple (examples are abundant online).</p><p name="a95d" id="a95d" class="graf graf--p graf-after--p">For us, the need to transform our data set format actually turned out to be a <strong class="markup--strong markup--p-strong">huge blessing in disguise</strong>. Faced with the need to modify our data creation flow, we embarked on a quest to port this stage of the workflow to AWS as well.</p><blockquote name="d9f6" id="d9f6" class="graf graf--pullquote graf-after--p graf--trailing">This ultimately led to an <strong class="markup--strong markup--pullquote-strong">enormous acceleration in our data creation time</strong> (from several days to a couple of hours) and thus to our overall development time.</blockquote></div></div></section><section name="f669" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="58fd" id="58fd" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 3: Data Preparation… in the Cloud</strong></h3><p name="0d93" id="0d93" class="graf graf--p graf-after--h3">Having adopted <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sagemaker</a> pipe input mode and the <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> format, you now need to ensure that your training data is prepared accordingly. Let’s go over what that means.</p><h4 name="6181" id="6181" class="graf graf--h4 graf-after--p">Splitting the data set</h4><p name="a7ba" id="a7ba" class="graf graf--p graf-after--h4">In the standard usage of pipe input mode, we set up a pipe by providing an <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">S3</a> prefix. When the pipe is opened, all of the files that match the given prefix are fed one by one into the pipe. The size of the files may impact the performance of the pipe. File sizes that are too small or too big will almost certainly slow down your training cycle. After a bit of experimentation (and consulting our trusted AWS rep), we settled on a target file size of <strong class="markup--strong markup--p-strong">100 Megabytes</strong>. Thus our first requirement was that the training data be broken down into <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> files of roughly 100 Megabytes each.</p><h4 name="349e" id="349e" class="graf graf--h4 graf-after--p">Shuffling the data set</h4><p name="6681" id="6681" class="graf graf--p graf-after--h4">A common practice in the world of Machine Learning (ML) is to shuffle your data before training. In the past, we relied on our ability to randomly access any sample in our data set to ensure appropriate shuffling. However, given the sequential nature of pipe input mode, we could no longer rely on this. Thus, our second requirement was that the training data be appropriately shuffled during preparation.</p><p name="6e16" id="6e16" class="graf graf--p graf-after--p">When you have massive amounts of data, as we do, the task of preparing your data can be quite daunting and time consuming.</p><blockquote name="a16a" id="a16a" class="graf graf--pullquote graf-after--p">Fortunately, we were able to leverage the nearly infinite scale opportunities offered by the <a href="https://aws.amazon.com/batch/" data-href="https://aws.amazon.com/batch/" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank">AWS Batch</a> service in order to accomplish this in a highly parallel and very efficient manner.</blockquote><p name="9fde" id="9fde" class="graf graf--p graf-after--pullquote">To ensure a sufficiently random shuffling, we employed a two step process. The first step performed the initial parsing and recording of the data records, and the second step grouped the records into 100 Megabyte <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> files in a random fashion. I will not dive any further into the details, as they are pretty use case specific. I will only note that the pay-per-second and spot fleet support that <a href="https://aws.amazon.com/batch/" data-href="https://aws.amazon.com/batch/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AWS Batch</a> offers can help in reaching cost efficiency.</p><figure name="dc3f" id="dc3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*f7_KvJMStZuRP7ypEIJX4Q.png" data-width="1608" data-height="412" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*f7_KvJMStZuRP7ypEIJX4Q.png"></figure><p name="ffb4" id="ffb4" class="graf graf--p graf-after--figure graf--trailing">You will likely need to separate your data into different groups, for example train and test. This is done by using a different prefix for the train and test data and then setting up corresponding pipes in the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> start up script.</p></div></div></section><section name="c7af" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fa0d" id="fa0d" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 4: Data Shuffling</strong></h3><p name="be19" id="be19" class="graf graf--p graf-after--h3">We have ensured that the data in <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">S3</a> is shuffled, but in some cases, we want to reshuffle the data before each data traversal (epoch). This can be accomplished quite trivially when you have access to your full data set, but when it comes to pipe mode with its inherently sequential nature, the solution for this is not immediate.</p><p name="18aa" id="18aa" class="graf graf--p graf-after--p">In order to address this need, we can use <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sagemaker</a>’s <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">ShuffleConfig</em></a> class to set up each pipe such that before each data traversal, <strong class="markup--strong markup--p-strong">the order in which the files are fed into the pipe is shuffled</strong>. We chose to add an additional level of shuffling, that would include <strong class="markup--strong markup--p-strong">shuffling at the training batch level</strong>, using the <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" data-href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Dataset</em></a> shuffle function. This function, which is applied to the <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a>, receives a shuffle window size that causes each successive record to be randomly chosen from the next “window size” elements on the pipe. The window size we chose was dictated by the number of records in each file while taking care not to add too much memory overload on the application.</p><pre name="1b26" id="1b26" class="graf graf--pre graf-after--p">train_data = s3_input(<br>   ‘s3://sagemaker-path-to-train-data‘,<br>   shuffle_config=ShuffleConfig(seed)<br>)</pre><p name="a033" id="a033" class="graf graf--p graf-after--pre graf--trailing">The solution above does not give us the same degree of shuffling that we used to have. For example, two records that appear in the same <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> file are more likely to appear in close vicinity of one another than at two opposite ends of the data stream. But the three levels of shuffling that I have described (during data creation, <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">ShuffleConfig</em></a> and <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> shuffle) were more than sufficient for our purposes.</p></div></div></section><section name="47e4" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ca3f" id="ca3f" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 5: Managing Your Training Data</strong></h3><p name="8bbb" id="8bbb" class="graf graf--p graf-after--h3">Now may be a good time to mention that there is a limitation to the number of pipes you can set up. As of this writing, this limitation stands at 20 pipe channels. You might be asking yourself: “<em class="markup--em markup--p-em">Why the heck would I need any more than twenty? Why would anyone need more than two</em>“. There are often times where we want to separate our data into different subsets and manipulate the data differently during train time.</p><h4 name="02ee" id="02ee" class="graf graf--h4 graf-after--p">Using pipes to boost under-represented classes</h4><p name="c27f" id="c27f" class="graf graf--p graf-after--h4">Let me attempt to demonstrate this via the following (made-up) example.</p><p name="01a4" id="01a4" class="graf graf--p graf-after--p">Suppose you are tasked with creating a DNN that identifies cars on the road. You are given 100,000 marked frames and have transformed these into <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> files as described above. Now suppose you run a few rounds of training and find that your resultant network now succeeds in identifying most cars pretty well, but consistently fails to identify pink cars. You go back to your training data and realize that there is no wonder that you are failing to learn pink cars as you only have 10 training records with pink cars. The solution that you want to attempt is to “boost” the pink cars in your input pipe, meaning, during each epoch, you will feed the 10 records with pink cars twice. If you had free access to your entire data set that would be a fairly simple task. But how do you do it using pipes?</p><figure name="aa2e" id="aa2e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SwVer51HDKF9AVJ1jjC7hg.png" data-width="1186" data-height="448" src="https://cdn-images-1.medium.com/max/800/1*SwVer51HDKF9AVJ1jjC7hg.png"><figcaption class="imageCaption">We need more pink cars!</figcaption></figure><p name="6431" id="6431" class="graf graf--p graf-after--figure">A terrible solution (again… only my opinion… but unequivocally true) would be to duplicate the ten records in your data set in <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">S3</a>:</p><ol class="postList"><li name="2241" id="2241" class="graf graf--li graf-after--p">This approach could potentially and needlessly blow up the size of your data set.</li><li name="7b26" id="7b26" class="graf graf--li graf-after--li">I can almost guarantee that one day later you will decide that the correct boost rate is 3 not 2. Or is it 5?</li></ol><p name="b84c" id="b84c" class="graf graf--p graf-after--li">An alternative solution is to <strong class="markup--strong markup--p-strong">create a dedicated training pipe</strong> for pink cars and then in the preprocessing phase of the training interleave between the <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a> corresponding to the pink cars and the <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a> corresponding to the rest of the train records, with their corresponding appropriate weights, before feeding them to the network. (One way to do this is using the <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" data-href="https://www.tensorflow.org/tutorials/load_data/tfrecord" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TFRecord</em></a> <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/sample_from_datasets" data-href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/sample_from_datasets" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">sample_from_datasets</em></a> routine.)</p><pre name="e019" id="e019" class="graf graf--pre graf-after--p">ds = tf.contrib.data.sample_from_datasets(datasets, weights)</pre><p name="cb8c" id="cb8c" class="graf graf--p graf-after--pre">Now you might say to yourself : “<em class="markup--em markup--p-em">Great! I have a bunch of free pipes, I’ll use one of them for pink cars</em>”. But a few days later, you realize that you need a different boost parameter for pink trucks, and a different one for black cars at night… and before you know it you have hit the limit.</p><p name="dae3" id="dae3" class="graf graf--p graf-after--p">Before I get into some of the ways that we addressed this issue, I would like to give another example where having multiple pipes can be very useful.</p><h4 name="4aec" id="4aec" class="graf graf--h4 graf-after--p">Using pipes for data augmentation</h4><p name="7ee5" id="7ee5" class="graf graf--p graf-after--h4">A common practice in ML is to artificially increase your training data set by performing data augmentations. In the olden days, we would apply each one of a fixed set of augmentations to each data record and feed it to the network while ensuring appropriate shuffling. Again, we relied on our access to the full data set, which we did not have when moving to <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sagemaker</a> <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a>.</p><p name="f357" id="f357" class="graf graf--p graf-after--p">One appealing solution was to randomize the augmentation for each input record. However, some networks required us to fix the augmentation type and ensure that each augmentation was applied to each of the records. Another solution could have been to create all of the different augmentations ahead of time. But, once again, this would have been very wasteful and would not have enabled us to play with the augmentation parameters.</p><p name="4955" id="4955" class="graf graf--p graf-after--p">We chose to address this requirement by creating N parallel training pipes, where N was the number of different augmentation types. Each corresponding <a href="https://github.com/aws/sagemaker-tensorflow-extensions" data-href="https://github.com/aws/sagemaker-tensorflow-extensions" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">PipeModeDataset</em></a> was implemented with the corresponding augmentation function, following which all of the pipes were interleaved together before being fed to the network. In this case, it was extremely important to use the <a href="http://ShuffleConfig" data-href="http://ShuffleConfig" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">ShuffleConfig</em></a> object we discussed above, to increase the likelihood that the different augmentations of a given record would be spread out rather than bunched together.</p><h4 name="6a51" id="6a51" class="graf graf--h4 graf-after--p">Keeping pipes under control</h4><p name="0caa" id="0caa" class="graf graf--p graf-after--h4">Now that you are convinced that there may be situations in which we need more pipes than we are allotted, I will describe one solution we used for decreasing the number of pipes.</p><p name="8952" id="8952" class="graf graf--p graf-after--p">One of the alternatives to configuring pipes with an <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">S3</a> prefix (as described above), is to create and point to a <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">manifest file</a>.</p><div name="6f87" id="6f87" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Provide Dataset Metadata to Training Jobs with an Augmented Manifest File</strong><br><em class="markup--em markup--mixtapeEmbed-em">To classify data into different groupings, you train a model by using a dataset and metadata that act as labels. To…</em>docs.aws.amazon.com</a><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1b0b628a39d3b932e389553a3a00bdab" data-thumbnail-img-id="0*4fmSwSNVYBpyt16z" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*4fmSwSNVYBpyt16z);"></a></div><p name="d2be" id="d2be" class="graf graf--p graf-after--mixtapeEmbed">In a manifest file, you explicitly point to the list of files that you want to feed into the network. In particular, if there are certain files that you want to be traversed twice, you can simply write them in the manifest file twice. This is a very useful solution for use cases in which we have more boost rates than allotted pipes. It does not solve the multiple pipes needed for augmentations.</p><pre name="373b" id="373b" class="graf graf--pre graf-after--p">data = s3_input(<br>   ‘s3://path-to-manifest-file‘, <br>   s3_data_type=’ManifestFile‘, <br>   shuffle_config=ShuffleConfig(seed)<br>)</pre><p name="82e1" id="82e1" class="graf graf--p graf-after--pre">Let me summarize some of the tips we covered:</p><ul class="postList"><li name="99ba" id="99ba" class="graf graf--li graf-after--p">Try to group your data so that you will not need more that the maximum number of pipes. If you can’t, consider using manifest files.</li><li name="4c77" id="4c77" class="graf graf--li graf-after--li">Use the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">ShuffleConfig</em></a> setting to shuffle the order of the input files before each traversal.</li><li name="bd5e" id="bd5e" class="graf graf--li graf-after--li">Use the <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">TensorFlow</a> shuffle for additional shuffling at the file level.</li></ul><p name="56b1" id="56b1" class="graf graf--p graf-after--li graf--trailing">Obviously, the details of you own implementation, and whether any of the tips above apply, will depend on the specifics of your use case.</p></div></div></section><section name="b78e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a7b2" id="a7b2" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 6: Multi-GPU training</strong></h3><p name="a6bf" id="a6bf" class="graf graf--p graf-after--h3">One of the advantages to using <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> is our ability to freely choose a training instance to match our current training job.</p><div name="265d" id="265d" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/sagemaker/pricing/instance-types/" data-href="https://aws.amazon.com/sagemaker/pricing/instance-types/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/sagemaker/pricing/instance-types/"><strong class="markup--strong markup--mixtapeEmbed-strong">Amazon SageMaker Instance Types - Amazon Web Services (AWS)</strong><br><em class="markup--em markup--mixtapeEmbed-em">Amazon SageMaker provides a selection of instance types optimized to fit different machine learning (ML) use cases…</em>aws.amazon.com</a><a href="https://aws.amazon.com/sagemaker/pricing/instance-types/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f599b7f9607d35f703ada24bd4b266c6" data-thumbnail-img-id="0*qnEGbrpxuKATl61u" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*qnEGbrpxuKATl61u);"></a></div><p name="097a" id="097a" class="graf graf--p graf-after--mixtapeEmbed">In particular, we can choose one or more machines with one or more GPUs. There is no shortage of documentation on the different methods and strategies for using multiple GPUs to speed up training. There are multiple considerations that one should take into account when choosing the ideal training instance. (If you aren’t already, you should start by using the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-overview.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-overview.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">metrics</a> to view the GPU, CPU and memory utilizations.) There are also multiple ways of adjusting one’s code to multi-GPU training. I wish only to briefly demonstrate how one’s decision to use the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> framework, and, in particular, <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> pipe input mode, may bear on some of the decisions regarding multi-GPU implementation.</p><h4 name="4636" id="4636" class="graf graf--h4 graf-after--p">Setting up multi-GPU training</h4><p name="69b5" id="69b5" class="graf graf--p graf-after--h4">For some of our training jobs, we found it appropriate to perform <strong class="markup--strong markup--p-strong">data parallelization over multiple GPUs on a single instance</strong>, to speed up training. There were two primary libraries we considered for implementing this.</p><div name="e77a" id="e77a" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/"><strong class="markup--strong markup--mixtapeEmbed-strong">Launching TensorFlow distributed training easily with Horovod or Parameter Servers in Amazon…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Amazon SageMaker supports all the popular deep learning frameworks, including TensorFlow. Over 85% of TensorFlow…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="74be6099f0a921172007b9460c1c9f9a" data-thumbnail-img-id="0*ifFgBBQUwzknGH8L" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ifFgBBQUwzknGH8L);"></a></div><ul class="postList"><li name="d8a3" id="d8a3" class="graf graf--li graf-after--mixtapeEmbed">Built-in multi-GPU <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">TensorFlow</a> support: If you are using <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">TensorFlow</a> estimators, then this is a very attractive option as it boils down to just adding a few lines of code, setting the appropriate strategy in a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig" data-href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">tf.estimator.RunConfig</em></a>.</li><li name="a73a" id="a73a" class="graf graf--li graf-after--li">The <a href="https://github.com/horovod/horovod" data-href="https://github.com/horovod/horovod" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Horovod</a> distributed training framework: <a href="https://github.com/horovod/horovod" data-href="https://github.com/horovod/horovod" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Horovod</a> enables you to easily add a wrapping layer to your training code, that controls the number of training instances threads and ensures appropriate data sharing (gradient sharing) between them. <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker</a> supports <a href="https://github.com/horovod/horovod" data-href="https://github.com/horovod/horovod" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Horovod</a> configuration directly.</li></ul><div name="b7cc" id="b7cc" class="graf graf--mixtapeEmbed graf-after--li"><a href="https://github.com/aws-samples/sagemaker-horovod-distributed-training" data-href="https://github.com/aws-samples/sagemaker-horovod-distributed-training" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/aws-samples/sagemaker-horovod-distributed-training"><strong class="markup--strong markup--mixtapeEmbed-strong">aws-samples/sagemaker-horovod-distributed-training</strong><br><em class="markup--em markup--mixtapeEmbed-em">This lab demonstrates two concepts on a simple MNIST dataset and a TensorFlow deep learning framework: SageMaker…</em>github.com</a><a href="https://github.com/aws-samples/sagemaker-horovod-distributed-training" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="660b64fe7a834c26dbae7994b0f63530" data-thumbnail-img-id="0*NkgrHMHH2Pf4e5Lb" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*NkgrHMHH2Pf4e5Lb);"></a></div><h4 name="f7c6" id="f7c6" class="graf graf--h4 graf-after--mixtapeEmbed">Multi-GPU and Pipe Mode</h4><p name="2dc4" id="2dc4" class="graf graf--p graf-after--h4">There is one significant difference in the way these two solutions work. While <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> opens a single input stream which is shared by all GPUs, <a href="https://github.com/horovod/horovod" data-href="https://github.com/horovod/horovod" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Horovod</a> wraps the entire training script, including the data input flow. This means that if you are using <a href="https://github.com/horovod/horovod" data-href="https://github.com/horovod/horovod" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Horovod</a> to train on an instance with 8 GPUs, you will need to <strong class="markup--strong markup--p-strong">configure 8 times as many pipes</strong> as on a single GPU job. Given the limitation on pipes that we mentioned above, you could see how using Horovod may incur some limitations.</p><p name="2854" id="2854" class="graf graf--p graf-after--p">Naturally, performance should be the number one consideration when deciding which path to choose (and as we saw, we can sometimes work around the pipe limitation). We found the performance of both frameworks on our DNN to be comparable, and we chose the <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> option due to the pipe limitation.</p><h4 name="658d" id="658d" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Optimizing training times</strong></h4><p name="494f" id="494f" class="graf graf--p graf-after--h4">One last tip regarding multi-GPU training before we move on. It is quite common to run training with multiple GPUs, and to run evaluation on a single GPU. Now suppose that your evaluation takes an hour. If you are running evaluation intermittently during training, you will find yourself spending hours utilizing only one GPU on your multi-GPU instance. This is an unforgivable waste of resources, not to mention a huge waste of money.</p><p name="03b4" id="03b4" class="graf graf--p graf-after--p">Consider the following instead. Each time you want to run evaluation, <strong class="markup--strong markup--p-strong">spin up a new single GPU instance on </strong><a href="https://aws.amazon.com/ec2" data-href="https://aws.amazon.com/ec2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Amazon EC2</strong></a><strong class="markup--strong markup--p-strong"> from within your training session and launch the evaluation there</strong>. Yes, this works, provided that you have installed the <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> SDK, which surprisingly is not there by default.</p><p name="9b8d" id="9b8d" class="graf graf--p graf-after--p graf--trailing">This has the added benefit of <strong class="markup--strong markup--p-strong">reducing the delay to your training</strong> (which doesn’t have to wait for evaluation to complete before resuming), and it might be a good idea even in the single GPU case.</p></div></div></section><section name="cc10" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4577" id="4577" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 7: Debugging on Sagemaker</strong></h3><p name="ed70" id="ed70" class="graf graf--p graf-after--h3">I wish I could tell you, dear reader, that once you have transformed your data, configured your training, ensured appropriate shuffling session, and overcome any pipe number limitations, everything will work perfectly. But, alas, as with most everything in life, certainly in the world of SW development such is not the case.</p><p name="8561" id="8561" class="graf graf--p graf-after--p">As always, you are likely to experience crashes, exceptions, training failures and other woes. Just, that now, the usual difficulties of debugging and solving such issues are compounded by the fact that your are running on a remote environment.</p><p name="8ee6" id="8ee6" class="graf graf--p graf-after--p">So, here is golden rule number one… and if you take nothing away from this blog but this, my time will have been well spent.</p><blockquote name="ae2a" id="ae2a" class="graf graf--pullquote graf-after--p"><strong class="markup--strong markup--pullquote-strong">Always start by running your training session in your local environment</strong></blockquote><p name="e317" id="e317" class="graf graf--p graf-after--pullquote">You can stick to a very small subset of your data — even just one or two batches before running on <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a>. This will save you lots of time (and money). It’s as simple as that.</p><div name="cc9a" id="cc9a" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/" data-href="https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/"><strong class="markup--strong markup--mixtapeEmbed-strong">Use the Amazon SageMaker local mode to train on your notebook instance | Amazon Web Services</strong><br><em class="markup--em markup--mixtapeEmbed-em">Amazon SageMaker recently launched support for local training using the pre-built TensorFlow and MXNet containers…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="18a70fb45760b38ccab52c89bb97f893" data-thumbnail-img-id="0*Mo14PMCM-oNxNKoD" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Mo14PMCM-oNxNKoD);"></a></div><p name="5b6e" id="5b6e" class="graf graf--p graf-after--mixtapeEmbed">The problem is that not all issues can be reproduced this way. Some issues are environment specific, other issues are related to <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a>, which (as of now) cannot be run locally, and yet other issues (such as lack of loss convergence), only come up when training on a large amount of data. Here are some pointers that you might find helpful:</p><ol class="postList"><li name="5395" id="5395" class="graf graf--li graf-after--p">The <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">SageMaker</a> logs (which can be accessed from the console) is probably the first thing to check. There you will get an initial indication if something went wrong, and if so, what.</li><li name="4e6d" id="4e6d" class="graf graf--li graf-after--li">If you suspect you may be facing an issue with <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Pipe Mode</a> (e.g. low throughput), the first thing you should do is open a ticket to AWS support. You could try to add <a href="https://www.tensorflow.org/api_docs/python/tf/print" data-href="https://www.tensorflow.org/api_docs/python/tf/print" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><em class="markup--em markup--li-em">tf.print</em></a> and what not to try and find the root cause, but the <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Pipe Mode</a> mechanism is a feature that we do not have much visibility into.</li><li name="febf" id="febf" class="graf graf--li graf-after--li">Use <a href="https://www.tensorflow.org/tensorboard" data-href="https://www.tensorflow.org/tensorboard" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">TensorBoard</a> to track the performance of your training. You can configure <a href="https://www.tensorflow.org/tensorboard" data-href="https://www.tensorflow.org/tensorboard" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">TensorBoard</a> (from the command line) to point directly to the <a href="https://aws.amazon.com/s3" data-href="https://aws.amazon.com/s3" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">S3</a> model directory, or (if you have many events) download the event file periodically and run locally.</li><li name="ad7e" id="ad7e" class="graf graf--li graf-after--li">Use the console to track CPU and GPU utilization metrics. Advanced users can add custom metrics (such as training loss), trigger alarms and apply other <a href="https://aws.amazon.com/cloudwatch" data-href="https://aws.amazon.com/cloudwatch" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">CloudWatch</a> techniques.</li><li name="d9b2" id="d9b2" class="graf graf--li graf-after--li graf--trailing">If you think of additional debugging features that would help you and the community at large, don’t hesitate to submit a feature request!</li></ol></div></div></section><section name="d4bd" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ae1f" id="ae1f" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 8: Using Spot Instances on Sagemaker</strong></h3><p name="c00f" id="c00f" class="graf graf--p graf-after--h3">Recently, AWS announced support for training in <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> on Spot Instances.</p><div name="4d3b" id="4d3b" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/" data-href="https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/"><strong class="markup--strong markup--mixtapeEmbed-strong">Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs | Amazon Web Services</strong><br><em class="markup--em markup--mixtapeEmbed-em">is a fully-managed, modular machine learning (ML) service that enables developers and data scientists to easily build…</em>aws.amazon.com</a><a href="https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="968d229035a164d0d2a86adb3a67b5b1" data-thumbnail-img-id="0*EbhZZHYfu105xThI" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*EbhZZHYfu105xThI);"></a></div><p name="b574" id="b574" class="graf graf--p graf-after--mixtapeEmbed">Spot Instances let you take advantage of unused compute capacity in the cloud, allowing you to significantly reduce cost. The catch, of course, is that if the machine is suddenly needed by a customer willing to pay the full price, your compute (in our case your training session) will be terminated midway and your training instance will be taken away from you. The good news is that <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SageMaker</a> will <strong class="markup--strong markup--p-strong">restart your training session</strong> as soon as a new Spot Instance is available. Of course, there is no guarantee how long that might take.</p><blockquote name="bc3c" id="bc3c" class="graf graf--pullquote graf-after--p">The opportunity to reduce cost is quite compelling.</blockquote><p name="68bc" id="68bc" class="graf graf--p graf-after--pullquote">Still, imagine training for a day or two or three, only to have your instance terminated on your last epoch!! Imagine the gut-wrenching, blood-curling despair.</p><p name="fc49" id="fc49" class="graf graf--p graf-after--p">Of course, there is an easy solution for that, and that is to <strong class="markup--strong markup--p-strong">periodically store checkpoints of your model during training</strong>.</p><blockquote name="9d50" id="9d50" class="graf graf--pullquote graf-after--p">If your training algorithm is halted midway, the job simply resumes from the latest stored checkpoint.</blockquote><p name="12d8" id="12d8" class="graf graf--p graf-after--pullquote">This is extremely straightforward when using <a href="https://www.tensorflow.org/" data-href="https://www.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow</a> estimators, which automatically searches for an existing checkpoint in your model directory when it starts up. All that is left for you to do, is to decide on the frequency at which you want to store checkpoints.</p><div name="7be6" id="7be6" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Using Checkpoints in Amazon SageMaker</strong><br><em class="markup--em markup--mixtapeEmbed-em">A checkpoint is a snapshot of the state of the model. They can be used with Managed Spot Training. If a training job is…</em>docs.aws.amazon.com</a><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6b30c7319cacbba72f85d7bbd4ef8849" data-thumbnail-img-id="0*HtQjVBhZErH7D7OM" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*HtQjVBhZErH7D7OM);"></a></div><p name="6908" id="6908" class="graf graf--p graf-after--mixtapeEmbed">But there is another, somewhat more delicate thing to consider. Suppose, you have the wild misfortune of having your Spot Instance terminated ten consecutive times, right after you have traversed precisely the first fifth of your data. The net effect is that you have trained your network (for ten epochs) on precisely a fifth of your data. You have not seen the rest of the data at all. You could see why that would be a problem as your model will biased towards the data it has seen.</p><p name="8ce3" id="8ce3" class="graf graf--p graf-after--p">Ideally, you would like to return to the exact location you were at before you were terminated (or more accurately, the location where the last checkpoint was saved), but this is not possible (today) in <a href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" data-href="https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pipe Mode</a>.</p><p name="52a5" id="52a5" class="graf graf--p graf-after--p">This problem is alleviated when you use the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ShuffleConfig.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">ShuffleConfig</em></a> class as we described above. This will ensure that each time the training restarts, it will <strong class="markup--strong markup--p-strong">start from a different location and on a different ordering of the data</strong>. This is likely to prevent the danger of developing a bias towards a subset of your data.</p><p name="92c9" id="92c9" class="graf graf--p graf-after--p graf--trailing">My non-binding advice would be to definitely take advantage of Spot Instances to reduce cost, but perhaps consider keeping your critical sessions on regular (non-spot) instances.</p></div></div></section><section name="07c9" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4aeb" id="4aeb" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">Chapter 9: Summary</strong></h3><p name="1f47" id="1f47" class="graf graf--p graf-after--h3">With that, I have come to the end of my story; the story of how we made <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">SageMaker</strong></a><strong class="markup--strong markup--p-strong"> work for us.</strong></p><p name="224b" id="224b" class="graf graf--p graf-after--p graf--trailing">Yes, as with the adoption of any new development environment, we had to go through some hoops and hurdles, especially given the scale at which we operate. We got some unexpected benefits, and I hope I have convinced you that <a href="https://aws.amazon.com/sagemaker/" data-href="https://aws.amazon.com/sagemaker/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">SageMaker</strong></a><strong class="markup--strong markup--p-strong"> can work for you too!</strong></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@julsimon" class="p-author h-card">Julien Simon</a> on <a href="https://medium.com/p/893365184233"><time class="dt-published" datetime="2019-12-04T16:30:21.168Z">December 4, 2019</time></a>.</p><p><a href="https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 26, 2025.</p></footer></article></body></html>
