<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Predicting world temperature with time series and DeepAR on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="49f0">Predicting world temperature with time series and DeepAR on Amazon SageMaker</h3><p id="c909">Predicting time-based values is a popular use case for Machine Learning. Indeed, a lot of phenomena — from rainfall to fast-food queues to stock prices — exhibit <strong class="markup--p-strong">time-based patterns</strong> that can be successfully captured by a Machine Learning model.</p><p id="17ce">In this post, you will learn how to predict <strong class="markup--p-strong">temperature time-series</strong> using <strong class="markup--p-strong">DeepAR</strong> — one of the latest <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" rel="nofollow noopener noopener" target="_blank">built-in algorithms</a> added to <a href="https://aws.amazon.com/sagemaker" rel="nofollow noopener noopener" target="_blank">Amazon SageMaker</a>. As usual, a <a href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/04-DeepAR-temperatures.ipynb" target="_blank">Jupyter notebook</a> is available on Github.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="7050">The very nice collection of <a class="markup--blockquote-anchor" href="https://github.com/awslabs/amazon-sagemaker-examples" target="_blank">SageMaker sample notebooks</a> includes another <a class="markup--blockquote-anchor" href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb" target="_blank">DeepAR example</a> and I strongly encourage you to check it out. Mine differs in two ways: it uses a real-life data set (not a synthetic one) and it doesn’t use the <em class="markup--blockquote-em">pandas</em> library, which I believe makes it a little easier to understand :)</blockquote><h4 class="graf-after--blockquote" id="8025">A word about DeepAR</h4><p id="0bc4">DeepAR is an algorithm introduced in 2017. It’s quite complex and I won’t go into details here. Let’s just say that unlike other techniques that train a different model for each time-series, DeepAR builds a <strong class="markup--p-strong">single model for all time-series </strong>and tries to identify similarities across them. Intuitively, this sounds like a good idea for temperature time-series as we could expect them to exhibit similar patterns year after year.</p><p id="ee91">If you’d like to know more about DeepAR, please refer to the original <a href="https://arxiv.org/abs/1704.04110" target="_blank">research article</a> as well as the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html" target="_blank">SageMaker documentation</a>.</p><h4 id="6a2f">The Berkeyley Earth dataset</h4><p id="3fc2">This <a href="http://berkeleyearth.org/data/" target="_blank">dataset</a> contains a very large number of temperatures recorded across the globe since the 18th century. Here, I will use the <a href="http://berkeleyearth.lbl.gov/auto/Global/Complete_TAVG_daily.txt" target="_blank">Daily Land</a> dataset, which holds a <strong class="markup--p-strong">daily temperature measure from 1880 to 2014</strong>. Temperatures are reported as a variation (aka anomaly) from the 1951–1980 average (8.68°C), as visible in the sixth column.</p><figure id="64b0"><script src="https://gist.github.com/juliensimon/2264e3e8261673a259518fd1ce9d15ed.js"></script></figure><p id="151f">Before we go any further, we have to decide how to build our time-series. Given data resolution (one data point per day), we should probably build <strong class="markup--p-strong">yearly</strong> series: having long enough series (hundreds of samples at least) is one of the requirements for a successful model. Thus, we’re going to build <strong class="markup--p-strong">135 series of 365 samples </strong>(or 366 for leap years) using the second and sixth columns in our file.</p><p id="b0c6">In addition, we should decide how many samples we’d like to predict: let’s go for 30, i.e. <strong class="markup--p-strong">predicting a month’s worth of temperatures</strong>.</p><h4 id="e8aa">Loading the dataset</h4><p id="8582">Very little dataset preparation is required (woohoo!). Once we’ve downloaded and cleaned up the file (remove header and empty lines), we can directly load it into a list (for plotting) and a dictionary (for training). Note that we’re also adding the average temperature to all samples in order to work with actual temperatures, not variations.</p><figure id="d9f2"><script src="https://gist.github.com/juliensimon/bbd6696397491e0d73dd8b45b496db86.js"></script></figure><h4 id="7ca5">Plotting the dataset</h4><p id="4f19">Let’s take a quick look at our dataset.</p><figure id="e5d0"><script src="https://gist.github.com/juliensimon/588480b77e1e9a27b67c10d2486c6ac3.js"></script></figure><p id="94cc">I see an upward trend, but I’ll let each of you come to their own conclusions.</p><figure id="110d"><img class="graf-image" src="image02.webp"/><figcaption>World temperature from 1880 to 2014.</figcaption></figure><h4 id="6243">Preparing the training and test sets</h4><p id="6024">We’re not going to split 80/20 like we usually would. Things are a bit different when working with time series:</p><ul class="postList"><li id="5d5c"><strong class="markup--li-strong">Training set</strong>: we need to <strong class="markup--li-strong">remove the last 30 sample points</strong> from each time series. Time series should also be shuffled, although it is unnecessary here because Python dictionaries are not ordered ;)</li><li id="88d9"><strong class="markup--li-strong">Test set</strong>: we can use the <strong class="markup--li-strong">full dataset</strong>.</li></ul><figure id="eb4b"><script src="https://gist.github.com/juliensimon/4040fce372c1b57fe2c7fd9e1f3196f3.js"></script></figure><h4 id="1fcc">Writing the datasets to JSON format</h4><p id="d0f8">The <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html" target="_blank">input format</a> for DeepAR is <a href="http://jsonlines.org/" target="_blank">JSON Lines</a>: <strong class="markup--p-strong">one sample per line</strong>, such as this one.</p><pre class="graf--pre" id="f8ff">{"start":"2009-11-01 00:00:00", "target": [4.3, 10.3, ...]}</pre><p class="graf-after--pre" id="0577">Easy enough, let’s take care of it.</p><figure id="3672"><script src="https://gist.github.com/juliensimon/7e77430ecbc63ec364ed490e79babb31.js"></script></figure><h4 id="2d0a">Uploading to S3</h4><p id="34e2">Next, let’s upload our data to S3. The <a href="https://github.com/aws/sagemaker-python-sdk" target="_blank">SageMaker SDK</a> has a nice little function to do this.</p><figure id="9cba"><script src="https://gist.github.com/juliensimon/819c360dc958d16b29d7efbc7d32bf7f.js"></script></figure><h4 id="ac0e">Configuring the training job</h4><p id="2553">As usual with built-in algorithms, we need to select the <strong class="markup--p-strong">container</strong> corresponding to the region we run in and then create an <a href="http://sagemaker.readthedocs.io/en/latest/estimators.html" target="_blank"><em class="markup--p-em">Estimator</em></a>. Nothing unusual here.</p><figure id="8f85"><script src="https://gist.github.com/juliensimon/aaa889936f96dec8fc34047f913bd983.js"></script></figure><p id="bf22">Now let’s look at hyper parameters.</p><h4 id="b029">Defining hyper parameters</h4><p id="5c03">Hyper parameters for DeepAR are detailed in the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html" target="_blank">documentation</a>. Let’s focus on the required ones:</p><ul class="postList"><li id="a7ca"><em class="markup--li-em">time_freq</em>: the <strong class="markup--li-strong">resolution of time series</strong> (from minutes to months). Ours has daily resolution.</li><li id="5780"><em class="markup--li-em">prediction_length</em>: <strong class="markup--li-strong">how many data samples we’re going to predict</strong> (30, remember?).</li><li id="5170"><em class="markup--li-em">context_length</em>: <strong class="markup--li-strong">how many data points we’re going to look at before predicting</strong>. We’ll use 30 too, it should be enough to figure out temperatures.</li><li id="28c0"><em class="markup--li-em">epochs</em>: I wonder what this does ;)</li></ul><p id="6d4a">In addition, after a unreasonable number of different tests, I ended up getting better results with only <strong class="markup--p-strong">two layers</strong> (default is three), a <strong class="markup--p-strong">smaller learning rate</strong> and a <strong class="markup--p-strong">large number of epochs</strong>.</p><blockquote class="graf--blockquote" id="1800">My intuition is that the data set being pretty small with rather short time-series, three layers tend to overfit more. Two layers don’t learn as well, but letting them learn longer with a smaller learning rate makes up for it. Or something. Curious to here your own opinion.</blockquote><figure class="graf-after--blockquote" id="72e7"><script src="https://gist.github.com/juliensimon/eada2a7d9e26244946d248220e766fd0.js"></script></figure><h4 id="4558">Training the model</h4><p id="5348">Super simple :)</p><figure id="d579"><script src="https://gist.github.com/juliensimon/8afeef113853f75b5254b257da34fc12.js"></script></figure><p id="06c8">Training stops early and the best epoch is selected (#206). Here are my three metrics: <strong class="markup--p-strong">loss for p50 and p90</strong> (which tell us how accurate the predicted distribution is), as well as <strong class="markup--p-strong">Root Mean Square Error</strong>.</p><pre class="graf--pre" id="8952">[01/31/2018 22:19:52 INFO 140078416930624] #test_score (algo-1, <strong class="markup--pre-strong">wQuantileLoss[0.5]</strong>): <strong class="markup--pre-strong">0.0584739</strong><br/>[01/31/2018 22:19:52 INFO 140078416930624] #test_score (algo-1, <strong class="markup--pre-strong">wQuantileLoss[0.9]</strong>): <strong class="markup--pre-strong">0.0294685</strong><br/>[01/31/2018 22:19:52 INFO 140078416930624] #test_score (algo-1, <strong class="markup--pre-strong">RMSE</strong>): <strong class="markup--pre-strong">0.62858571005</strong></pre><p class="graf-after--pre" id="5fa4">OK, now let’s deploy this model and use it.</p><h4 id="6054">Deploying the model</h4><p id="ee25">Nothing complicated here: create an <strong class="markup--p-strong">endpoint</strong> hosting our model and create a <em class="markup--p-em">RealTimePredictor</em> to send requests to.</p><figure id="14b3"><script src="https://gist.github.com/juliensimon/64869098ce9bc2edea8c67c35264b728.js"></script></figure><h4 id="bfa7">Building a prediction request</h4><p id="850b">According to the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html" target="_blank">inference format</a> for DeepAR, here’s what we should send to the endpoint:</p><ul class="postList"><li id="3b85"><strong class="markup--li-strong">JSON-formatted samples</strong> (we’ll use only one at a time)</li><li id="8bb1">an <strong class="markup--li-strong">optional configuration</strong> listing the <strong class="markup--li-strong">series</strong> we’d like to receive (mean values, quantiles and raw samples: we’ll take all of them, thank you) as well as the <strong class="markup--li-strong">number of samples</strong> from which they’re built.</li></ul><figure id="179f"><script src="https://gist.github.com/juliensimon/35bcc0fedcbceeafa7889d2b7ed3d8d7.js"></script></figure><p id="4fde">Here’s an example request, where we provide the first 30 data points.</p><pre class="graf--pre" id="fe75">{"<strong class="markup--pre-strong">instances</strong>": [ {"<strong class="markup--pre-strong">start</strong>": "2018-01-01 00:00:00", "<strong class="markup--pre-strong">target</strong>": [8.371208085491508, 8.38437885371535, 8.860699073980985, 8.047195011672134, 9.42771383264719, 8.02120332304575, 9.839234913116105, 9.237618947392374, 8.214949470821212, 9.814497679561292, 9.052164695305954, 8.102437854966766, 8.928941871965348, 9.844116398312188, 9.221646100693144, 8.853571486995326, 8.560903044968434, 8.240263518568812, 9.221323908588538, 9.448381346299827, 9.996678314417732, 8.520757726306975, 9.978841260562627, 9.196420806291513, 9.587904493744922, 9.367880938747199, 9.606228859687628, 9.277298500001638, 8.694011829622228, 8.264125277439893]}], <br/>"<strong class="markup--pre-strong">configuration</strong>": {"<strong class="markup--pre-strong">output_types</strong>": ["mean", "quantiles", "samples"], "<strong class="markup--pre-strong">quantiles</strong>": ["0.1", "0.9"], "<strong class="markup--pre-strong">num_samples</strong>": 100}}</pre><h4 class="graf-after--pre" id="5d5b">Extracting prediction results</h4><p id="eb86">Once we get prediction results, we need to extract each time series for plotting. Obviously, we’re not interested in the 100 raw sample series, let’s just pick one at random.</p><figure id="5217"><script src="https://gist.github.com/juliensimon/8e328a3859bae2aacc4c3f402c997c87.js"></script></figure><h4 id="d1be">Plotting prediction results</h4><p id="7d8f">Pretty graphs: everyone loves them. They’ll also help us get a sense of how well we’re predicting. We’ll throw in ground truth for good measure.</p><figure id="bc53"><script src="https://gist.github.com/juliensimon/44863b030d62fc3baf52ee5b0c704595.js"></script></figure><h4 id="29d6">Predicting some samples</h4><p id="efef">All right, time to put all of this to work!</p><p id="c6f6">First, let’s predict the <strong class="markup--p-strong">last 30 days of 1984</strong> and compare to <strong class="markup--p-strong">ground truth</strong>.</p><figure id="bf60"><script src="https://gist.github.com/juliensimon/711c03d03f436ec27304ddcc6145c3ed.js"></script></figure><figure id="6ad4"><img class="graf-image" src="image03.webp"/><figcaption>Purple vs blue: not too bad!</figcaption></figure><p id="fe56">Let’s try another example. This time, suppose that we have <strong class="markup--p-strong">data samples for the first 90 days of 2018 </strong>(we’ll just use random values here) and that we want to <strong class="markup--p-strong">predict the next 30 days</strong>. Here’s how you would do it.</p><figure id="cf6b"><script src="https://gist.github.com/juliensimon/3be1369fd8113bf0f54ee23eb08c2e4e.js"></script></figure><figure id="7b66"><img class="graf-image" src="image01.webp"/></figure><h4 id="45f5">Conclusion</h4><p id="54fb">As you can see, built-in algorithms like DeepAR are a great way to get the job done quickly: <strong class="markup--p-strong">no training code to write, no infrastructure drama to endure</strong>. We can thus focus on experimenting with our time series and hyper-parameters to get the best result possible.</p><p id="de7d">If you’re curious about other<strong class="markup--p-strong"> SageMaker built-in algorithms</strong>, here are some previous posts on:</p><ul class="postList"><li id="9d00"><a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" target="_blank">spam classification with XGBoost</a>,</li><li id="69fa"><a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" target="_blank">image classification with Deep Learning</a>,</li><li id="a8fc"><a href="https://medium.com/@julsimon/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8" target="_blank">movie recommendation with Factorization Machines</a>.</li></ul><p id="ca29">As always, thank you for reading. Happy to answer questions on <a href="https://twitter.com/julsimon/" rel="nofollow noopener noopener" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="c91e"><em class="markup--p-em">“Do you wanna know the truth, son? Lord, I’ll tell you the truth. Your soul’s gonna burn in a lake of fire”.</em></p><figure id="aa91"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/k2rR5HaXnZQ?feature=oembed" width="640"></iframe></figure></div></div></section>
</section>
</article></body></html>