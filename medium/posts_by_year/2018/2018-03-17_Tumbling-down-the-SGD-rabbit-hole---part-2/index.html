<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Tumbling down the SGD rabbit hole — part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="3202">Tumbling down the SGD rabbit hole — part 2</h3><p id="cfae">In the <a href="http://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7" target="_blank">first part</a> of this post, we studied the <strong class="markup--p-strong">Stochastic Gradient Descent </strong>optimizer and discussed <strong class="markup--p-strong">five problems</strong> that could hamper training of Deep Learning models:</p><p id="437f"><strong class="markup--p-strong">1- Local minima,</strong></p><p id="32c6"><strong class="markup--p-strong">2- Slow convergence,</strong></p><p id="1247"><strong class="markup--p-strong">3- Different slopes,</strong></p><p id="e1fe"><strong class="markup--p-strong">4- Saddle points,</strong></p><p id="e584"><strong class="markup--p-strong">5- Gradient size &amp; distributed training.</strong></p><p id="614f">In this post, we’ll discuss <strong class="markup--p-strong">solutions</strong> to these problems and how to apply them.</p><figure id="61be"><img class="graf-image" src="image05.webp"/><figcaption>You know you’re having a bad day when the light at the end of the tunnel is actually a black hole…</figcaption></figure><h3 id="8f07">1- Local minima</h3><p id="650b">The problem posed by local minima in deep neural networks has been debated for a long time. However, <strong class="markup--p-strong">intuition tells us that they should be rare</strong>. Indeed, the number of parameters in such networks is very large (millions at least): for a point to be a local minimum, all dimensions should have a positive slope. The probability of this happening would be the inverse of 2 to the power of the number of dimension. The inverse of 2^1,000,000? Of 2^10,000,000? Pretty slim chance…</p><p id="2473">Based on this, one could also conclude that using larger-than-needed models would actually be a good idea: <strong class="markup--p-strong">more parameters would mean lower probability for local minima</strong>, right? Indeed, a 2015 paper by Itay Safran and Ohad Shamir (“<a href="https://arxiv.org/abs/1511.04210" target="_blank"><em class="markup--p-em">On the Quality of the Initial Basin in Overspecified Neural Networks</em></a>”) shows that this is the case: “<em class="markup--p-em">higher dimensions also means more potential directions of descent, so perhaps the gradient descent procedures used in practice are more unlikely to get stuck in poor local minima and plateaus</em>”.</p><p id="a25a">Another paper published in 2014 paper by Ian J. Goodfellow, Oriol Vinyals and Andrew M. Sax (“<a href="https://arxiv.org/abs/1412.6544" target="_blank"><em class="markup--p-em">Qualitatively characterizing neural network optimization problems</em></a>”) concludes empirically that local minima are not a problem when training deep neural networks. <strong class="markup--p-strong">Yes, they may exist but in practice they’re hardly ever encountered during SGD</strong>. Even when they are, it does just fine escaping them, thank you.</p><p id="f366">When all is said and done, please remember one thing: <strong class="markup--p-strong">the purpose of the training process is not to find *the* global mimimum — </strong>this<strong class="markup--p-strong"> </strong>is a NP-hard problem, so forget about it.<strong class="markup--p-strong"> It is to find *a* minimum that generalizes well enough</strong>, yielding a test accuracy compatible with our business problem.</p><figure id="efa4"><img class="graf-image" src="image03.webp"/><figcaption>Stop worrying and learn to love SGD!</figcaption></figure><h3 id="f35d">2 &amp; 3 - Slow convergence and different slopes</h3><p id="7f7c">These problems are related: for very high dimension problems such as deep neural networks, it will take a <strong class="markup--p-strong">long time</strong> to reach an acceptable minimum if you use a <strong class="markup--p-strong">small</strong> <strong class="markup--p-strong">fixed learning rate.</strong></p><p id="d170">Over the years, a number of improvements were designed to speed up <strong class="markup--p-strong">SGD</strong> (Robbins and Monro, 1951). Techniques like <strong class="markup--p-strong">Momentum</strong> (Polyak, 1964) and <strong class="markup--p-strong">Nesterov Accelerated Gradient</strong> (Nesterov, 1983) were designed to accelerate progress in the direction of steepest descent.</p><p id="aacb">As Deep Learning gained popularity and as networks grow larger, researchers realized that some dimensions had <strong class="markup--p-strong">steep slopes</strong> while some exhibited <strong class="markup--p-strong">vast plateaus</strong>. Thus, they worked on adapt to these different conditions by not only <strong class="markup--p-strong">modifying the learning rate</strong> but also by <strong class="markup--p-strong">using different learning rates for different dimensions</strong>.</p><p id="0ae8">Enter the Ada* family!</p><figure id="5072"><img class="graf-image" src="image04.webp"/><figcaption>“We’re a happy family, we’re a happy family, hey Mom and Daddy”</figcaption></figure><h4 id="16ab"><strong class="markup--h4-strong">Adaptive optimizers</strong></h4><p id="33f9"><strong class="markup--p-strong">AdaGrad</strong> (2011, <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank">PDF</a>) came first. For a given parameter, it <strong class="markup--p-strong">increases the learning rate if it receives small updates</strong> (i.e. speed up on plateaus) and <strong class="markup--p-strong">decrease it if it receives larges updates</strong> (i.e. slow down on steep slopes). This is achieved by dividing the learning rate by the sum of all squared past gradient updates (aka the l2 norm): <strong class="markup--p-strong">larger gradient updates will reduce the learning rate while tiny gradient updates will increase it</strong>.</p><p id="898c">AdaGrad has a problem, however: as the sum grows monotonically, the learning rates will end up converging to <strong class="markup--p-strong">zero</strong> for large models and long trainings, effectively preventing any further progress.</p><p id="3072"><strong class="markup--p-strong">RMSProp</strong> (2012) and <a href="https://arxiv.org/abs/1212.5701" target="_blank"><strong class="markup--p-strong">AdaDelta</strong></a> (2012) solve this problem by <strong class="markup--p-strong">decaying the accumulated sum</strong> before adding the new gradient. This prevents the sum from exploding and the learning rates from going to zero.</p><p id="17cf"><a href="https://arxiv.org/abs/1412.6980" target="_blank"><strong class="markup--p-strong">Adam</strong></a> (2015) adds <strong class="markup--p-strong">momentum</strong> to AdaDelta, further focusing progress in the direction of steepest descent.</p><p id="0e77">Here’s a great visualization. The adaptive optimizers race to the minimum, momentum and NAG go for a walk in the park before heading out to the right spot… and Grand Pa SGD gets there too but really slowly.</p><figure id="f458"><img class="graf-image" src="image01.webp"/><figcaption>Source: Alec Radford</figcaption></figure><p id="e787">All these optimizers are available in your favorite Deep Learning library. <strong class="markup--p-strong">Adam is a popular choice as it seems to work well in most situations</strong>. Does it mean that it can’t be improved? Of course not: research never sleeps and this nice <a href="http://ruder.io/deep-learning-optimization-2017/" target="_blank">post</a> by Sebastian Ruder lists the latest developments.</p><h4 id="32a5">SGD strikes back?</h4><p id="817a">One of these developments is surprising. Some researchers now question that adaptive optimizers are the best choice for deep neural networks, as illustrated by “<a href="https://arxiv.org/abs/1705.08292" target="_blank"><em class="markup--p-em">The Marginal Value of Adaptive Gradient Methods in Machine Learning</em></a>” (Ashia Wilson et al., 2017).</p><p id="d6f1">This paper show that <strong class="markup--p-strong">well-tuned SGD with learning rate decay at specific epochs ends up outperforming all adaptive optimizers</strong>: “<em class="markup--p-em">Despite the fact that our experimental evidence demonstrates that adaptive methods are not advantageous for machine learning, the Adam algorithm remains incredibly popular. We are not sure exactly as to why, but hope that our step-size tuning suggestions make it easier for practitioners to use standard stochastic gradient methods in their research</em>”.</p><p id="f7b7">Now, of course, figuring out the learning rate decay schedule is another complex problem in itself, so I wouldn’t bury Adam just yet :)</p><figure id="4d5d"><img class="graf-image" src="image07.webp"/><figcaption>And now for something completely different…</figcaption></figure><h4 id="1a03">The FTML optimizer</h4><p id="7602">So far, we’ve only studied SGD and its variants. Of course, there are other techniques out there. One of them is the Follow the Moving Leader algorithm aka <strong class="markup--p-strong">FTML</strong> (“<em class="markup--p-em">Follow the Moving Leader in Deep Learning</em>” by Shuai Zheng and James T. Kwok, 2017, <a href="http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf" target="_blank">PDF</a>).</p><p id="a1cb">I won’t go into details in this already long post, but let’s take a quick look at some of the results: learning <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">CIFAR-10</a> with <a href="http://yann.lecun.com/exdb/lenet/" target="_blank">LeNet</a> and <a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet-110</a>.</p><figure id="e8fa"><img class="graf-image" src="image08.webp"/><figcaption>LeNet CNN training on CIFAR-10</figcaption></figure><figure id="1cc7"><img class="graf-image" src="image02.webp"/><figcaption>ResNet-110 training CIFAR-10</figcaption></figure><p id="977b"><strong class="markup--p-strong">Is there a new sheriff in Optimizer City?</strong> You’ll find more results in the research paper (tl;dr: FTML matches or surpasses Adam on CIFAR-100 and on LSTM tasks).</p><p id="5211"><strong class="markup--p-strong">FTML is available in Apache MXNet 1.1</strong>. Using it is as simple as:</p><pre class="graf--pre" id="fcb4">mod.init_optimizer(optimizer=’ftml’)</pre><h3 class="graf-after--pre" id="ceb9">4- Saddle points</h3><h4 id="8810">Noisy SGD</h4><p id="7f13">In 2015, Rong Ge et al. proposed a <strong class="markup--p-strong">noisy version of SGD</strong> (“<a href="https://arxiv.org/abs/1503.02101" target="_blank"><em class="markup--p-em">Escaping From Saddle Points — Online Stochastic Gradient for Tensor Decomposition</em></a>”): when updating parameters, adding a tiny amount of noise on top of the gradient is enough to nudge SGD to a slightly lower point instead of getting stuck at the saddle point. Very cool idea :)</p><blockquote class="graf--blockquote" id="f565">“<a class="markup--blockquote-anchor" href="http://www.offconvex.org/" target="_blank">Off the convex path</a>” — Rong Ge’s blog — is an excellent resource. Check it out.</blockquote><h4 class="graf-after--blockquote" id="5287">Random initialization</h4><p id="6135">In 2016, Lee et al. (“<a href="https://arxiv.org/abs/1602.04915" target="_blank"><em class="markup--p-em">Gradient Descent Converges to Minimizers</em></a>”) showed that even without adding noise, <strong class="markup--p-strong">random initialization of parameters lets SGD evade first-order saddle points: “</strong><em class="markup--p-em">gradient descent with a random initialization and sufficiently small constant step size converges to a local minimizer or negative infinity </em><strong class="markup--p-strong"><em class="markup--p-em">almost surely</em></strong>”.</p><p id="ded9">All Deep Learning libraries will let you do this. For example, you could use either of these in Apache MXNet:</p><pre class="graf--pre" id="a3b5">mod.init_params(initializer=mx.init.Normal())<br/>mod.init_params(initializer=mx.init.Xavier())</pre><h4 class="graf-after--pre" id="6bfe">Breaking out of high-order saddle points</h4><p id="f1e2">Noisy SGD and random initialization save us from computing the Hessian, which is a costly operation slowing down the training process. Still, in the previous post, I gave you an example of a third-order saddle point defeating the Hessian. Is there hope for these?</p><p id="90cd">In 2016, <a href="https://twitter.com/animaanandkumar" target="_blank">Anima Anandkumar</a> (a Principal Scientist at AWS) and Rong Ge proposed <strong class="markup--p-strong">the first method to solve third-order saddle points</strong>: “<a href="https://arxiv.org/abs/1602.05908" target="_blank"><em class="markup--p-em">Efficient approaches for escaping higher order saddle points in non-convex optimization</em></a>”. I’m not aware of any Deep Learning library implementing this technique, but please feel free to get in touch if there’s one :)</p><h3 id="ff67">5- Gradient size &amp; distributed training</h3><p id="69ce">Last month, Jeremy Bernstein et al. proposed two variant of SGD called SignSGD and Signum (adding momentum) : “<a href="https://arxiv.org/abs/1802.04434" target="_blank"><em class="markup--p-em">SIGNSGD: compressed optimisation for non-convex problems</em></a>”. They solve the gradient size problem by <strong class="markup--p-strong">sending only the sign of the gradient, not its 32-bit value</strong>! Thus, this reduces the amount of data to send by a factor of 32. The network says thanks ;)</p><p id="da3d">Amazingly, these algorithms converge at the same rate or even faster than Adam, only losing out to highly-tuned SGD (hmmm, again). Here are some results:</p><figure id="6dcf"><img class="graf-image" src="image06.webp"/><figcaption>Learning ImageNet with Signum</figcaption></figure><p id="79ab"><strong class="markup--p-strong">SignSGD is available in Apache MXNet 1.1</strong>. Using it is as simple as:</p><pre class="graf--pre" id="7b86">mod.init_optimizer(optimizer=’signum’)</pre><blockquote class="graf--blockquote graf--hasDropCapModel graf-after--pre" id="2984"><em class="markup--blockquote-em">Another interesting paper was published in late 2017 by Yujun Lin et al. : “</em><a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1712.01887" target="_blank">Deep Gradient Compression: reducing the communication bandwidth for distributed training</a><em class="markup--blockquote-em">”. This is based on a complete different technique: gradients are only sent when updates reach a certain threshold value. Good read.</em></blockquote><h3 class="graf-after--blockquote" id="e66e">Conclusion</h3><p id="5c6a">We covered a lot of ground again. Let’s try to sum things up.</p><ul class="postList"><li id="9104"><strong class="markup--li-strong">Local minima </strong>are not a problem in very large networks. A word of warning: larger networks will be more expensive to train and will overfit the training set faster.</li><li id="7d5f"><strong class="markup--li-strong">Adaptive optimizers</strong> solve the headache of picking a fixed learning and they converge faster.</li><li id="9d1b"><strong class="markup--li-strong">Adam</strong> is a popular choice but <strong class="markup--li-strong">FTML</strong> is the new kid on the block and it means business. Add it to your lineup.</li><li id="7e25"><strong class="markup--li-strong">SGD</strong> can still deliver the best accuracy, at the expense of a lot of complicated tuning. Hopefully hyper-parameter optimization (available in preview in <a href="http://aws.amazon.com/sagemaker" target="_blank">SageMaker</a>) will solve this for us.</li><li id="190b"><strong class="markup--li-strong">Saddle points</strong> is the real problem we didn’t know we had :-/</li><li id="d4c4">Combining <strong class="markup--li-strong">random weight initialization</strong> and a <strong class="markup--li-strong">very small learning rate</strong> is a proven technique to avoid first-order saddle points.</li><li id="276a">Modern optimizers should be able to break out of <strong class="markup--li-strong">first-order and second-order saddle points</strong>. It doesn’t look like we have an off-the-shelf solution for higher-order ones at the moment.</li><li id="973d">Sharing gradients during distributed training can become a severe performance bottleneck: the <strong class="markup--li-strong">SignSGD</strong> optimizer solves the issue with no loss of accuracy.</li></ul><p id="428e">That’s it for today. I hope that you learned a lot and that these techniques will help you build better models.</p><p id="d77a">As always, thanks for reading! Please feel free to reach out on <a href="https://twitter.com/julsimon/" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="8bd4"><em class="markup--p-em">This should be my intro tape at AWS events :D</em></p><figure id="1bca"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/Sp3zaeOyL7Q?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
