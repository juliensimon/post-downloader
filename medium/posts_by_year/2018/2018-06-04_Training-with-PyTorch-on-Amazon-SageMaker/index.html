<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Training with PyTorch on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="db4c">Training with PyTorch on Amazon SageMaker</h3><p id="a226"><a href="https://pytorch.org/" target="_blank"><strong class="markup--p-strong">PyTorch</strong></a> is a flexible open source framework for Deep Learning experimentation. In this post, you will learn how to train PyTorch jobs on <a href="https://aws.amazon.com/sagemaker" target="_blank"><strong class="markup--p-strong">Amazon SageMaker</strong></a>. I’ll show you how to:</p><ul class="postList"><li id="292e">build a <strong class="markup--li-strong">custom Docker container</strong> for CPU and GPU training,</li><li id="1e62">pass <strong class="markup--li-strong">parameters</strong> to a PyTorch script,</li><li id="b576">save the <strong class="markup--li-strong">trained model</strong>.</li></ul><p id="b79f">As usual, you’ll find my code on <a href="https://github.com/juliensimon/dlnotebooks/tree/master/pytorch/01-custom-container" target="_blank">Github</a> :)</p><figure id="c4d3"><img class="graf-image" src="image02.webp"/><figcaption>Uruks train PyTorch on SageMaker. That’s a fact.</figcaption></figure><h3 id="221f">Building a custom container</h3><p id="1a66">SageMaker provides a collection of <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" target="_blank">built-in algorithms</a> as well as environments for TensorFlow and MXNet… but not for PyTorch. Fortunately, developers have the option to <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" target="_blank">build custom containers for training and prediction</a>.</p><p id="6b17">Obviously, a number of <strong class="markup--p-strong">conventions</strong> need to be defined for SageMaker to successfully invoke a custom container:</p><ul class="postList"><li id="cae4"><strong class="markup--li-strong">Name of the training and prediction scripts</strong>: by default, they should respectively be set to ‘<em class="markup--li-em">train</em>’ and ‘<em class="markup--li-em">serve</em>’, be executable and have no extension. SageMaker will start training by running ‘<em class="markup--li-em">docker run your_container train</em>’.</li><li id="2487"><strong class="markup--li-strong">Location of hyper parameters </strong>in the container: <em class="markup--li-em">/opt/ml/input/config/hyperparameters.json</em>.</li><li id="6a68"><strong class="markup--li-strong">Location of input data parameters </strong>in the container: <em class="markup--li-em">/opt/ml/input/data</em>.</li></ul><p id="46f5">This will require some changes in our PyTorch script, the well-known example of <a href="https://github.com/pytorch/examples/blob/master/mnist/main.py" target="_blank">learning MNIST with a simple CNN</a>. As you will see in a moment, they are quite minor and you won’t have any trouble adding them to your own code.</p><h4 id="43b6">Building a Docker container</h4><p id="aa4d">Here’s the <strong class="markup--p-strong">Docker file</strong>.</p><figure id="6c3b"><script src="https://gist.github.com/juliensimon/24713fec3b336ba314d29f8cc7f77f43.js"></script></figure><p id="57aa">We start from the <strong class="markup--p-strong">CUDA 9.0 image</strong>, which is also based on Ubuntu 16.04. This one has all the CUDA libraries that PyTorch needs. We then add <strong class="markup--p-strong">Python 3</strong> and the PyTorch packages.</p><blockquote class="graf--blockquote" id="f242">Unlike MXNet, PyTorch comes in a single package that support both CPU and GPU training.</blockquote><p class="graf-after--blockquote" id="7aa3">Once this is done, we clean up various caches to shrink the container size a bit. Then, we copy the <strong class="markup--p-strong">PyTorch script</strong> to <em class="markup--p-em">/opt/program</em> with the proper name (‘<em class="markup--p-em">train</em>’) and we make it executable.</p><blockquote class="graf--blockquote" id="8ff0">For more flexibility, we could write a generic launcher that would fetch the actual training script from an S3 location passed as an hyper parameter. This is left as an exercise for the reader ;)</blockquote><p class="graf-after--blockquote" id="5a63">Finally, we set the directory of our script as the <strong class="markup--p-strong">work directory</strong> and add it to the <strong class="markup--p-strong">path</strong>.</p><p id="7214">It’s not a long file, but as usual with these things, every detail counts.</p><h4 id="8f5c">Creating a Docker repository in Amazon ECR</h4><p id="753a">SageMaker requires that the containers it fetches are hosted in <a href="http://aws.amazon.com/ecr" target="_blank"><strong class="markup--p-strong">Amazon ECR</strong></a>. Let’s create a repo and login to it.</p><figure id="c801"><script src="https://gist.github.com/juliensimon/08388e7358a618e62f92de1d409f6808.js"></script></figure><h4 id="2ca1">Building and pushing our containers to ECR</h4><p id="c21d">OK, now it’s time to build both containers and push them to their repos. We’ll do this separately for the CPU and GPU versions. Strictly Docker stuff. Please refer to the notebook for details on variables.</p><figure id="da51"><script src="https://gist.github.com/juliensimon/edb9fedb661479a9db77c21fbddd9ed6.js"></script></figure><p id="8b48">Once we’re done, things should look like this and you should also see your container in ECR.</p><figure id="5102"><img class="graf-image" src="image01.webp"/></figure><p id="19eb">The Docker part is over. Now let’s configure our training job in SageMaker.</p><h3 id="4ccd">Configuring the training job</h3><p id="2181">This is actually quite underwhelming, which is great news: <strong class="markup--p-strong">nothing really differs from training with a built-in algorithm!</strong></p><p id="2653">First we need to <strong class="markup--p-strong">upload the MNIST data set</strong> from our local machine to S3. We’ve done this many times before, nothing new here.</p><figure id="0339"><script src="https://gist.github.com/juliensimon/c105a2d19ef6357253e1223bf2e50dae.js"></script></figure><p id="ddf7">Then, we configure the training job by:</p><ul class="postList"><li id="d140">selecting one of the containers we just built and setting the usual parameters for SageMaker <a href="https://sagemaker.readthedocs.io/en/latest/estimators.html" target="_blank"><strong class="markup--li-strong">estimators,</strong></a></li><li id="b235">passing <strong class="markup--li-strong">hyper parameters</strong> to the PyTorch script.</li><li id="f40e">passing <strong class="markup--li-strong">input data</strong> to the PyTorch script.</li></ul><blockquote class="graf--blockquote" id="85c8">Unlike Keras, PyTorch has <a class="markup--blockquote-anchor" href="https://pytorch.org/docs/master/cuda.html" target="_blank">APIs</a> to check if CUDA is available and to detect how many GPUs are available. Thus, we don’t need to pass this information in an hyper parameter. <a class="markup--blockquote-anchor" href="https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html" target="_blank">Multi-GPU training</a> is also possible but requires extra work: MXNet makes it <a class="markup--blockquote-anchor" href="https://mxnet.incubator.apache.org/faq/multi_devices.html" target="_blank">much simpler</a>.</blockquote><figure class="graf-after--blockquote" id="6f6e"><script src="https://gist.github.com/juliensimon/a93769c712b522765bc5c08a2408bb7a.js"></script></figure><p id="cea8">That’s it for training. The last part we’re missing is adapting our PyTorch script for SageMaker. Let’s get to it.</p><h3 id="1286">Adapting the PyTorch script for SageMaker</h3><p id="62c6">We need to take care of hyper parameters, input data, multi-GPU configuration, loading the data set and saving models.</p><h4 id="6d28">Passing hyper parameters and input data configuration</h4><p id="306a">As mentioned earlier, SageMaker copies hyper parameters to <strong class="markup--p-strong"><em class="markup--p-em">/opt/ml/input/config/hyperparameters.json</em></strong>. All we have to do is read this file, extract parameters and set default values if needed.</p><figure id="d9dd"><script src="https://gist.github.com/juliensimon/003c0f78a095fd7d8049a20f549f3477.js"></script></figure><p id="934c">In a similar fashion, SageMaker copies the input data configuration to <em class="markup--p-em">/opt/ml/input/data. </em>We’ll handle things in exactly the same way.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="1d93">In this example, I don’t need this configuration info, but this is how you’d read it if you did :)</blockquote><h4 class="graf-after--blockquote" id="1dbd">Loading the training and validation set</h4><p id="9f98">When training in file mode (which is the case here), SageMaker <strong class="markup--p-strong">automatically</strong> copies the data set to <em class="markup--p-em">/opt/ml/input/&lt;channel_name&gt;</em>: here, we defined the <em class="markup--p-em">train</em> and <em class="markup--p-em">validation</em> channels, so we’ll have to:</p><ul class="postList"><li id="44f4">read the MNIST files from the corresponding directories,</li><li id="0a5d">build <a href="https://pytorch.org/docs/master/data.html" target="_blank"><em class="markup--li-em">DataSet</em></a> objects for the training and validation set,</li><li id="bfaf">load them using the <a href="https://pytorch.org/docs/master/data.html" target="_blank"><em class="markup--li-em">DataLoader</em></a> object.</li></ul><figure id="e439"><script src="https://gist.github.com/juliensimon/6bb0bfeabeb7e0e85b1a86bfafb1ca11.js"></script></figure><h4 id="9b98">Saving the model</h4><p id="031b">The very last thing we need to do once training is complete is to save the model in <strong class="markup--p-strong"><em class="markup--p-em">/opt/ml/model: </em></strong>SageMaker will grab all artefacts present in this directory, build a file called <em class="markup--p-em">model.tar.gz</em> and copy it to the S3 bucket used by the training job.</p><figure id="fcf9"><script src="https://gist.github.com/juliensimon/15eb65e539ade151990b33d5c83e8a11.js"></script></figure><p id="1bda">That’s it. As you can see, it’s all about interfacing your script with SageMaker input and output. The bulk of your PyTorch code doesn’t require any modification.</p><h3 id="0f95">Running the script</h3><p id="7de9">Alright, let’s run this on a <strong class="markup--p-strong">p3.2xlarge</strong> instance.</p><figure id="f403"><script src="https://gist.github.com/juliensimon/aa182bb4bd2da4510f4c6460f0c4143b.js"></script></figure><p id="57be">Let’s check the <strong class="markup--p-strong">S3 bucket</strong>.</p><pre class="graf--pre" id="1639">$ <strong class="markup--pre-strong">aws</strong> s3 ls $BUCKET/pytorch/output/pytorch-mnist-cnn-2018-06-02-08-16-11-355/output/<br/>2018-06-02 08:20:28      86507 model.tar.gz<br/>$ <strong class="markup--pre-strong">aws</strong> s3 cp $BUCKET/pytorch/output/pytorch-mnist-cnn-2018-06-02-08-16-11-355/output/ .<br/>$ <strong class="markup--pre-strong">tar</strong> tvfz model.tar.gz<br/>-rw-r--r-- 0/0   99436 2018-06-02 08:20 mnist-cnn-10.pt</pre><p class="graf-after--pre" id="3d40">Pretty cool, right? We can now use this model <strong class="markup--p-strong">anywhere</strong> we like.</p><p id="ca57">That’s it for today. Another (hopefully) nice example of using SageMaker to <strong class="markup--p-strong">train your custom jobs on fully-managed infrastructure</strong>!</p><p id="6890">Happy to answer questions here or on <a href="https://twitter.com/julsimon" rel="noopener nofollow noopener noopener nofollow noopener noopener noopener noopener noopener noopener" target="_blank">Twitter</a>. For more content, please feel free to check out my <a href="https://www.youtube.com/juliensimonfr" rel="nofollow noopener noopener noopener nofollow noopener noopener noopener noopener noopener noopener" target="_blank">YouTube channel</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="3e64"><em class="markup--p-em">Obvious choice ;)</em></p><figure id="300d"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/M_KTlSbaVUU?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
