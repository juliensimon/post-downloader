<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Building a movie recommender with Factorization Machines on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="dc7b">Building a movie recommender with Factorization Machines on Amazon SageMaker</h3><p id="cb2e"><a href="https://en.wikipedia.org/wiki/Recommender_system" target="_blank">Recommendation</a> is one of the most popular applications in Machine Learning. In this post, you will learn how to build a movie recommendation model based on Factorization Machines — one of the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" target="_blank">built-in algorithms</a> of <a href="https://aws.amazon.com/sagemaker" target="_blank">Amazon SageMaker</a> — and the popular <a href="https://grouplens.org/datasets/movielens/" target="_blank">MovieLens</a> dataset.</p><figure id="965c"><img class="graf-image" src="image01.webp"/><figcaption>Preparing a data set for recommendation, no doubt.</figcaption></figure><p id="52eb">At the time of writing, this use case is not covered by the very nice collection of <a href="https://github.com/awslabs/amazon-sagemaker-examples" target="_blank">SageMaker sample notebooks</a>, so I figured it could help many of you out there if I put one together. Get ready to learn about one-hot encoding, sparse matrices, protobuf files and more :)</p><blockquote class="graf--blockquote" id="3990">As usual, a <a class="markup--blockquote-anchor" href="https://github.com/juliensimon/dlnotebooks/blob/master/sagemaker/03-Factorization-Machines-Movielens.ipynb" target="_blank">Jupyter notebook</a> is available on Github.</blockquote><p class="graf-after--blockquote" id="94fb">Several of my AWS colleagues provided excellent advice as well as debugging tips, so please let me thank <strong class="markup--p-strong">Sireesha Muppala</strong>, <strong class="markup--p-strong">Yuri Astashanok</strong>, <strong class="markup--p-strong">David Arpin</strong> and <strong class="markup--p-strong">Guy Ernest</strong>.</p><h4 id="f1ce">A word about Factorization Machines</h4><p id="13fa">Factorization Machines (FM) are a <strong class="markup--p-strong">supervised Machine Learning technique</strong> introduced in 2010 (<a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank">research paper</a>, PDF). FM get their name from their ability to <strong class="markup--p-strong">reduce problem dimensionality</strong> thanks to <strong class="markup--p-strong">matrix factorization</strong>.</p><p id="41eb">They can be used for <strong class="markup--p-strong">classification</strong> or <strong class="markup--p-strong">regression</strong> and are much more <strong class="markup--p-strong">computationally efficient </strong>on <strong class="markup--p-strong">large sparse data sets</strong> than traditional algorithms like linear regression.This property is why FM are widely used for <strong class="markup--p-strong">recommendation</strong>: user count and item count are typically very large although the actual number of recommendations is very small (users don’t rate all available items!).</p><p id="0558">Here’s a toy example, where a <strong class="markup--p-strong">sparse rating matrix</strong> (dimension 4x4) is factored into a <strong class="markup--p-strong">dense user matrix</strong> (dimension 4x2) and a <strong class="markup--p-strong">dense item matrix</strong> (2x4). As you can see, the <strong class="markup--p-strong">number of factors</strong> (2) is smaller than the number of columns of the rating matrix (4). In addition, this multiplication also lets us <strong class="markup--p-strong">fill all blank values</strong> in the rating matrix, which we can then use to <strong class="markup--p-strong">recommend new items</strong> to any user.</p><figure id="7240"><img class="graf-image" src="image02.webp"/><figcaption>Source: data-artisans.com</figcaption></figure><h4 id="b588">The MovieLens dataset</h4><p id="f4af">This <a href="https://grouplens.org/datasets/movielens/" target="_blank">dataset</a> is a great starting point for recommendation. It comes in multiples sizes and in this post, we’ll use <strong class="markup--p-strong">ml100k</strong>:<strong class="markup--p-strong"> 100,000 ratings</strong> from <strong class="markup--p-strong">943 users</strong> on <strong class="markup--p-strong">1682 movies</strong>. As you can see, the ml100k rating matrix is quite sparse (<strong class="markup--p-strong">93.6%</strong> to be precise) as it only holds <strong class="markup--p-strong">100,000</strong> ratings out of a possible <strong class="markup--p-strong">1,586,126</strong> (943*1682).</p><p id="3250">Here are the first 10 lines in the data set: user #754 gave movie #595 a 2-star rating and so on.</p><pre class="graf--pre" id="cb58"># user id, movie id, rating, timestamp<br/>754	595	2	879452073<br/>932	157	4	891250667<br/>751	100	4	889132252<br/>101	820	3	877136954<br/>606	1277	3	878148493<br/>581	475	4	879641850<br/>13	50	5	882140001<br/>457	59	5	882397575<br/>111	321	3	891680076<br/>123	657	4	879872066</pre><h4 class="graf-after--pre" id="efd5">Data set preparation</h4><p id="5a56">As explained earlier, FM work best on <strong class="markup--p-strong">high-dimension</strong> datasets. As a consequence, we’re going to <a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science" target="_blank"><strong class="markup--p-strong">one-hot encode</strong></a> user ids and movie ids (we’ll ignore timestamps). Thus, each sample in our data set will be a <strong class="markup--p-strong">2,625 boolean vector</strong> (943+1682) with only two values set to 1 with respect to the user id and movie id.</p><p id="bb70">We’re going to build a <strong class="markup--p-strong">binary recommender</strong> (i.e. like/don’t like). 3-star, 4-star and 5-star ratings are set to 1. Lower ratings are set to 0.</p><p id="6b01">One last thing: the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" target="_blank">FM implementation</a> in SageMaker requires training and test data to be stored in <strong class="markup--p-strong"><em class="markup--p-em">float32</em> tensors in </strong><a href="https://developers.google.com/protocol-buffers/" target="_blank"><strong class="markup--p-strong">protobuf</strong></a><strong class="markup--p-strong"> format</strong>. Yes, that sounds complicated :) However, the <a href="https://github.com/aws/sagemaker-python-sdk" target="_blank">SageMaker SDK</a> provides a convenient utility function that takes cares of this, so don’t worry too much about it.</p><h4 id="bef8">The high-level view</h4><p id="5068">OK, then: here are the steps we need to implement:</p><ul class="postList"><li id="a406">load the MovieLens training set and test set from disk,</li><li id="02fc">for each set, build a sparse matrix holding one-hot encoded data samples,</li><li id="fbb1">for each set, build a label vector holding ratings,</li><li id="3457">write both sets to protobuf-encoded files,</li><li id="6306">copy these files to an S3 bucket,</li><li id="576c">configure and run a Factorization Machines training job on SageMaker,</li><li id="4a51">deploy the corresponding model to an endpoint,</li><li id="2a2f">run some predictions.</li></ul><p id="0328">Let’s get going!</p><h4 id="e4d2">Loading the MovieLens dataset</h4><p id="9876">ml-100k contains multiple text files, but we’re only going to use two of them to build our model:</p><ul class="postList"><li id="7216"><strong class="markup--li-strong"><em class="markup--li-em">ua.base</em></strong> (90,570 samples) will be our training set.</li><li id="4640"><strong class="markup--li-strong"><em class="markup--li-em">ua.test</em></strong> (9,430 samples) will be our test set.</li></ul><p id="56a4">Both files have the same <strong class="markup--p-strong">tab-separated format</strong>:</p><ul class="postList"><li id="c3f3"><strong class="markup--li-strong">user id</strong> (integer between 1 and 943),</li><li id="be58"><strong class="markup--li-strong">movie id</strong> (integer between 1 and 1682),</li><li id="712a"><strong class="markup--li-strong">rating</strong> (integer between 1 and 5),</li><li id="dec5"><strong class="markup--li-strong">timestamp</strong> (epoch-based integer).</li></ul><p id="3c29">As a consequence, we’re going to build the following data structures:</p><ul class="postList"><li id="095e"><strong class="markup--li-strong">a training sparse matrix</strong>: 90,570 lines and 2,625 columns (943 one-hot encoded features for the user id, plus 1682 one-hot encoded features for the movie id),</li><li id="18a9"><strong class="markup--li-strong">a training label array</strong>: 90,570 ratings,</li><li id="5225"><strong class="markup--li-strong">a test sparse matrix</strong>: 9,430 lines and 2,625 columns,</li><li id="a4e5"><strong class="markup--li-strong">a test label array</strong>: 9,430 ratings.</li></ul><blockquote class="graf--blockquote" id="7a0b">Reminder: each sample must be a single one-hot encoded feature vector. Yes, you do need to <strong class="markup--blockquote-strong">concatenate the one-hot encoded values for user id, movie id and any additional feature you may add</strong>: building a list of distinct vectors (one for the user id, one for the movie id, etc.) isn’t the right way.</blockquote><p class="graf-after--blockquote" id="15e3">Our training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is <strong class="markup--p-strong">99.92% sparse</strong>. Storing this as a dense matrix would be a massive waste of both <strong class="markup--p-strong">storage</strong> and <strong class="markup--p-strong">computing power</strong>!</p><p id="1fe5">To avoid this, let’s use a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html" target="_blank"><em class="markup--p-em">scipy.lil_matrix</em></a><em class="markup--p-em"> </em><strong class="markup--p-strong">sparse matrix for samples</strong> and a <a href="http://www.numpy.org" target="_blank"><em class="markup--p-em">numpy</em></a> <strong class="markup--p-strong">array for labels</strong>.</p><figure id="b378"><script src="https://gist.github.com/juliensimon/e57c320f630b4ee804cdf33076955704.js"></script></figure><p id="29ef">We should check that we have approximately the same number of samples per class. An unbalanced data set is a serious problem for classifiers.</p><pre class="graf--pre" id="85ab">print(np.count_nonzero(Y_train)/nbRatingsTrain)<br/>0.55<br/>print(np.count_nonzero(Y_test)/nbRatingsTest)<br/>0.58</pre><p class="graf-after--pre" id="6e64">Slightly unbalanced, but nothing bad. Let’s move on!</p><h4 id="8630">Writing to protobuf files</h4><p id="f04e">Next, we’re going to write the training set and the test set to two <strong class="markup--p-strong">protobuf</strong> files stored in <strong class="markup--p-strong">S3</strong>. Fortunately, we can rely on the <strong class="markup--p-strong"><em class="markup--p-em">write_spmatrix_to_sparse_tensor()</em></strong> utility function: it writes our samples and labels into an in-memory protobuf-encoded sparse multi-dimensional array (aka tensor).</p><p id="83be">Then, we commit the buffer to <strong class="markup--p-strong">S3</strong>. Once this step is complete, we’re done with data preparation and can now focus on our training job.</p><figure id="f334"><script src="https://gist.github.com/juliensimon/d2c5b1c0d996cb35a961891cc6a106d6.js"></script></figure><blockquote class="graf--blockquote" id="dd2c">There (could) be dragons. Here are some troubleshooting tips:<br/>- are both samples and labels float32 values?<br/>- are samples stored in a sparse matrix (not a numpy array or anything else)?<br/>- are labels stored in a vector (not any kind of matrix)?<br/>- write_spmatrix_to_sparse_tensor() undefined? It was added in <a class="markup--blockquote-anchor" href="https://github.com/aws/sagemaker-python-sdk/blob/master/CHANGELOG.rst" target="_blank">SDK 1.0.2</a> and you probably need to upgrade the SageMaker SDK: see the appendix at the end of the post.</blockquote><p class="graf-after--blockquote" id="d0ef">Here’s our training set in S3: only <strong class="markup--p-strong">5.5MB.</strong> Sparse matrices FTW!</p><pre class="graf--pre" id="f485">$ aws s3 ls s3://jsimon-sagemaker-us/sagemaker/fm-movielens/train/train.protobuf</pre><pre class="graf--pre graf-after--pre" id="a229">2018-01-28 16:50:29    5796480 train.protobuf</pre><h4 class="graf-after--pre" id="f09c">Running the training job</h4><p id="f07d">Let’s start by creating an <strong class="markup--p-strong">Estimator</strong> based on the FM container available in our region. Then, we have to set some FM-specific <strong class="markup--p-strong">hyper-parameters</strong> (full list in the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html" target="_blank">documentation</a>):</p><ul class="postList"><li id="e0fd"><strong class="markup--li-strong"><em class="markup--li-em">feature_dim</em></strong>: the number of features in each sample (2,625 in our case).</li><li id="1304"><strong class="markup--li-strong"><em class="markup--li-em">predictor_type</em></strong>: ‘<em class="markup--li-em">binary_classifier</em>’ is what we’re going to use.</li><li id="8fe0"><strong class="markup--li-strong"><em class="markup--li-em">num_factors</em></strong>: the common dimension for the user and item matrices (as explained in the example at the start of the post).</li></ul><p id="fa86">The other ones used here are optional (and quite self-explanatory).</p><p id="7335">Finally, let’s run the training job: calling the <strong class="markup--p-strong"><em class="markup--p-em">fit()</em></strong> API is all it takes, passing both the training and test sets hosted in S3. Simple and elegant.</p><figure id="bba1"><script src="https://gist.github.com/juliensimon/fea671b5f6e91f9bdab2126e2d8b20e5.js"></script></figure><p id="241d">A few minutes later, training is complete and we can check out the <strong class="markup--p-strong">training log</strong> either in the notebook or in CloudWatch Logs (in the <em class="markup--p-em">/aws/sagemaker/trainingjobs </em>log group).</p><p id="7f55">After 50 epochs, <strong class="markup--p-strong">test accuracy is 71.5%</strong> and the <a href="https://en.wikipedia.org/wiki/F1_score" target="_blank"><strong class="markup--p-strong">F1 score</strong></a><strong class="markup--p-strong"> </strong>(a typical metric for binary classifier) is <strong class="markup--p-strong">0.75 </strong>(1 indicates a perfect classifier). Not great but with all that sparse matrix and protobuf excitement, I didn’t spend much time tuning hyper-parameters. Surely you can do better :)</p><pre class="graf--pre" id="be34">[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--pre-strong">binary_classification_accuracy</strong><br/>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--pre-strong">0.7159</strong><br/>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : binary_classification_cross_entropy<br/>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : 0.581087609863<br/>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--pre-strong">binary_f_1</strong>.000<br/>[01/29/2018 13:42:41 INFO 140015814588224] #test_score (algo-1) : <strong class="markup--pre-strong">0.74558968389</strong></pre><p class="graf-after--pre" id="ad2b">We have one last step to cover: <strong class="markup--p-strong">model deployment</strong>.</p><h4 id="cb90">Deploying the model</h4><p id="1963">All it takes to deploy the model is a <strong class="markup--p-strong">simple API call</strong>. In the old days (2 months or so ago), this would have required quite a bit of work, even on AWS. Here, just call <strong class="markup--p-strong">deploy()</strong> and voila!</p><figure id="420a"><script src="https://gist.github.com/juliensimon/96a1a63468a4fb52e838e09378d44726.js"></script></figure><p id="c980">We’re now ready to invoke the model’s HTTP endpoint thanks to the <em class="markup--p-em">predict() </em>API. The format for both <strong class="markup--p-strong">request and response data</strong> is <strong class="markup--p-strong">JSON</strong>, which requires us to provide a simple <strong class="markup--p-strong">serializer</strong> to convert our sparse matrix samples to JSON.</p><figure id="f7a5"><script src="https://gist.github.com/juliensimon/e8ca5a3cf2d768ac8962d1902c4d39fa.js"></script></figure><p id="b45a">We’re now able to classify any movie for any user: just build a new data set, process it the same way as the training and test set and use <em class="markup--p-em">predict()</em> to get results. You should also experiment with different <strong class="markup--p-strong">prediction thresholds (</strong>set prediction to 1 above a given score and to 0 under it) and see what value gives you the most efficient recommendations. The MovieLens data set also includes movie titles, so there’s plenty more to explore :)</p><h4 id="67a9">Conclusion</h4><p id="54fb">As you can see, built-in algorithms are a great way to get the job done quickly, without having to write any training code. There’s quite a bit of data preparation involved, but as we saw, it’s key to make very large training jobs <strong class="markup--p-strong">fast</strong> and <strong class="markup--p-strong">scalable</strong>.</p><p id="9e53">If you’re curious about other<strong class="markup--p-strong"> SageMaker built-in algorithms</strong>, here are a couple of previous posts on:</p><ul class="postList"><li id="9d00"><a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" target="_blank">spam classification with XGBoost</a>,</li><li id="69fa"><a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" target="_blank">image classification with Deep Learning</a>.</li></ul><p id="3ec9">In addition, if you’d like to know more about <strong class="markup--p-strong">recommendation systems</strong>, here are a few resources you may find interesting.</p><ul class="postList"><li id="9279">“<a href="https://www.computer.org/csdl/mags/ic/2017/03/mic2017030012.html" target="_blank"><em class="markup--li-em">Two Decades of Recommender Systems at Amazon.com</em></a>” — Research paper.</li><li id="07fa"><a href="https://github.com/amzn/amazon-dsstne" target="_blank">Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine</a> — Github.</li><li id="758c">“<a href="https://aws.amazon.com/blogs/big-data/generating-recommendations-at-amazon-scale-with-apache-spark-and-amazon-dsstne/" target="_blank"><em class="markup--li-em">Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE</em></a>” — AWS blog.</li><li id="7863">“<a href="https://www.youtube.com/watch?v=TjaIKijl-IY" target="_blank"><em class="markup--li-em">A quick demo of Amazon DSSTNE</em></a>” — YouTube video.</li><li id="9600">“<a href="https://www.youtube.com/watch?v=cftJAuwKWkA" target="_blank"><em class="markup--li-em">Using MXNet for Recommendation Modeling at Scale (MAC306)</em></a>” — AWS re:Invent 2016 video.</li><li id="11af">“<a href="https://fr.slideshare.net/AmazonWebServices/building-content-recommendation-systems-using-apache-mxnet-and-gluon-mcl402-reinvent-2017" target="_blank"><em class="markup--li-em">Building Content Recommendation Systems Using Apache MXNet and Gluon (MCL402)</em></a>” — AWS re:Invent 2017 presentation.</li></ul><p id="0e22">As always, thank you for reading. Happy to answer questions on <a href="https://twitter.com/julsimon/" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><h4 id="e4bc">Appendix: upgrading to the latest SageMaker SDK</h4><ul class="postList"><li id="03a9">Open your notebook instance.</li><li id="f03c">On your instance, open a Jupyter terminal.</li><li id="0989">Activate the Conda environment where you’d like to upgrade the SDK, e.g:</li></ul><pre class="graf--pre" id="e965">source activate mxnet_p27</pre><ul class="postList"><li class="graf-after--pre" id="7523">Install the latest <em class="markup--li-em">pip</em> package for SageMaker.</li></ul><pre class="graf--pre" id="6d4f">pip install sagemaker --upgrade</pre><ul class="postList"><li class="graf-after--pre" id="7671">Alternatively, you could also clone the SageMaker SDK and install it. This is the best way to grab the very latest version.</li></ul><pre class="graf--pre" id="1e1e">git clone <a class="markup--pre-anchor" href="https://github.com/aws/sagemaker-python-sdk.git" target="_blank">https://github.com/aws/sagemaker-python-sdk.git</a><br/>cd <a class="markup--pre-anchor" href="https://github.com/aws/sagemaker-python-sdk.git" target="_blank">sagemaker-python-sdk</a><br/>python setup.py sdist<br/>pip install dist/sagemaker-1.x.x.tar.gz --upgrade</pre></div></div></section><section class="section"><div><hr/></div><div><div><p id="bc3d"><em class="markup--p-em">This post was completed during a flight from Paris to Las Vegas, so obviously…</em></p><figure id="0c06"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/gikKRv1hUjk?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
