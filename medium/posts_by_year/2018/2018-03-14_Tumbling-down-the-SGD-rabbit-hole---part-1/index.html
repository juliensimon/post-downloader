<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Tumbling down the SGD rabbit hole — part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="16c8">Tumbling down the SGD rabbit hole — part 1</h3><p id="7934">As could be feared, my Deep Learning excursions have led me in increasingly strange places. Lately, I’ve become quite obsessed with optimizers, maybe the single most important factor in successfully training a Deep Learning model.</p><p id="c189">This post assumes a basic knowledge of Deep Learning (if you’re new to the topic, <a href="https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459" target="_blank">this previous post</a> is probably a better one to read).</p><p id="ce17">We shall start with a basic explanation of <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">Stochatic Gradient Descent</a>, the most commonly used optimization function used in Deep Learning. And then, we’ll fall into darkness :D</p><figure id="0532"><img class="graf-image" src="image02.webp"/><figcaption>It’s a long way down</figcaption></figure><h3 id="073a"><strong class="markup--h3-strong">What Deep Learning really is</strong></h3><p id="2e6d">Deep Learning boils down to:</p><ul class="postList"><li id="4820">building a <strong class="markup--li-strong">data set</strong>,</li><li id="130b">designing a complicated <strong class="markup--li-strong">function</strong> with a l<strong class="markup--li-strong">arge number of parameters</strong> (the neural network),</li><li id="59a3">using the data set, trying to <strong class="markup--li-strong">find a set of parameters</strong> yielding a level of <strong class="markup--li-strong">accuracy</strong> in line with our expectations.</li></ul><p id="b2bf">That’s it. Zooming in on the last item, what we’re really trying to achieve is find a set of parameters for which the <strong class="markup--p-strong">loss function</strong> (which computes the difference between ground truth and predicted values) reaches <strong class="markup--p-strong">a small enough value</strong>.</p><p id="24ee">This set of parameters cannot be computed algebraically: it has to be <strong class="markup--p-strong">approximated</strong> iteratively using an <strong class="markup--p-strong">optimization function</strong>.</p><blockquote class="graf--blockquote" id="c47d">As <a class="markup--blockquote-anchor" href="http://ruder.io" target="_blank">Sebastian Ruder</a> puts it: “« Deep Learning ultimately is about finding a minimum that generalizes well, with bonus points for finding one fast and reliably ». Indeed, we’re not necessarily looking for the absolute minimum, just for one that would generalize “well enough” to solve our business problem.</blockquote><h4 class="graf-after--blockquote" id="01d8">The SGD optimizer</h4><p id="aa0c"><strong class="markup--p-strong">Stochastic Gradient Descent</strong> was invented in 1951 by Robbins and Monro. It’s still very widely used today. All it requires is the ability to <strong class="markup--p-strong">derive</strong> a function, plus a single <strong class="markup--p-strong">hyper-parameter</strong> called the <strong class="markup--p-strong">learning rate</strong> (noted ‘lr’ below: 0.1 is a typical value).</p><p id="7959">Here’s the SGD update rule when the function has a single parameter called ‘w’: <strong class="markup--p-strong">w = w-(lr*f’(w))</strong></p><p id="429f">In plain-speak: we compute the <strong class="markup--p-strong">function derivative at our present location</strong>. This tells us what the <strong class="markup--p-strong">slope</strong> is and which way is <strong class="markup--p-strong">down</strong>. Accordingly, we then take <strong class="markup--p-strong">a small step down</strong> and <strong class="markup--p-strong">repeat</strong> until we get to the <strong class="markup--p-strong">minimum</strong> value for f().</p><p id="f3cb">An example is the best way to explain it :)</p><h4 id="ee6f">SGD with a single parameter</h4><p id="07f5">Let’s apply this to a very simple function : <strong class="markup--p-strong">f(x) = x²</strong>. I hope that you’ll remember from high-school that its derivative is <strong class="markup--p-strong">2x</strong> ;)</p><p id="9cfa">At x=1 (our starting point for this example), the derivative is equal to 2. The slope is positive, which means that if we have to decrease x to get closer to the minimum (If the slope was negative, we’d increase x to get closer to the minimum)</p><blockquote class="graf--blockquote" id="0668">Reember, SGD updates parameters in the <strong class="markup--blockquote-strong">reverse direction of the slope.</strong></blockquote><p class="graf-after--blockquote" id="5de0">Let’s apply SGD with a learning rate of 0.25: x = x-(0.25*2) = 0.5.</p><p id="e6e7">For x=0.5, the derivative is equal to 1. Let’s run another round of SGD:<br/>x = x-(0.25*1) =0.25.</p><p id="d152">After a few iterations, this is what we get.</p><figure id="bd2e"><img class="graf-image" src="image03.webp"/></figure><p id="e04c">As you can see, we gradually get closer and closer to the function’s minimum, i.e. x=0. SGD works, woohoo.</p><blockquote class="graf--blockquote" id="efc9">…unless we pick a very large value — causing divergence — or a very small value — causing intolerably slow convergence. “Neither too small nor too large” seems to be the mantra of Deep Learning practitioners :)</blockquote><h4 class="graf-after--blockquote" id="7367">SGD with many parameters</h4><p id="529e">In a deep neural networks, <strong class="markup--p-strong">each weight is a parameter</strong>. As a consequence, the network’s function has as many dimensions as the network has weights… which could be <strong class="markup--p-strong">millions</strong> and even <strong class="markup--p-strong">tens of millions</strong>. Mind-boggling.</p><p id="1455">Still, SGD works the same. However, for additional performance, we don’t want to update weights one at a time: during the training phase, <a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank">backpropagation</a> should instead update all the weights in a given layer at once (starting from the last layer and moving on to the previous one until it reaches the input layer).</p><p id="5296">To do so, let’s define a <strong class="markup--p-strong">vector storing the weights</strong> for this layer: <br/><strong class="markup--p-strong">L = [w1, w2, …]</strong></p><p id="1b0c">Similarly, let’s define a <strong class="markup--p-strong">vector storing the partial derivatives</strong> of our function. This vector is called the <strong class="markup--p-strong">gradient</strong> (yes, that’s the ‘G’ in ‘SGD): it informs us about the slope in each dimension.</p><p id="8780"><strong class="markup--p-strong">grad(f)(L) = [(df/dW1)(w1), (df/dW2)(w2), … ]</strong></p><p id="2feb">Fortunately, Deep Learning libraries like Apache MXNet or TensorFlow know how to compute partial derivatives automatically, so <strong class="markup--p-strong">we’ll never have to do that ourselves</strong>. Pfeew.</p><p id="20ff">The SGD update now becomes: <strong class="markup--p-strong">L = L-(lr*grad(f)(L))</strong></p><p id="31d0">Here’s a simulated example with two parameters, where <strong class="markup--p-strong">we literally “walk down the mountain” step by step to reach a low point in the “valley”</strong>.</p><figure id="342a"><img class="graf-image" src="image09.webp"/><figcaption>z=x²- xy. Generated with love at academo.org.</figcaption></figure><p id="986d">See? It’s not that complicated: you’ve now conquered SGD. No matter the number of dimensions, we have a <strong class="markup--p-strong">simple iterative algorithm that lets us find the smallest value possible for our function</strong>.</p><p id="d660">Does this <strong class="markup--p-strong">really</strong> work every single time? Well… no :)</p><h3 id="7b51">Problem #1: local minima</h3><p id="3440">A function may exhibit many <strong class="markup--p-strong">local minima</strong>. Look at the weird beast below.</p><figure id="e5bb"><img class="graf-image" src="image01.webp"/><figcaption>z=(cos(x)/x)*(sin(y)/y). Generated with love at academo.org.</figcaption></figure><p id="00b0">Lots of <strong class="markup--p-strong">very shallow local minima</strong>, some <strong class="markup--p-strong">deeper</strong> ones and a <strong class="markup--p-strong">global</strong> one. As we iterate on SGD, we’d definitely want to avoid getting stuck in any of the shallow ones — this could happen if we used a really small learning rate, preventing us from crawling out. Ideally, we’d like to end up that global minimum, but we could end up falling into a deep local one.</p><p id="799d"><strong class="markup--p-strong">Is this a theoretical problem? Do these situations actually take place with deep neural networks? If so, would there be any way to catapult ourselves out of these local minima?</strong></p><h3 id="00e2">Problem #2: slow convergence</h3><p id="e426">Imagine a part of the parameter space where the slope would be <strong class="markup--p-strong">close to zero in every dimension</strong>. Let’s call this a plateau, even if the word doesn’t really make sense in high-dimension spaces.</p><p id="a76d">There, all components of the <strong class="markup--p-strong">gradient</strong> would be close to <strong class="markup--p-strong">zero</strong>, right? Hardly any slope. The consequence would be <strong class="markup--p-strong">near-zero updates for all weights</strong>, which in turn would mean that we would <strong class="markup--p-strong">hardly</strong> move towards the minimum. We’d be <strong class="markup--p-strong">stuck</strong> on that plateau for a long time and training would be extremely <strong class="markup--p-strong">slow</strong>, no matter how much hardware we’d throw at it. Definitely an undesirable situation (unless we’d reached a nice minimum, of course).</p><p id="a4bd"><strong class="markup--p-strong">Could we speed up, i.e. increase the learning rate when slope is minimal?</strong></p><h3 id="89e4">Problem #3: different slopes</h3><p id="b846">Should we really expect all dimensions to have <strong class="markup--p-strong">roughly the same slope</strong>? Or could there be <strong class="markup--p-strong">steep dimensions</strong> — where we’d make good progress quickly — and <strong class="markup--p-strong">flatter dimension</strong>s — where we’d move much slower?</p><p id="0b47">Surely that’s a reasonable hypothesis. As SGD uses the same learning rate for all parameters, this would surely cause <strong class="markup--p-strong">uneven progress</strong> and slow down the training process.</p><p id="7877"><strong class="markup--p-strong">Could we have adapt the learning rate to the slope of each dimension?</strong></p><h3 id="d0d5">Problem #4: saddle points</h3><p id="122a">Now, imagine we’d reach a specific point in the parameter space where <strong class="markup--p-strong">all components of the gradient are actually equal to zero </strong>(yes, these points actually exist, more on this in a minute). What would happen then? <strong class="markup--p-strong">No more weight updates</strong>! We’d be stuck there for all eternity. Doom on us.</p><blockquote class="graf--pullquote" id="c812"><strong class="markup--pullquote-strong">“There are more things</strong> in heaven and earth, Horatio, than are dreamt of in your philosophy”</blockquote><h4 class="graf-after--pullquote" id="d466">Defeating the gradient</h4><p id="61af">Let’s look at an example: <strong class="markup--p-strong">f(x,y) = x²- y²</strong>. This does look like a <strong class="markup--p-strong">horse saddle</strong>, doesn’t it?</p><figure id="368e"><img class="graf-image" src="image11.webp"/><figcaption>The horse saddle. Generated with love at academo.org.</figcaption></figure><p id="9607">Now, let’s compute the <strong class="markup--p-strong">partial derivatives</strong>: df/dx = 2x, df/dy = -2y. Hence, the <strong class="markup--p-strong">gradient</strong> of this function is <strong class="markup--p-strong">[2x, -2y]</strong>. Obviously, at the point (0,0), both components of the gradient are equal to zero.</p><p id="afc2">Looking at the graph above, indeed we see that this point is a <strong class="markup--p-strong">minimum along the x axis</strong> and a <strong class="markup--p-strong">maximum along the y axis</strong>. Such points are called <strong class="markup--p-strong">saddle points</strong>: a minimum or a maximum for every dimension.</p><p id="63b7">Should SGD lead us to this exact place, our training process would be toast as <strong class="markup--p-strong">weights would not be updated any longer</strong>. Our model would probably be no good either since we’d definitely not have reached a minimum value.</p><p id="1756">Saddle points have been proven to be very common in Deep Learning optimization problems (we’ll see in the next post how to try and limit their occurence).For now, let’s see if we can break out!</p><h4 id="9275">Enter the Hessian</h4><p id="7f2b">Our problem here is that we’re only looking at <strong class="markup--p-strong">unidirectional slopes</strong>. Instead, if we looked at the <strong class="markup--p-strong">curvature</strong> around the saddle point (i.e. how much the surface deviates from a plane), then maybe we’d figure out that there’s actually a way down along th y axis.</p><p id="4f34">Studying the curvature of a function requires the computation of <strong class="markup--p-strong">second-order partial derivatives</strong> (ah come on, don’t quit on me now. Hang on!). They’re stored in a square matrix called the <strong class="markup--p-strong">Hessian</strong>.</p><p id="b3cb">Let’s do this for our previous example:</p><ul class="postList"><li id="d14b"><strong class="markup--li-strong">d²f/dx² = 2, d²f/dxdy = 0</strong></li><li id="563f"><strong class="markup--li-strong">d²f/dydx = 0, d²f/dy²= -2</strong></li></ul><p id="8093">Thus, the Hessian for our function is:</p><figure id="1465"><img class="graf-image" src="image08.webp"/></figure><p id="effb">By multiplying this matrix with <strong class="markup--p-strong">unit vectors along the x and y axes</strong>, we’re going to find out what the <strong class="markup--p-strong">curvature</strong> looks like:</p><ul class="postList"><li id="8d7d">H * [0, 1] = [0 -2] = <strong class="markup--li-strong">-2</strong>*[0, 1]</li><li id="d947">H * [0, -1] = [0 2] = <strong class="markup--li-strong">-2</strong>*[0, -1]</li><li id="5189">H * [1, 0] = [2 0] = <strong class="markup--li-strong">2</strong>*[1, 0]</li><li id="0bb4">H * [ -1, 0] = [-2 0] = <strong class="markup--li-strong">2</strong>*[-1, 0]</li></ul><p id="9bde">What do we see here? Multiplying H by a unit vector along the x axis yields a <strong class="markup--p-strong">positive multiple of the vector</strong>: curvature is <strong class="markup--p-strong">positive</strong>, we can only go <strong class="markup--p-strong">up</strong>. Indeed, (0,0) is a minimum along the x axis.</p><p id="4f77">On the contrary, multiplying H by a unit vector along the y axis yields a <strong class="markup--p-strong">negative multiple of the vector</strong>. This indicates a <strong class="markup--p-strong">negative</strong> curvature, which means that there is a way <strong class="markup--p-strong">down</strong>. Victory!</p><blockquote class="graf--blockquote" id="10ba">A little more algebra (hate me, I don’t care): when a non-zero vector v and a square matrix M are such that M*v is a multiple of v, the vector v is called an <strong class="markup--blockquote-strong">eigenvector</strong> of M and the multiple is called an <strong class="markup--blockquote-strong">eigenvalue</strong>. So, whenever we encounter a saddle point, <strong class="markup--blockquote-strong">computing the Hessian and looking for a negative eigenvalue is how we can figure out which way is down</strong>. Ain’t life grand?</blockquote><p class="graf-after--blockquote" id="4230">Now we know how to break out of saddle points. The only problem is that <strong class="markup--p-strong">computing the Hessian is quite expensive</strong>. There are other ways to avoid saddle points, albeit less rigorous ones (more on this in the next post).</p><p id="dacf">Are we out of the woods when it comes to saddle points? Far from it, I’m afraid.</p><blockquote class="graf--pullquote" id="1768">“Hell is empty and all the devils are here.”</blockquote><h4 class="graf-after--pullquote" id="dc69">Defeating the Hessian</h4><p id="0f59">Here’s an example, called the monkey saddle: <strong class="markup--p-strong">f(x,y) = x³- 3xy².</strong></p><figure id="a780"><img class="graf-image" src="image04.webp"/><figcaption>The monkey saddle. Generated with love at academo.org.</figcaption></figure><p id="6575">Let’s compute the <strong class="markup--p-strong">gradient</strong> and the <strong class="markup--p-strong">Hessian </strong>again<strong class="markup--p-strong">.</strong></p><p id="f584">The gradient is <strong class="markup--p-strong">[3x²- 3y², -6xy]</strong>, which is equal to [0, 0] at point (0,0). This is a saddle point again.</p><p id="4758">Accordingly, the Hessian is:</p><figure id="c502"><img class="graf-image" src="image05.webp"/></figure><p id="16fa"><strong class="markup--p-strong">H(0,0) is a zero matrix!</strong> Multiplying it by a unit vector (or any vector, for that matter) will result in a zero vector: <strong class="markup--p-strong">we’re unable to figure out curvature</strong>. Neither the gradient nor the Hessian provide any information on which way is down: such points are <strong class="markup--p-strong">degenerate saddle points</strong>. Oh, the agony…</p><p id="4464"><strong class="markup--p-strong">Should we lose all hope? Are there more advanced techniques to detect these conditions?</strong></p><h3 id="f517">Problem #5: gradient size &amp; distributed training</h3><p id="e616">This last problem is different from the previous ones. Imagine that we’re working with a <strong class="markup--p-strong">very large data set</strong>: <strong class="markup--p-strong">distributing training</strong> — splitting computation across multiple instances — would surely deliver a nice speed up.</p><p id="d5ed">The way this typically works is quite straightforward. Each instance in the training cluster grabs a <strong class="markup--p-strong">batch</strong> of samples, processes it (forward propagation to compute the loss for the batch, then backprogation to compute gradients for all layers). Then, each instance would <strong class="markup--p-strong">send the computed gradients for this batch</strong> to all other instances (or to a central location in charge of propagating them). Each instance would then <strong class="markup--p-strong">update their weights</strong> accordingly.</p><p id="c0a8">All good, right? Not quite. For large models, gradients are actually very heavy: <strong class="markup--p-strong">close to 100MB for ResNet-50</strong>, a popular image classification model. This means that <strong class="markup--p-strong">each instance would have to send 100MB to all other instances after each round of backpropagation!</strong> Even with a limited number of instances, this could quickly become a <strong class="markup--p-strong">performance bottleneck</strong>, stalling our computing efforts and slowing down the training process.</p><p id="19c3"><strong class="markup--p-strong">Could we reduce the gradient size? Compress this data, maybe?</strong></p><h3 id="8fa8">Now what?</h3><p id="1721">In this post, we looked at the <strong class="markup--p-strong">mathematical foundation of SGD</strong>. We learned about <strong class="markup--p-strong">mathematical tools</strong> that help us find which way is down in the quest for a nice minimum. This was a bit math-heavy, but hopefully you made it to the end. Remember, <strong class="markup--p-strong">you can do this</strong>! Stephen Hawking passed away yesterday and he once said:</p><blockquote class="graf--pullquote" id="17b5">Equations are just the boring part of mathematics. I attempt to see things in terms of geometry.</blockquote><p class="graf-after--pullquote" id="2773">I feel that this is great advice when trying to understand Deep Learning.</p><p id="c4b8"><a href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-2-bed3be4761d3" target="_blank"><strong class="markup--p-strong">In the next post</strong></a><strong class="markup--p-strong">, we’ll look at solutions for the issues discussed above…and how to apply them in practice.</strong></p><p id="8492">Thank you for reading.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="0b63"><em class="markup--p-em">In search of the global minimum, we might have to break out from the bottom of the well indeed \m/</em></p><figure id="3bdc"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/2zxsAVCGOZE?feature=oembed" width="700"></iframe></figure><figure id="c0a7"><iframe frameborder="0" height="350" scrolling="no" src="https://upscri.be/8f5f8b?as_embed=true" width="700"></iframe></figure></div><div><figure id="02dc" style="width: 33.333%;"><a href="https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c" target="_blank"><img class="graf-image" src="image10.webp"/></a></figure><figure class="graf--layoutOutsetRowContinue" id="775a" style="width: 33.333%;"><a href="https://upscri.be/8f5f8b" target="_blank"><img class="graf-image" src="image06.webp"/></a></figure><figure class="graf--layoutOutsetRowContinue" id="54ed" style="width: 33.333%;"><a href="https://becominghuman.ai/write-for-us-48270209de63" target="_blank"><img class="graf-image" src="image07.webp"/></a></figure></div></div></section>
</section>
</article></body></html>