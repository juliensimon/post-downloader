<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Yet another 10 Deep Learning projects based on Apache MXNet</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="5d94">Yet another 10 Deep Learning projects based on Apache MXNet</h3><p id="6afd">In previous articles, I listed <a href="https://medium.com/@julsimon/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64" target="_blank">10 Deep Learning projects</a> based on <a href="https://mxnet.incubator.apache.org" rel="nofollow noopener noopener" target="_blank">Apache MXNet</a>…. and then <a href="https://medium.com/@julsimon/10-more-deep-learning-projects-based-on-apache-mxnet-a2dababe455f" target="_blank">10 more</a>… and what do you know, here is another batch!</p><figure id="bdd0"><img class="graf-image" src="image04.webp"/><figcaption>Oh, we’ll get there… eventually.</figcaption></figure><h3 id="fa2b">Models</h3><h4 id="f30b">#1 — Dual Path Networks</h4><p id="2496">This is an implementation of the architecture described on <a href="https://arxiv.org/abs/1707.01629" target="_blank">the self-titled paper</a> by Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan and Jiashi Feng.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="bfb6">This architecture won the <a class="markup--blockquote-anchor" href="http://image-net.org/challenges/LSVRC/2017/results" target="_blank">ImageNet 2017 object localization competition</a> with a top-5 error of <strong class="markup--blockquote-strong">6.22%</strong>.</blockquote><p class="graf-after--blockquote" id="03be">Quoting from the paper: “<em class="markup--p-em">On the ImageNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed</em>”.</p><div class="graf--mixtapeEmbed" id="4e16"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/cypw/DPNs" title="https://github.com/cypw/DPNs"><strong class="markup--mixtapeEmbed-strong">cypw/DPNs</strong><br/><em class="markup--mixtapeEmbed-em">DPNs - Dual Path Networks</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8f572be880d03a39a381cd35497cd4a5" data-thumbnail-img-id="0*hIBcjAuBeO34FE1u." href="https://github.com/cypw/DPNs" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*hIBcjAuBeO34FE1u.);"></a></div><h4 class="graf-after--mixtapeEmbed" id="89d9">#2— Squeeze-and-Excitation Networks</h4><p id="8470">This is an implementation of the architecture described on <a href="https://arxiv.org/abs/1709.01507" target="_blank">the self-titled paper</a> by Jie Hu, Li Shen and Gang Sun.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="096d">This architecture won the <a class="markup--blockquote-anchor" href="http://image-net.org/challenges/LSVRC/2017/results" target="_blank">ImageNet 2017 classification competition</a> with a top-5 error of <strong class="markup--blockquote-strong">2.251%</strong>.</blockquote><div class="graf--mixtapeEmbed graf-after--blockquote" id="1b98"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/bruinxiong/SENet.mxnet" title="https://github.com/bruinxiong/SENet.mxnet"><strong class="markup--mixtapeEmbed-strong">bruinxiong/SENet.mxnet</strong><br/><em class="markup--mixtapeEmbed-em">SENet.mxnet — :fire::fire: A MXNet implementation of Squeeze-and-Excitation Networks (SE-ResNext, SE-Resnet…</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="055b985f09e7a6c9aa6b6e85c7cddd43" data-thumbnail-img-id="0*sA048PAKJ0aMiMLj." href="https://github.com/bruinxiong/SENet.mxnet" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*sA048PAKJ0aMiMLj.);"></a></div><h4 class="graf-after--mixtapeEmbed" id="ff53">#3 — Capsule Networks (Symbolic API)</h4><p id="369f">This project implements the CapsNet architecture presented in the “<a href="https://arxiv.org/abs/1710.09829" target="_blank"><strong class="markup--p-strong">Dynamic Routing Between Capsules</strong></a>” paper by Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. In a nutshell, capsule networks are an exciting new development designed to <strong class="markup--p-strong">overcome the limitations of convolutional neural networks</strong>.</p><div class="graf--mixtapeEmbed" id="fc27"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/Soonhwan-Kwon/capsnet.mxnet" title="https://github.com/Soonhwan-Kwon/capsnet.mxnet"><strong class="markup--mixtapeEmbed-strong">Soonhwan-Kwon/capsnet.mxnet</strong><br/><em class="markup--mixtapeEmbed-em">capsnet.mxnet - MXNet implementation of CapsNet</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6c7bc2f35d611f4d471ac174385bf95e" data-thumbnail-img-id="0*x5QAIfyzNC6bKo0u." href="https://github.com/Soonhwan-Kwon/capsnet.mxnet" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*x5QAIfyzNC6bKo0u.);"></a></div><p class="graf-after--mixtapeEmbed" id="432d">This code achieves <strong class="markup--p-strong">99.71%</strong> accuracy on the <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">MNIST</a> dataset, which is in line with the scores reported in the paper.</p><figure id="069d"><img class="graf-image" src="image03.webp"/></figure><h4 id="fdef">#4 — Capsule Networks (Gluon API)</h4><p id="adf6">This project also implements the CapsNet architecture, but it does so using the imperative Gluon API (here’s an <a href="https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55" target="_blank">introduction</a> to Gluon if you’re not familiar with it).</p><div class="graf--mixtapeEmbed" id="c145"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/GarrickLin/Capsnet.Gluon" title="https://github.com/GarrickLin/Capsnet.Gluon"><strong class="markup--mixtapeEmbed-strong">GarrickLin/Capsnet.Gluon</strong><br/><em class="markup--mixtapeEmbed-em">Capsnet.Gluon - Capsule Net implementation in Gluon</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="10f76d1c47fcf6780fcde75061cf382f" data-thumbnail-img-id="0*-gAuqMBIEs1HySHJ." href="https://github.com/GarrickLin/Capsnet.Gluon" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*-gAuqMBIEs1HySHJ.);"></a></div><p class="graf-after--mixtapeEmbed" id="a0e2">This implementation achieves <strong class="markup--p-strong">99.53%</strong> accuracy on MNIST, which the author suggests could be improved by adding more data augmentation.</p><figure id="f029"><img class="graf-image" src="image02.webp"/></figure><h4 id="99ab">#5 — MobileNets</h4><p id="3472">This is an implementation of the architecture described in “<a href="https://arxiv.org/abs/1704.04861" target="_blank"><strong class="markup--p-strong">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</strong></a>” by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto and Hartwig Adam.</p><p id="f08a">Quoting from the paper: MobileNets are “<em class="markup--p-em">a class of efficient models (…) for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks”.</em></p><p id="05f4">A model pre-trained on ImageNet is provided, with a top-5 accuracy of <strong class="markup--p-strong">90.15%</strong>.</p><div class="graf--mixtapeEmbed" id="368a"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/KeyKy/mobilenet-mxnet" title="https://github.com/KeyKy/mobilenet-mxnet"><strong class="markup--mixtapeEmbed-strong">KeyKy/<em class="markup--mixtapeEmbed-em">mobilenet-mxnet</em></strong><br/>mobilenet-mxnetgithub.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="3771bcd89206fb5a4537d2c3cbcfe59e" data-thumbnail-img-id="0*Ld142kgCPFl-Ui4A." href="https://github.com/KeyKy/mobilenet-mxnet" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Ld142kgCPFl-Ui4A.);"></a></div><h3 class="graf-after--mixtapeEmbed" id="f225">Applications</h3><h4 id="bece">#6— Face Recognition</h4><p id="bd47">This is an implementation of the architecture described in “<a href="https://arxiv.org/abs/1801.07698" target="_blank"><strong class="markup--p-strong">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</strong></a>” by Jiankang Deng, Jia Guo, and Stefanos Zafeiriou.</p><p id="5453">InsightFace is a new face recognition method, which achieves state-of-the art scores of <strong class="markup--p-strong">99.80%</strong>+ on <a href="http://vis-www.cs.umass.edu/lfw/" target="_blank">LFW</a> and <strong class="markup--p-strong">98%</strong>+ on <a href="http://megaface.cs.washington.edu/" target="_blank">Megaface</a>.</p><div class="graf--mixtapeEmbed" id="cd9e"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/deepinsight/insightface" title="https://github.com/deepinsight/insightface"><strong class="markup--mixtapeEmbed-strong">deepinsight/insightface</strong><br/><em class="markup--mixtapeEmbed-em">insightface - Face Recognition Project on MXNet</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="76b0674adaee07e3e24509753b4e5c1c" data-thumbnail-img-id="0*3uOAKjCdBCM3Z1qf." href="https://github.com/deepinsight/insightface" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*3uOAKjCdBCM3Z1qf.);"></a></div><h4 class="graf-after--mixtapeEmbed" id="6491">#7 — Speech to Text</h4><p id="35db">This is an implementation of the architecture described in “<a href="https://arxiv.org/abs/1512.02595" target="_blank"><strong class="markup--p-strong">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</strong></a>” by Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan and Zhenyao Zhu (pfew!).</p><p id="ac10">This is a great project if you want to build a speech-to-text model. Please bear in mind that you will need a <strong class="markup--p-strong">very large dataset</strong>. Quoting from the original paper: “<em class="markup--p-em">our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours. We use data synthesis to further augment the data during training</em>”.</p><div class="graf--mixtapeEmbed" id="a6da"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/samsungsds-rnd/deepspeech.mxnet" title="https://github.com/samsungsds-rnd/deepspeech.mxnet"><strong class="markup--mixtapeEmbed-strong">samsungsds-rnd/deepspeech.mxnet</strong><br/><em class="markup--mixtapeEmbed-em">deepspeech.mxnet - A MXNet implementation of Baidu's DeepSpeech architecture</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="c5073d0e712a982054b72397063f4643" data-thumbnail-img-id="0*N-HrDEp_Nig8Whmh." href="https://github.com/samsungsds-rnd/deepspeech.mxnet" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*N-HrDEp_Nig8Whmh.);"></a></div><h4 class="graf-after--mixtapeEmbed" id="6985">#8— 3D face reconstruction</h4><p id="b312">This is an implementation of the architecture described in “<strong class="markup--p-strong">End-to-end 3D face reconstruction with deep neural networks</strong>” by Pengfei Dou, Shishir K. Shah and Ioannis A. Kakadiaris.</p><p id="1644">Thanks to this project, you can build a 3D model of a a face using only a single 2D image. Quite impressive!</p><div class="graf--mixtapeEmbed" id="f6c2"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/ShownX/mxnet-E2FAR" title="https://github.com/ShownX/mxnet-E2FAR"><strong class="markup--mixtapeEmbed-strong">ShownX/mxnet-E2FAR</strong><br/><em class="markup--mixtapeEmbed-em">mxnet-E2FAR - MXNET/Gluon Implementation of End-to-end 3D Face Reconstruction with Deep Neural Networks</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1a63d0926f01666c287e723412d0266c" data-thumbnail-img-id="0*_EgYe6FUVrTm-7VN." href="https://github.com/ShownX/mxnet-E2FAR" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*_EgYe6FUVrTm-7VN.);"></a></div><figure class="graf-after--mixtapeEmbed" id="aabc"><img class="graf-image" src="image01.webp"/><figcaption>Examples taken from the original paper</figcaption></figure><h3 id="1213">Tools</h3><h4 id="a453">#9 — Deepo</h4><p id="6e4b">Deepo is a set of pre-built containers for Deep Learning. It supports MXNet as well as other frameworks. Containers will run on Linux (CPU/GPU), Windows (CPU) and MacOS (CPU) with either Python 2.7 or Python 3.6.</p><div class="graf--mixtapeEmbed" id="9a86"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/ufoym/deepo" title="https://github.com/ufoym/deepo"><strong class="markup--mixtapeEmbed-strong">ufoym/deepo</strong><br/><em class="markup--mixtapeEmbed-em">deepo - A series of Docker images (and their generator) that allows you to quickly set up your deep learning research…</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8c90ade65399c50c200e12b7a79bb0f3" data-thumbnail-img-id="0*oYFsnsjVoVZl0tYO." href="https://github.com/ufoym/deepo" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*oYFsnsjVoVZl0tYO.);"></a></div><p class="graf-after--mixtapeEmbed" id="01df">This is pretty handy if you want to work locally, and of course on AWS with one of our Docker services: <a href="http://aws.amazon.com/ecs" target="_blank">ECS</a>, <a href="http://aws.amazon.com/eks" target="_blank">EKS</a> or <a href="http://aws.amazon.com/fargate" target="_blank">Fargate</a>.</p><h4 id="166e">#10 — MXNet finetuner</h4><p id="d5e9">This tool simplifies the process of fine-tuning an image classification dataset on your own dataset (here’s an <a href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" target="_blank">introduction to fine-tuning</a> if you’re unfamiliar with this technique).</p><p id="3526">It wil automatically build RecordIO files from a tree of images, download pre-trained models, replace the last layer according to the number of classes in your dataset, add data augmentation, run fine-tuning, visualize results, etc.</p><p id="2484">Good stuff!</p><div class="graf--mixtapeEmbed" id="c454"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/knjcode/mxnet-finetuner" title="https://github.com/knjcode/mxnet-finetuner"><strong class="markup--mixtapeEmbed-strong">knjcode/mxnet-finetuner</strong><br/><em class="markup--mixtapeEmbed-em">mxnet-finetuner - An all-in-one Deep Learning toolkit for image classification to fine-tuning pretrained models using…</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="83d94166eb7bd151ff7e6fb5e1cd27b0" data-thumbnail-img-id="0*otZg4sS6BoMonC31." href="https://github.com/knjcode/mxnet-finetuner" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*otZg4sS6BoMonC31.);"></a></div></div></div></section><section class="section"><div><hr/></div><div><div><p id="9e9d">That’s it for today. Kudos to all project authors for their fascinating work. I hope they will inspire you to <a href="https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459" target="_blank">get started with Deep Learning</a>.</p><p id="f0dc">As always, thanks a lot for reading!</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="c47c"><em class="markup--p-em">One of the most addictive albums I’ve heard in years. Listen once, sing it forever \m/</em></p><figure id="786d"><iframe frameborder="0" height="380" scrolling="no" src="https://open.spotify.com/embed/album/3xlz8I3SbJc8UsZy7dyBUD" width="300"></iframe></figure></div></div></section>
</section>
</article></body></html>