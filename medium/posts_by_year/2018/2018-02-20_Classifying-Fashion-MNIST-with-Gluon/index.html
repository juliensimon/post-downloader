<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Classifying Fashion-MNIST with Gluon</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="72fa">Classifying Fashion-MNIST with Gluon</h3><p id="fb31">In a <a href="https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55" target="_blank">previous post</a>, we took a first look at the <strong class="markup--p-strong">Gluon API</strong>, a <a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#gluon-api" rel="nofollow noopener noopener" target="_blank">high-level API</a> for built on top of <a href="http://mxnet.incubator.apache.org" rel="nofollow noopener noopener" target="_blank">Apache MXNet</a>.</p><p id="490d">In this post, we’ll keep exploring Gluon but first we need a cool dataset to work with.</p><blockquote class="graf--blockquote" id="ae28">Code is available on <a class="markup--blockquote-anchor" href="https://github.com/juliensimon/dlnotebooks" target="_blank">Github</a>.</blockquote><h4 class="graf-after--blockquote" id="4dbf">The Fashion-MNIST dataset</h4><p id="9e55">Put together by e-tailer <a href="https://jobs.zalando.com/tech/blog/" target="_blank">Zalando</a>, <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank"><strong class="markup--p-strong">Fashion-MNIST</strong></a> is a <strong class="markup--p-strong">drop-in replacement</strong> for the well-known (and probably over-used) <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">MNIST</a> dataset: same number of samples, same number of classes and same filenames! You’ll find plenty of details in this <a href="https://arxiv.org/abs/1708.07747" target="_blank">technical report</a>.</p><p id="ff29">Instead of digits, this data set contains the following <strong class="markup--p-strong">fashion items</strong>: t-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag and ankle boot. Some items look very similar, which is likely to make our classification job harder.</p><figure id="17af"><img class="graf-image" src="image04.webp"/><figcaption>Samples from Fashion-MNIST</figcaption></figure><h4 id="a15e">Loading the dataset</h4><p id="120d">Thanks to the <strong class="markup--p-strong">Gluon vision API</strong>, it couldn’t be simpler to load the training set and the validation set. Each sample is a 28x28 greyscale image shaped (28, 28, 1). We’ll use a simple transform function to reshape it to (1,28,28).</p><figure id="f23e"><script src="https://gist.github.com/juliensimon/6a3fef5400df6f2a5fec486ccc90139e.js"></script></figure><h4 id="0aa8"><strong class="markup--h4-strong">Building a configurable Convolutional Neural Network</strong></h4><p id="efd2">We’re going to try out a number of network architectures, so let’s write a function that lets us build a variety of CNNs from the following <a href="https://mxnet.incubator.apache.org/api/python/gluon.html" target="_blank">Gluon layers</a>:</p><ul class="postList"><li id="831c"><strong class="markup--li-strong">Convolution</strong> and <strong class="markup--li-strong">Max Pooling</strong> layers, with parameters for kernel size, padding, pooling and stride.</li><li id="525a"><strong class="markup--li-strong">Dense</strong> layer for final classification,</li><li id="d3d3"><strong class="markup--li-strong">Dropout</strong> and <strong class="markup--li-strong">Batch Normalization</strong> layers to fight overfitting and help our network learn better.</li></ul><p id="523b">By default, we’ll use the <strong class="markup--p-strong">ReLU activation function</strong> but let’s also plan for <strong class="markup--p-strong">Leaky ReLU</strong>, which is a separate layer in Gluon.</p><figure id="ba40"><script src="https://gist.github.com/juliensimon/fe8edac20205f63c99f7c0ab7205dda3.js"></script></figure><p id="ac59">Thanks to this function, we can now build a variety of CNNs with one single line of code. Here’s a simple example.</p><figure id="52a0"><script src="https://gist.github.com/juliensimon/08c2a83bd3be3bb3cad834ff85aebe3e.js"></script></figure><p id="1b13">The <strong class="markup--p-strong">flexibility</strong> of the Gluon API is really great here. I must admit that this would have been more work with the <a href="https://mxnet.incubator.apache.org/api/python/symbol.html" target="_blank">symbolic API</a> in MXNet :)</p><p id="970a"><strong class="markup--p-strong">Initializing the model</strong></p><p id="0f7a">We have to initialize weights, pick an optimizer and set its parameters. Nothing unusual. Let’s settle for Xavier initialization, but feel free to try <a href="https://mxnet.incubator.apache.org/api/python/optimization.html" target="_blank">something else</a>.</p><figure id="17dd"><script src="https://gist.github.com/juliensimon/1792f21b1670516ae9764def5fca0ea6.js"></script></figure><h4 id="0233">Computing accuracy</h4><p id="5ff6">During training, we’d like to measure <strong class="markup--p-strong">training</strong> and <strong class="markup--p-strong">validation</strong> accuracy. Let’s use an MXNet <a href="https://mxnet.incubator.apache.org/api/python/metric.html" target="_blank"><strong class="markup--p-strong">metric</strong></a> to compute them.</p><figure id="38f0"><script src="https://gist.github.com/juliensimon/bcaeba6f98381f35081218e3d2d785d2.js"></script></figure><h4 id="aa31">Training the model</h4><p id="f547">Our training loop is the standard Gluon training loop:</p><ul class="postList"><li id="b06b">Iterate over <strong class="markup--li-strong">epochs</strong> and <strong class="markup--li-strong">batches</strong>,</li><li id="0bdd">Record <strong class="markup--li-strong">gradients</strong> while propagating, computing the <strong class="markup--li-strong">loss function</strong> and back propagating,</li><li id="79a7">Applying a <strong class="markup--li-strong">training step</strong>, i.e. updating the weights.</li></ul><p id="b3b1">In the process, we’re also computing accuracies and storing their values for plotting purposes.</p><figure id="0930"><script src="https://gist.github.com/juliensimon/8d29330b068fdfe8490b24ea90af220a.js"></script></figure><h4 id="3b77">Measuring and plotting accuracy</h4><p id="462e">Once training is complete, let’s plot training accuracy and validation accuracy vs epochs. As usual, we’ll use <a href="http://matplotlib.org" target="_blank">matplotlib</a>. Pretty standard stuff, right?</p><figure id="ca81"><script src="https://gist.github.com/juliensimon/358c8a0db3fcd5d1c14819ab55bfba7b.js"></script></figure><h4 id="9c77">Strategy</h4><p id="7dff">Network architecture, hyper parameters, layer parameters: so many combinations to explore… Let’s try to set some guidelines.</p><p id="2443">We’ll start from a <strong class="markup--p-strong">basic CNN</strong>, known to work well on the <strong class="markup--p-strong">MNIST</strong> dataset. We’ll apply it as is to Fashion-MNIST to get a baseline.</p><p id="67f0">First, we’ll work on getting the <strong class="markup--p-strong">best training performance</strong> possible, making sure that the network is large enough to learn the training dataset.</p><p id="e7eb">We’ll probably end up <strong class="markup--p-strong">overfitting</strong> it in the process, which is why we’ll then work on <strong class="markup--p-strong">improving validation accuracy</strong>.</p><p id="346b">Very well then, let’s get to work!</p><h4 id="f11c">First try: basic CNN</h4><p id="826d">The following network scores <strong class="markup--p-strong">99.2%</strong> validation accuracy on MNIST.</p><ul class="postList"><li id="4be0"><strong class="markup--li-strong">Convolutional layer</strong> with 64 3x3 filters, padding and stride set to 1 (1x28x28 → 64x28x28). This layer doesn’t shrink the image (a.k.a. ‘same’ convolution) as it’s quite small already.</li><li id="0179"><strong class="markup--li-strong">Max Pooling layer</strong> with 2x2 pooling and stride set to 2 (64x28x28 → 64x13x13).</li><li id="718c"><strong class="markup--li-strong">Convolutional layer</strong> with 64 3x3 filters, padding and stride set to 1 (64x13x13 → 64x10x10).</li><li id="46e1"><strong class="markup--li-strong">Max Pooling layer</strong> with 2x2 pooling and stride set to 2 (64x10x10 → 64x5x5)</li><li id="7f5a"><strong class="markup--li-strong">Flatten layer</strong> (64x5x5 →1600)</li><li id="f35a"><strong class="markup--li-strong">Fully connected layer</strong> with 256 neurons (1600 → 256).</li><li id="8b42"><strong class="markup--li-strong">Fully connected layer</strong> with 64 neurons (256 → 64).</li><li id="961b"><strong class="markup--li-strong">Output layer</strong> with 10 neurons (64 → 10).</li></ul><p id="0243">We’ll use <strong class="markup--p-strong">ReLU</strong> for all activation layers.</p><figure id="305a"><script src="https://gist.github.com/juliensimon/c7f7af159fcebe758f02dd7dd26773f5.js"></script></figure><p id="9ded">Here’s the result after 50 epochs (<a href="https://gist.github.com/juliensimon/e9b318825d864978f254ebd5afcd0720" target="_blank">training log</a>).</p><figure id="4efd"><img class="graf-image" src="image01.webp"/><figcaption>Epoch#49 Training=<strong class="markup--figure-strong">0.9988</strong> Validation=<strong class="markup--figure-strong">0.9242</strong></figcaption></figure><p id="9980">Top validation accuracy is<strong class="markup--p-strong"> 92.42%</strong>. This is significantly <strong class="markup--p-strong">lower</strong> than the MNIST score (99.2%), which goes to show that Fashion MNIST is indeed more difficult to learn. Good :-&gt;</p><p id="5669">On the bright side, it does look like this network is capable of learning the dataset. It also scored much higher than all <strong class="markup--p-strong">non Deep Learning</strong> based techniques <a href="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/" target="_blank">benchmarked</a> on Fashion-MNIST (the top one is a variation of Support Vector Machines at 89.7%).</p><p id="a1e0">So, hurrah for Deep Learning, but let’s improve this score, shall we?</p><h4 id="3b4b">Second try: use a better optimizer</h4><p id="f4e4">We used SGD with a fixed learning rate, which is ok to get a quick feeling for how the network performs. However, more <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank"><strong class="markup--p-strong">modern optimizers</strong></a> will definitely improve performance.</p><p id="2c7f">Popular choices includes <strong class="markup--p-strong">AdaDelta</strong> (<a href="https://arxiv.org/abs/1212.5701" target="_blank">paper</a>), <strong class="markup--p-strong">AdaGrad</strong> (<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank">pdf</a>) or <strong class="markup--p-strong">Adam</strong> (<a href="https://arxiv.org/abs/1412.6980" target="_blank">paper</a>). Which one should we pick? It looks like everyone tends to rely on Adam, so let’s try it for 50 epochs (<a href="https://gist.github.com/juliensimon/ddfe77f496ff116e134f388f26224095" target="_blank">training log</a>).</p><figure id="5f21"><img class="graf-image" src="image05.webp"/><figcaption>Epoch#10 Training=<strong class="markup--figure-strong">0.9716</strong> Validation=<strong class="markup--figure-strong">0.9305</strong></figcaption></figure><p id="0c42">Top validation accuracy is<strong class="markup--p-strong"> 93.05% </strong>at epoch #10 (!). Adam does learn very fast indeed.</p><h4 id="6388">Third try: add Batch Normalization</h4><p id="1b00"><strong class="markup--p-strong">Batch Normalization</strong> (<a href="https://arxiv.org/abs/1502.03167" target="_blank">paper</a>) is a technique that helps <strong class="markup--p-strong">train faster</strong> and <strong class="markup--p-strong">avoid overfitting</strong> by normalizing values for each training batch. The authors recommend applying it to the inputs of activation layers.</p><p id="4d34">Let’s update our network accordingly and use this technique for both the convolutional layers and fully connected layers.</p><figure id="5a72"><script src="https://gist.github.com/juliensimon/664bb2daec8467d308fd7df11c6020ac.js"></script></figure><p id="c2cc">Here’s the result (<a href="https://gist.github.com/juliensimon/58e1ba6271c5c1d09241ad8a9ef6b7b5" target="_blank">training log</a>).</p><figure id="6e5f"><img class="graf-image" src="image02.webp"/><figcaption>Epoch#45 Training=<strong class="markup--figure-strong">0.9994</strong> Validation=<strong class="markup--figure-strong">0.9331</strong></figcaption></figure><p id="4408">Compared to previous runs, this one learned <strong class="markup--p-strong">even faster</strong>. With respect to validation accuracy, we got a small improvement at <strong class="markup--p-strong">93.31%</strong>.</p><h4 id="d7da">Fourth try: add Dropout</h4><p id="05fe">Training performance is now very good. Let’s now work on improving <strong class="markup--p-strong">validation accuracy</strong>. Batch Normalization did help a bit, but we should be able to do even better by adding <strong class="markup--p-strong">Dropout</strong> layers.</p><p id="78db">Dropout (<a href="https://arxiv.org/abs/1207.0580" target="_blank">paper</a>) is a technique that <strong class="markup--p-strong">randomly sets to zero a configurable fraction of connections</strong> between two layers. By throwing this wrench into the training process, we slow it down, make it <strong class="markup--p-strong">work harder</strong> at figuring out <strong class="markup--p-strong">unexpected inputs</strong> and hopefully help the model <strong class="markup--p-strong">generalize better</strong>.</p><p id="82d3">Let’s add <strong class="markup--p-strong">30% dropout</strong> after each convolution block and before each Dense layer. That’s a lot of Dropout: training should be much slower, so we’ll train for 100 epochs.</p><figure id="c71a"><script src="https://gist.github.com/juliensimon/66ebb8f24c1fe41adcbfad7699dca764.js"></script></figure><p id="9714">Here’s the <a href="https://gist.github.com/juliensimon/2f120c911ddc7d604c93c7e24408e40b" target="_blank">training log</a>. The best validation accuracy is reached at epoch #72: <strong class="markup--p-strong">94.39%</strong>. Dropout helped us squeeze <strong class="markup--p-strong">an extra 1%</strong> accuracy!</p><figure id="8b9e"><img class="graf-image" src="image03.webp"/><figcaption>Epoch#72 Training=<strong class="markup--figure-strong">0.9956</strong> Validation=<strong class="markup--figure-strong">0.9439</strong></figcaption></figure><h4 id="c335">Now what?</h4><p id="f6d9">I’m sure we could go higher if we kept experimenting: tuning dropout , trying out different activation functions like Leaky ReLU, using data augmentation, maybe adding more convolution kernels and so on. This is a rather long post already, so let’s stop there :) However, please <strong class="markup--p-strong">keep tweaking</strong>, it’s the best way to learn (pun not intended) and the Gluon API makes it particularly easy to <strong class="markup--p-strong">build networks programatically</strong>.</p><blockquote class="graf--blockquote" id="6be8">Just out of curiosity, I ran this improved network on MNIST and got to 99.52% accuracy after only 16 epochs!</blockquote><p class="graf-after--blockquote" id="7d41">As always, thanks for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon/" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="2d37"><em class="markup--p-em">Improving validation accuracy… there’s no way but the hard way :*)</em></p><figure id="9331"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/4DHHv5OuEzY?feature=oembed" width="640"></iframe></figure></div></div></section>
</section>
</article></body></html>
