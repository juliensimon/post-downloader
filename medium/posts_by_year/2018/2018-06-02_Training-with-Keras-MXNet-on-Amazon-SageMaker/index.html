<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Training with Keras-MXNet on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="76a1">Training with Keras-MXNet on Amazon SageMaker</h3><p id="a226">As <a href="https://medium.com/@julsimon/apache-mxnet-as-a-backend-for-keras-2-9993f97843e7" target="_blank">previously discussed</a>, <a href="https://mxnet.apache.org/" target="_blank"><strong class="markup--p-strong">Apache MXNet</strong></a> is now available as a backend for <a href="https://keras.io" target="_blank"><strong class="markup--p-strong">Keras 2</strong></a>, aka <a href="https://github.com/awslabs/keras-apache-mxnet" target="_blank"><strong class="markup--p-strong">Keras-MXNet</strong></a>.</p><p id="6add">In this post, you will learn how to train Keras-MXNet jobs on <a href="https://aws.amazon.com/sagemaker" target="_blank"><strong class="markup--p-strong">Amazon SageMaker</strong></a>. I’ll show you how to:</p><ul class="postList"><li id="292e">build <strong class="markup--li-strong">custom Docker containers</strong> for CPU and GPU training,</li><li id="c8bb">configure <strong class="markup--li-strong">multi-GPU training</strong>,</li><li id="1e62">pass <strong class="markup--li-strong">parameters</strong> to a Keras script,</li><li id="b576">save the <strong class="markup--li-strong">trained models</strong> in Keras and MXNet formats.</li></ul><p id="b79f">As usual, you’ll find my code on <a href="https://github.com/juliensimon/dlnotebooks/tree/master/keras/01-custom-container" target="_blank">Github</a> :)</p><figure id="c342"><img class="graf-image" src="image01.webp"/><figcaption>That’s how it feels when your custom container runs without error :)</figcaption></figure><h3 id="e152">Configuring Keras for MXNet</h3><p id="feca">All it takes is really setting the ‘<em class="markup--p-em">backend</em>’ to ‘<em class="markup--p-em">mxnet’</em> in <em class="markup--p-em">.keras/keras.json</em>, but setting ‘<em class="markup--p-em">image_data_format</em>’ to ‘<em class="markup--p-em">channels_first</em>’ will make MXNet training faster.</p><blockquote class="graf--blockquote" id="9940">When working with image data, the input shape can either be ‘channels_first’, i.e. (number of channels, height, width), or ‘channels_last’, i.e. (height, width, number of channels). For MNIST, this would either be (1, 28, 28) or (28, 28, 1) : one channel (black and white pictures), 28 pixels by 28 pixels. For ImageNet, it would be (3, 224, 224) or (224, 224, 3): three channels (red, green and blue), 224 pixels by 224 pixels.</blockquote><p class="graf-after--blockquote" id="3ba9">Here’s the <strong class="markup--p-strong">configuration file</strong> we’ll use for our container.</p><figure id="beaf"><script src="https://gist.github.com/juliensimon/06f9a9af6728747fbce0e6261d2ee80b.js"></script></figure><h3 id="221f">Building custom containers</h3><p id="1a66">SageMaker provides a collection of <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" target="_blank">built-in algorithms</a> as well as environments for TensorFlow and MXNet… but not for Keras. Fortunately, developers have the option to <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" target="_blank">build custom containers for training and prediction</a>.</p><p id="6b17">Obviously, a number of <strong class="markup--p-strong">conventions</strong> need to be defined for SageMaker to successfully invoke a custom container:</p><ul class="postList"><li id="cae4"><strong class="markup--li-strong">Name of the training and prediction scripts</strong>: by default, they should respectively be set to ‘<em class="markup--li-em">train</em>’ and ‘<em class="markup--li-em">serve</em>’, be executable and have no extension. SageMaker will start training by running ‘<em class="markup--li-em">docker run your_container train</em>’.</li><li id="2487"><strong class="markup--li-strong">Location of hyper parameters </strong>in the container: <em class="markup--li-em">/opt/ml/input/config/hyperparameters.json</em>.</li><li id="6a68"><strong class="markup--li-strong">Location of input data parameters </strong>in the container: <em class="markup--li-em">/opt/ml/input/data</em>.</li></ul><p id="46f5">This will require some changes in our Keras script, the well-known example of <a href="https://github.com/awslabs/keras-apache-mxnet/blob/master/examples/mnist_cnn.py" target="_blank">learning MNIST with a simple CNN</a>. As you will see in a moment, they are quite minor and you won’t have any trouble adding them to your own code.</p><h4 id="43b6">Building a CPU-based Docker container</h4><p id="aa4d">Here’s the <strong class="markup--p-strong">Docker file</strong>.</p><figure id="6c3b"><script src="https://gist.github.com/juliensimon/74688c43c5a8b8840c34299c398f801d.js"></script></figure><p id="a702">We start from an <strong class="markup--p-strong">Ubuntu 16.04 image</strong> and install :</p><ul class="postList"><li id="718c"><strong class="markup--li-strong">Python 3</strong> as well as <strong class="markup--li-strong">native dependencies</strong> for MXNet.</li><li id="d2bf">the latest and greatest packages of <strong class="markup--li-strong">MXNet</strong> and <strong class="markup--li-strong">Keras-MXNet</strong>.</li></ul><blockquote class="graf--blockquote" id="726a">You don’t have to install pre-releases packages. I just like to live dangerously and add extra spice to my oh-so-quiet everyday life :*)</blockquote><p class="graf-after--blockquote" id="7aa3">Once this is done, we clean up various caches to shrink the container size a bit. Then, we copy :</p><ul class="postList"><li id="1e49">the <strong class="markup--li-strong">Keras script</strong> to <em class="markup--li-em">/opt/program</em> with the proper name (‘<em class="markup--li-em">train</em>’) and we make it executable.</li></ul><blockquote class="graf--blockquote" id="8ff0">For more flexibility, we could write a generic launcher that would fetch the actual training script from an S3 location passed as an hyper parameter. This is left as an exercise for the reader ;)</blockquote><ul class="postList"><li class="graf-after--blockquote" id="e35b">the <strong class="markup--li-strong">Keras configuration file</strong> to <em class="markup--li-em">/root/.keras/keras.json</em>.</li></ul><p id="5a63">Finally, we set the directory of our script as the <strong class="markup--p-strong">work directory</strong> and add it to the <strong class="markup--p-strong">path</strong>.</p><p id="7214">It’s not a long file, but as usual with these things, every detail counts.</p><h4 id="229c">Building a GPU-based Docker container</h4><p id="57c1">Now let’s build its GPU counterpart. It differs in only two ways:</p><ul class="postList"><li id="ca7b">we start from the <strong class="markup--li-strong">CUDA 9.0 image</strong>, which is also based on Ubuntu 16.04. This one has all the CUDA libraries that MXNet needs (unlike the smaller 9.0-base, don’t bother trying it).</li><li id="2d3e">we install the <strong class="markup--li-strong">CUDA 9.0-enabled MXNet</strong> package.</li></ul><p id="9bdf">Everything else is the same as before.</p><figure id="a086"><script src="https://gist.github.com/juliensimon/705e282bd5497b8a9f9351517871c56b.js"></script></figure><h4 id="8f5c">Creating a Docker repository in Amazon ECR</h4><p id="753a">SageMaker requires that the containers it fetches are hosted in <a href="http://aws.amazon.com/ecr" target="_blank"><strong class="markup--p-strong">Amazon ECR</strong></a>. Let’s create a repo and login to it.</p><figure id="c801"><script src="https://gist.github.com/juliensimon/08388e7358a618e62f92de1d409f6808.js"></script></figure><h4 id="2ca1">Building and pushing our containers to ECR</h4><p id="c21d">OK, now it’s time to build both containers and push them to their repos. We’ll do this separately for the CPU and GPU versions. Strictly Docker stuff. Please refer to the notebook for details on variables.</p><figure id="da51"><script src="https://gist.github.com/juliensimon/edb9fedb661479a9db77c21fbddd9ed6.js"></script></figure><p id="8b48">Once we’re done, things should look like this and you should also see your two containers in ECR.</p><figure id="d33c"><img class="graf-image" src="image02.webp"/></figure><p id="19eb">The Docker part is over. Now let’s configure our training job in SageMaker.</p><h3 id="4ccd">Configuring the training job</h3><p id="2181">This is actually quite underwhelming, which is great news: <strong class="markup--p-strong">nothing really differs from training with a built-in algorithm!</strong></p><p id="2653">First we need to <strong class="markup--p-strong">upload the MNIST data set</strong> from our local machine to S3. We’ve done this many times before, nothing new here.</p><figure id="0339"><script src="https://gist.github.com/juliensimon/c105a2d19ef6357253e1223bf2e50dae.js"></script></figure><p id="ddf7">Then, we configure the training job by:</p><ul class="postList"><li id="d140">selecting one of the containers we just built and setting the usual parameters for SageMaker <a href="https://sagemaker.readthedocs.io/en/latest/estimators.html" target="_blank"><strong class="markup--li-strong">estimators,</strong></a></li><li id="b235">passing <strong class="markup--li-strong">hyper parameters</strong> to the Keras script,</li><li id="5778">passing <strong class="markup--li-strong">input data</strong> to the Keras script.</li></ul><figure id="6f60"><script src="https://gist.github.com/juliensimon/907c86df678b5c85255fa5f6137b2549.js"></script></figure><p id="cea8">That’s it for training. The last part we’re missing is adapting our Keras script for SageMaker. Let’s get to it.</p><h3 id="1286">Adapting the Keras script for SageMaker</h3><p id="62c6">We need to take care of hyper parameters, input data, multi-GPU configuration, loading the data set and saving models.</p><h4 id="6d28">Passing hyper parameters and input data configuration</h4><p id="306a">As mentioned earlier, SageMaker copies hyper parameters to <strong class="markup--p-strong"><em class="markup--p-em">/opt/ml/input/config/hyperparameters.json</em></strong>. All we have to do is read this file, extract parameters and set default values if needed.</p><figure id="d9dd"><script src="https://gist.github.com/juliensimon/72c41b0beba8b6ad809523869d097ca2.js"></script></figure><p id="934c">In a similar fashion, SageMaker copies the input data configuration to <em class="markup--p-em">/opt/ml/input/data. </em>We’ll handle things in exactly the same way.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="1d93">In this example, I don’t need this configuration info, but this is how you’d read it if you did :)</blockquote><h4 class="graf-after--blockquote" id="1dbd">Loading the training and validation set</h4><p id="9f98">When training in file mode (which is the case here), SageMaker <strong class="markup--p-strong">automatically</strong> copies the data set to <em class="markup--p-em">/opt/ml/input/&lt;channel_name&gt;</em>: here, we defined the <em class="markup--p-em">train</em> and <em class="markup--p-em">validation</em> channels, so we’ll simply read the MNIST files from the corresponding directories.</p><figure id="8f2e"><script src="https://gist.github.com/juliensimon/216609616f5fb1af0e2a18203ca2f243.js"></script></figure><h4 id="1f1e">Configuring multi-GPU training</h4><p id="a427">As explained in a previous post, Keras-MXNet makes it very easy to set up multi-GPU training. Depending on the <em class="markup--p-em">gpu_count</em> hyper parameter, we just need to wrap our model with a bespoke Keras API before compiling it:</p><figure id="9d57"><script src="https://gist.github.com/juliensimon/492e137a28ada394711b2a2218da478f.js"></script></figure><p id="682b">Ain’t life grand?</p><h4 id="9b98">Saving models</h4><p id="031b">The very last thing we need to do once training is complete is to save the model in <strong class="markup--p-strong"><em class="markup--p-em">/opt/ml/model: </em></strong>SageMaker will grab all artefacts present in this directory, build a file called <em class="markup--p-em">model.tar.gz</em> and copy it to the S3 bucket used by the training job.</p><p id="3999">In fact, we’re going to save the trained model in two different formats : the <strong class="markup--p-strong">Keras format</strong> (i.e. an HDF5 file) and the native <strong class="markup--p-strong">MXNet format</strong> (i.e. a JSON file and a .<em class="markup--p-em">params</em> file). This will allow us to use it with both libraries!</p><figure id="fcf9"><script src="https://gist.github.com/juliensimon/e42ddb2c5a251ff22069d788c0974961.js"></script></figure><p id="1bda">That’s it. As you can see, it’s all about interfacing your script with SageMaker input and output. The bulk of your Keras code doesn’t require any modification.</p><h3 id="0f95">Running the script</h3><p id="7de9">Alright, let’s run the GPU version! We’ll train on <strong class="markup--p-strong">2 GPUs</strong> hosted in a <strong class="markup--p-strong">p3.8xlarge</strong> instance.</p><figure id="f403"><script src="https://gist.github.com/juliensimon/16a252b6284bcfde3a29025ccb8c4c8e.js"></script></figure><p id="57be">Let’s check the <strong class="markup--p-strong">S3 bucket</strong>.</p><pre class="graf--pre" id="1639">$ <strong class="markup--pre-strong">aws</strong> s3 ls $BUCKET/keras-mxnet-gpu/output/keras-mxnet-mnist-cnn-2018-05-30-17-39-50-724/output/<br/>2018-05-30 17:43:34    8916913 model.tar.gz<br/>$ <strong class="markup--pre-strong">aws</strong> s3 cp $BUCKET/keras-mxnet-gpu/output/keras-mxnet-mnist-cnn-2018-05-30-17-39-50-724/output/model.tar.gz .<br/>$ <strong class="markup--pre-strong">tar</strong> tvfz model.tar.gz<br/>-rw-r--r-- 0/0   4822688 2018-05-30 17:43 <strong class="markup--pre-strong">mnist-cnn-10.hd5</strong><br/>-rw-r--r-- 0/0   4800092 2018-05-30 17:43 <strong class="markup--pre-strong">mnist-cnn-10-0000.params</strong><br/>-rw-r--r-- 0/0      4817 2018-05-30 17:43 <strong class="markup--pre-strong">mnist-cnn-10-symbol.json</strong></pre><p class="graf-after--pre" id="3d40">Wunderbar, as they say on the other side of the Rhine ;) We can now use these models <strong class="markup--p-strong">anywhere</strong> we like.</p><p id="ca57">That’s it for today. Another (hopefully) nice example of using SageMaker to <strong class="markup--p-strong">train your custom jobs on fully-managed infrastructure</strong>!</p><p id="6890">Happy to answer questions here or on <a href="https://twitter.com/julsimon" rel="noopener nofollow noopener noopener nofollow noopener noopener noopener noopener noopener" target="_blank">Twitter</a>. For more content, please feel free to check out my <a href="https://www.youtube.com/juliensimonfr" rel="nofollow noopener noopener noopener nofollow noopener noopener noopener noopener noopener" target="_blank">YouTube channel</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="3e64"><em class="markup--p-em">Time to burn… some clock cycles :)</em></p><figure id="5acd"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/f6pJfDZEAf8?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
