<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Gluon: building blocks for your Deep Learning universe</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="7a03">Gluon: building blocks for your Deep Learning universe</h3><p id="9714"><a href="https://aws.amazon.com/blogs/aws/introducing-gluon-a-new-library-for-machine-learning-from-aws-and-microsoft/" target="_blank">Launched in October 2017</a>, Gluon is a new Open Source <a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#gluon-api" target="_blank">high-level API</a> for Deep Learning developers. Right now, it’s available on top of <a href="http://mxnet.incubator.apache.org" target="_blank">Apache MXNet</a>.</p><p id="74e3">Yet another API? Well, not quite. Here are <strong class="markup--p-strong">ten reasons</strong> why you should take a good look at Gluon.</p><figure id="034e"><img class="graf-image" src="image01.webp"/><figcaption>Source: <a class="markup--figure-anchor" href="https://www.quantamagazine.org" target="_blank">Quanta Magazine</a></figcaption></figure><h4 id="a3a2">1 — Extraordinary documentation</h4><p id="0cf5">I’m not exaggerating. Calling it documentation doesn’t do it justice: Gluon actually comes with a <a href="http://gluon.mxnet.io/index.html" target="_blank"><strong class="markup--p-strong">full-fledged book</strong></a><strong class="markup--p-strong"> on Deep Learning</strong>!</p><p id="aa03">Concepts, how to implement them from scratch, how to implement them with Gluon, pretty much all network architectures from perceptrons to Generative Adversial Networks… and a ton of <a href="https://github.com/zackchase/mxnet-the-straight-dope" target="_blank"><strong class="markup--p-strong">notebooks</strong></a><strong class="markup--p-strong">.</strong></p><p id="472d">VERY impressive work by my colleague <a href="https://github.com/zackchase" target="_blank">Zach Lipton</a>. If you’d like to help him out, I’m sure he’d be happy to review your pull requests ;)</p><h4 id="a0f5">2— Plenty of pre-defined layers and loss functions</h4><p id="1e53">Gluon includes an extensive collection of <a href="https://mxnet.incubator.apache.org/api/python/gluon/nn.html" target="_blank"><strong class="markup--p-strong">pre-defined layers</strong></a>: from basic ones (Dense, Activation, Dropout, Embedding, etc.) to Convolution (2D, 3D, transposed) to Pooling (average, max and global max in 1D, 2D and 3D).</p><p id="03a0">You’ll also find <a href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html" target="_blank"><strong class="markup--p-strong">layers for recurrent networks</strong></a> (RNN, LSTM, GRU), as well as <strong class="markup--p-strong">individuals cells</strong>. The latter allow you full control over your networks should you need to build them cell by cell.</p><p id="2743">In addition, you’ll find a collection of <a href="https://mxnet.incubator.apache.org/api/python/gluon/contrib.html" target="_blank"><strong class="markup--p-strong">experimental features</strong></a> contributed by the Gluon community, such as convolutional recurrent cells.</p><p id="ba53">Last but not least, Gluon also includes a nice collection of <a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html" target="_blank"><strong class="markup--p-strong">loss functions</strong></a>, from basic ones to more advanced ones like the Triplet Loss function used to build face recognition models.</p><h4 id="d030"><strong class="markup--h4-strong">3 — Simple definition of models</strong></h4><p id="85c7">For reference, this is how we’d define a simple network with the <a href="https://mxnet.incubator.apache.org/api/python/symbol/symbol.html" target="_blank">symbolic API</a> in Apache MXNet.</p><pre class="graf--pre" id="2158">import mxnet as mx<br/>from mxnet import sym,mod</pre><pre class="graf--pre graf-after--pre" id="c5bb">data = sym.Variable('data')<br/>fc1 = sym.FullyConnected(data, name='fc1', num_hidden=128)<br/>relu1 = sym.Activation(fc1, name='relu1', act_type="relu")<br/>fc2 = sym.FullyConnected(relu1, name='fc2', num_hidden=64)<br/>relu2 = sym.Activation(fc2, name='relu1', act_type="relu")<br/>out = sym.FullyConnected(relu2, name='out', num_hidden=10)<br/>mod = mod.Module(out)</pre><p class="graf-after--pre" id="4608">Here’s the same network defined with Gluon. All we have to do is to add layers sequentially.</p><pre class="graf--pre" id="0ad5">import mxnet as mx<br/>from mxnet.gluon import nn</pre><pre class="graf--pre graf-after--pre" id="2a69">net = nn.Sequential()<br/>with net.name_scope():<br/>   net.add(nn.Dense(128, activation="relu"))<br/>   net.add(nn.Dense(64, activation="relu"))<br/>   net.add(nn.Dense(10))</pre><p class="graf-after--pre" id="6a21">A bit clearer, isn’t it? :)</p><h4 id="1792">4— Automatic shape of input layer</h4><p id="143e">As you can see above, we don’t have to define the <strong class="markup--p-strong">input shape</strong> when building a network. With Gluon, all we have to do is initialize parameters and forward data to the network.</p><p id="5f4b">For instance, this is how we’d apply the network above to a 256-float vector.</p><pre class="graf--pre" id="684d">net.collect_params().initialize()</pre><pre class="graf--pre graf-after--pre" id="402d"><em class="markup--pre-em"># Define a random 256-float vector and forward it to the network<br/></em>data = mx.nd.random_uniform(low=0, high=1, shape=(1,256))<br/>net(data)</pre><pre class="graf--pre graf-after--pre" id="abff"><strong class="markup--pre-strong">[[ 1.3353475e-03 -1.1403845e-02  8.6122309e-05  1.3773030e-02<br/>   9.9888537e-03  6.7939619e-03 -1.8021716e-02 -6.2033422e-03<br/>  -1.3288442e-02  1.0132480e-02]]</strong></pre><p class="graf-after--pre" id="85a1">This is an advantage over Keras where we’d have to build the input shape into the model definition.</p><pre class="graf--pre" id="97e2">from keras.model import Sequential<br/>from keras.layers import Dense</pre><pre class="graf--pre graf-after--pre" id="162a">model = Sequential() <br/>model.add(Dense(128, activation='relu', input_shape=(<strong class="markup--pre-strong">256</strong>,)))<br/>model.add(Dense(64, activation='relu'))<br/>model.add(Dense(10))</pre><h4 class="graf-after--pre" id="6f39">5—Intuitive access to network layers and parameters</h4><p id="8409">Gluon makes it intuitive to explore network layers, as well as their <a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html#mxnet.gluon.Parameter" target="_blank">parameters</a>.</p><p id="72ce">Here’s how we can iterate through layers.</p><pre class="graf--pre" id="a860">for layer in net:<br/>  print(layer)<br/>  print(layer.params)</pre><pre class="graf--pre graf-after--pre" id="44f7"><strong class="markup--pre-strong">Dense(64 -&gt; 128, Activation(relu))<br/>sequential2_dense0_ (<br/>  Parameter sequential2_dense0_weight (shape=(128L, 64L), dtype=&lt;type 'numpy.float32'&gt;)<br/>  Parameter sequential2_dense0_bias (shape=(128L,), dtype=&lt;type 'numpy.float32'&gt;)<br/>)</strong></pre><pre class="graf--pre graf-after--pre" id="81c4"><strong class="markup--pre-strong">Dense(128 -&gt; 64, Activation(relu))<br/>sequential2_dense1_ (<br/>  Parameter sequential2_dense1_weight (shape=(64L, 128L), dtype=&lt;type 'numpy.float32'&gt;)<br/>  Parameter sequential2_dense1_bias (shape=(64L,), dtype=&lt;type 'numpy.float32'&gt;)<br/>)</strong></pre><pre class="graf--pre graf-after--pre" id="dbde"><strong class="markup--pre-strong">Dense(64 -&gt; 10, linear)<br/>sequential2_dense2_ (<br/>  Parameter sequential2_dense2_weight (shape=(10L, 64L), dtype=&lt;type 'numpy.float32'&gt;)<br/>  Parameter sequential2_dense2_bias (shape=(10L,), dtype=&lt;type 'numpy.float32'&gt;)<br/>)</strong></pre><p class="graf-after--pre" id="78d2">Reading and writing parameters is equally straightforward.</p><pre class="graf--pre" id="f2d2">params=net[0].weight.data()<br/>print("%s %s" % (type(params), params.shape))</pre><pre class="graf--pre graf-after--pre" id="d6d6"><strong class="markup--pre-strong">&lt;class 'mxnet.ndarray.ndarray.NDArray'&gt; (128L, 64L)</strong></pre><pre class="graf--pre graf-after--pre" id="a9ce">params[0][0]=0.123<br/>print(params)</pre><pre class="graf--pre graf-after--pre" id="af84"><strong class="markup--pre-strong">[[ 0.123 -0.0177393  -0.00650402 ... -0.04026533 -0.04062188<br/>  -0.03885795]<br/> [ 0.05647313  0.0380233   0.01031513 ...  0.0654735   0.04788432<br/>  -0.03103536]<br/> [ 0.02013787  0.01294949  0.02260739 ... -0.0699827   0.01811036<br/>  -0.05699452]<br/> ...<br/> [-0.04240721  0.01670218  0.0533151  ...  0.000951    0.05940091<br/>   0.00070946]<br/> [-0.00068477  0.00757013 -0.04234412 ... -0.04753195  0.01538438<br/>  -0.04391037]<br/> [-0.01510854 -0.03736208  0.01939485 ... -0.04374463 -0.03795088<br/>  -0.01618673]]</strong></pre><h4 class="graf-after--pre" id="9b91">6 — Flexible data loading and transformation</h4><p id="ad54">The <a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html" target="_blank">Data API</a> provides convenient methods to load datasets stored in <a href="https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html" target="_blank"><strong class="markup--p-strong">NDArrays</strong></a> (which is how MXNet stores tensors), <strong class="markup--p-strong">numpy arrays</strong>, <a href="https://mxnet.incubator.apache.org/api/python/io/io.html#module-mxnet.recordio" target="_blank"><strong class="markup--p-strong">RecordIO</strong></a><strong class="markup--p-strong"> files</strong> and <strong class="markup--p-strong">image folders</strong>.</p><pre class="graf--pre" id="9ae1">train_data = gluon.data.DataLoader(<br/>    gluon.data.ArrayDataset(X, y),<br/>    batch_size=batch_size, <br/>    shuffle=True)</pre><p class="graf-after--pre" id="be13">We can also download <strong class="markup--p-strong">popular datasets</strong> like MNIST, Fashion MNIST, CIFAR-10 and CIFAR-100.</p><pre class="graf--pre" id="80b4">train_data = mx.gluon.data.DataLoader(<br/>    mx.gluon.data.vision.MNIST(train=True))</pre><p class="graf-after--pre" id="deb7">Transformations can be applied at loading time by providing a <strong class="markup--p-strong">transform function</strong>. For example, here’s how we would normalize pixel values for the MNIST dataset.</p><pre class="graf--pre" id="b81d">def transform(data, label):   <br/>    return data.astype(np.float32)/255, label.astype(np.float32)</pre><pre class="graf--pre graf-after--pre" id="151b">train_data = mx.gluon.data.DataLoader(<br/>    mx.gluon.data.vision.MNIST(<br/>       train=True, <br/>       transform=transform))</pre><h4 class="graf-after--pre" id="6df5">7 — Rich model zoo</h4><p id="66f3">The Gluon model zoo is more complete than its counterparts in <a href="https://mxnet.apache.org/model_zoo/index.html" target="_blank">Apache MXNet</a>, <a href="https://keras.io/applications/" target="_blank">Keras</a> and <a href="http://pytorch.org/docs/master/torchvision/models.html" target="_blank">PyTorch</a>.</p><p id="147c">At the time of writing, you can grab pre-trained versions of <a href="https://arxiv.org/abs/1404.5997" target="_blank"><strong class="markup--p-strong">AlexNet</strong></a><strong class="markup--p-strong">, </strong><a href="https://arxiv.org/abs/1608.06993" target="_blank"><strong class="markup--p-strong">DenseNet</strong></a><strong class="markup--p-strong">, </strong><a href="http://arxiv.org/abs/1512.00567" target="_blank"><strong class="markup--p-strong">Inception V3</strong></a><strong class="markup--p-strong">, </strong><a href="https://arxiv.org/abs/1512.03385" target="_blank"><strong class="markup--p-strong">ResNet V1, ResNet V2</strong></a><strong class="markup--p-strong">, </strong><a href="https://arxiv.org/abs/1602.07360" target="_blank"><strong class="markup--p-strong">SqueezeNet</strong></a><strong class="markup--p-strong">, </strong><a href="https://arxiv.org/abs/1409.1556" target="_blank"><strong class="markup--p-strong">VGG</strong></a><strong class="markup--p-strong"> and </strong><a href="https://arxiv.org/abs/1704.04861" target="_blank"><strong class="markup--p-strong">MobileNet</strong></a>, in multiple <strong class="markup--p-strong">depths</strong> and <strong class="markup--p-strong">configurations</strong>.</p><p id="d591">All of these will come in handy for transfer learning and fine-tuning. Downloading a model couldn’t be simpler.</p><pre class="graf--pre" id="17de">from mxnet.gluon.model_zoo import vision<br/>net = vision.squeezenet1_1(pretrained=True)</pre><h4 class="graf-after--pre" id="14ac"><strong class="markup--h4-strong">8 — Imperative-style execution</strong></h4><p id="d657">In traditional Deep Learning frameworks like Tensorflow and Apache MXNet, network definition and training run in symbolic mode (aka define-then-run).</p><blockquote class="graf--blockquote" id="9123">In October 2017, Tensorflow introduced an experimental imperative mode, aka <a class="markup--blockquote-anchor" href="https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html" target="_blank">eager mode</a>.</blockquote><p class="graf-after--blockquote" id="3086">Here’s a typical example using Apache MXNet.</p><pre class="graf--pre" id="03e6"><em class="markup--pre-em"># Define network with symbolic API<br/></em>...<br/>mod = mod.Module(out)<br/><em class="markup--pre-em"># Train network<br/></em>mod.bind(<br/>  data_shapes=iter.provide_data,<br/>  label_shapes=iter.provide_label)<br/>mod.fit(train_iter, num_epoch=50)</pre><p class="graf-after--pre" id="ee68">There are good reasons for doing this! Since a symbolic network is <strong class="markup--p-strong">pre-defined</strong>, its execution graph can be <strong class="markup--p-strong">optimized for speed and memory</strong> prior to training, then run with highly-efficient C++ primitives: all of this makes it <strong class="markup--p-strong">more efficient</strong> than its imperative counterpart written in Python. However, it comes at the expense of <strong class="markup--p-strong">flexibility</strong> (networks cannot be modified) and <strong class="markup--p-strong">visibility</strong> (networks are hard / impossible to inspect).</p><p id="ddec">In contrast, Gluon relies exclusively on <strong class="markup--p-strong">imperative</strong> (aka define-by-run) programming: network definition and training loop are based on Python code, allowing us to use all <strong class="markup--p-strong">language features</strong> (loops, conditional execution, classes, etc.) for maximal <strong class="markup--p-strong">flexibility</strong>.</p><p id="137b">To illustrate this, here’s a typical training loop.</p><pre class="graf--pre" id="d83d">for e in range(epochs):<br/>    cumulative_loss = 0<br/>    for i, (data, label) in enumerate(train_data):<br/>        data = data.as_in_context(model_ctx)<br/>        label = label.as_in_context(model_ctx)<br/>        with autograd.record():<br/>            output = net(data)<br/>            loss = softmax_cross_entropy(output, label)<br/>        loss.backward()<br/>        trainer.step(data.shape[0])<br/>        cumulative_loss += nd.sum(loss).asscalar()</pre><p class="graf-after--pre" id="1f9a">Thanks to imperative programming, it’s possible to <strong class="markup--p-strong">debug every step of the training process</strong>: inspecting parameters, saving them to disk, tweaking them if certain conditions happen, etc. Even inside of Jupyter notebooks, we can use the <strong class="markup--p-strong">Python debugger</strong> by inserting a single line of code. This is invaluable when trying to understand why training goes wrong.</p><pre class="graf--pre" id="254a"><code class="markup--code markup--pre-code">import pdb; pdb.set_trace()</code></pre><blockquote class="graf--blockquote graf-after--pre" id="c460">I have one minor gripe about the lack of a high-level API to train a model, similar to <em class="markup--blockquote-em">model.fit()</em> in MXNet or Keras. Sure, it’s easy to write, but hopefully the Gluon team will add it. Lazyness is a virtue ;)</blockquote><h4 class="graf-after--blockquote" id="d107">9 — Combining custom objects and built-in objects</h4><p id="b8e0">Gluon makes it very easy to <strong class="markup--p-strong">define your own objects</strong>. Here’s a class for a multi-layer perceptron. Once again, the imperative programming style allows us to define the <strong class="markup--p-strong"><em class="markup--p-em">forward()</em></strong> operation exactly the way we want it: we could apply conditional processing based on network parameters, number of epochs, etc.</p><pre class="graf--pre" id="c934">class MLP(Block):<br/>    def __init__(self, **kwargs):<br/>        super(MLP, self).__init__(**kwargs)<br/>        with self.name_scope():<br/>            self.dense0 = nn.Dense(128)<br/>            self.dense1 = nn.Dense(64)<br/>            self.dense2 = nn.Dense(10)<br/>        <br/>    def forward(self, x):<br/>        x = nd.relu(self.dense0(x))<br/>        x = nd.relu(self.dense1(x))<br/>        return self.dense2(x)</pre><p class="graf-after--pre" id="01a9">We can also define <strong class="markup--p-strong">custom layers</strong>, as highlighted by <a href="http://gluon.mxnet.io/chapter03_deep-neural-networks/custom-layer.html" target="_blank">this example</a> taken from the Gluon documentation. As you can see, they can be <strong class="markup--p-strong">seamlessly</strong> integrated with the rest of the Gluon API, so we still rely on existing objects to make our life easier.</p><pre class="graf--pre" id="edbe">class CenteredLayer(Block):<br/>    def __init__(self, **kwargs):<br/>        super(CenteredLayer, self).__init__(**kwargs)<br/>    def forward(self, x):<br/>        return x - nd.mean(x)</pre><pre class="graf--pre graf-after--pre" id="9a7a">net = nn.Sequential()<br/>with net.name_scope():<br/>    net = nn.Sequential()<br/>    net.add(nn.Dense(128))<br/>    net.add(nn.Dense(10))<br/>    net.add(CenteredLayer())</pre><h4 class="graf-after--pre" id="6197">10 — Flexibility and speed: pick two</h4><p id="505f">We discussed earlier the benefits of imperative programming while noting that performance would be inferior to symbolic programming.</p><p id="3ca9">Let’s run a quick test by <strong class="markup--p-strong">predicting 1,000 MNIST images</strong> with this simple multi-layer perceptron (for the sake of brevity, I’ll just show the network definition)</p><pre class="graf--pre" id="aaf3">net = nn.Sequential()<br/>with net.name_scope():<br/>  net.add(nn.Dense(256, activation="relu"))<br/>  net.add(nn.Dense(128, activation="relu"))<br/>  net.add(nn.Dense(2))<br/>  # initialize the parameters<br/>  net.collect_params().initialize()<br/>  return net</pre><p class="graf-after--pre" id="592c">Total prediction time is <strong class="markup--p-strong">0.37 second</strong>.</p><p id="91c4">Now, let’s change replace the <em class="markup--p-em">Sequential</em> object with its hybrid equivalent. This will allow Gluon to compile the network to <strong class="markup--p-strong">symbolic form</strong> and to use <strong class="markup--p-strong">optimized lower-level primitives</strong>.</p><pre class="graf--pre" id="3237">net = nn.HybridSequential()<br/>with net.name_scope():<br/>  net.add(nn.Dense(256, activation="relu"))<br/>  net.add(nn.Dense(128, activation="relu"))<br/>  net.add(nn.Dense(2))<br/>  # initialize the parameters<br/>  net.collect_params().initialize()<br/>  return net</pre><pre class="graf--pre graf-after--pre" id="20cb">net.hybridize()</pre><p class="graf-after--pre" id="0968">This time, total prediction time is <strong class="markup--p-strong">0.21 second</strong>, almost <strong class="markup--p-strong">2x faster</strong>. Is there a catch? Well, yes: you lose the flexibility to write a custom <em class="markup--p-em">forward()</em> function as well as the ability to debug it. Still, once you’ve successfully built and trained a network, <em class="markup--p-em">hybridizing it is a easy way to improve inference performance</em>.</p><p id="c238">For reference, let’s run the same test with the <strong class="markup--p-strong">symbolic API</strong> of Apache MXNet.</p><pre class="graf--pre" id="1009">data = mx.sym.Variable('data')<br/>data = mx.sym.Flatten(data=data)<br/>fc1  = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=64)<br/>act1 = mx.sym.Activation(data=fc1, name='relu1', act_type="relu")<br/>fc2  = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden = 64)<br/>act2 = mx.sym.Activation(data=fc2, name='relu2', act_type="relu")<br/>fc3  = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden=10)<br/>mlp  = mx.sym.SoftmaxOutput(data=fc3, name='softmax')</pre><p class="graf-after--pre" id="cc8b">Prediction time is<strong class="markup--p-strong"> 0.16 second</strong>, more than <strong class="markup--p-strong">30% faster</strong> than the hybridized version. When <strong class="markup--p-strong">top speed</strong> is required — for inference and even more so for training —<strong class="markup--p-strong"> </strong>the highly-optimized primitives of MXNet remains the best option.</p><h4 id="5714">Conclusion</h4><p id="b4bc">Gluon has a lot going for it. I think it improves on symbolic MXNet and even on Keras in several respects . The <strong class="markup--p-strong">documentation</strong> and the <strong class="markup--p-strong">model zoo</strong> alone are worth the price of admission, especially if you’re beginning with Deep Learning. Go try it out and tell me what *you* think :)</p><p id="9f33">As always, thanks for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="9dcb"><em class="markup--p-em">Subatomic particles, gamma rays, black holes, lightspeed. Proper Metal material \m/</em></p><figure id="b801"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/vY1CZRwV-aw?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
