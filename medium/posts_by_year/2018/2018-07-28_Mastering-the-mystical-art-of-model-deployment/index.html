<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Mastering the mystical art of model deployment</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="ab79">Mastering the mystical art of model deployment</h3><p id="0dc8">With all the talk about algorithm selection, hyper parameter optimization and so on, you could think that training models is the hardest part of the Machine Learning process. However, in my experience, the really tricky step is to deploy these models safely in a web production environment.</p><p id="d74d">In this post, I‚Äôll first talk about the typical tasks required to <strong class="markup--p-strong">deploy and validate</strong> models in production. Then, I‚Äôll present several <strong class="markup--p-strong">model deployment techniques </strong>and how to implement them with <a href="http://aws.amazon.com/sagemaker" target="_blank">Amazon SageMaker</a>. In particular, I‚Äôll show you in detail how to <strong class="markup--p-strong">host multiple models on the same prediction endpoint</strong>, an important technique to minimize deployment risks.</p><figure id="fce0"><img class="graf-image" src="image02.webp"/><figcaption>That guy played Alan Turing and Dr Strange. ‚Äònuff¬†said.</figcaption></figure><h3 id="ac5e">Validating a¬†model</h3><p id="89ae">Even if you‚Äôve carefully trained and evaluated a model in your Data Science sandbox, additional work is required to check that it will work correctly in your production environment. This usually involve tasks like:</p><ul class="postList"><li id="37b9">setting up a <strong class="markup--li-strong">monitoring system</strong> to store and visualize model metrics,</li><li id="002a">building a <strong class="markup--li-strong">test web application</strong> bundling the model and running <strong class="markup--li-strong">technical tests</strong> (is the model fast? how much RAM does it require? etc.) as well as <strong class="markup--li-strong">prediction tests</strong> (is my model still predicting as expected?).</li><li id="4bd2">integrating the model with your <strong class="markup--li-strong">business application</strong> and running end to end tests,</li><li id="d002">deploying the application in production using techniques like <strong class="markup--li-strong">blue-green deployment</strong> or <strong class="markup--li-strong">canary testing</strong> (more on this in a minute),</li><li id="6f7e">running <strong class="markup--li-strong">different versions of the same model in parallel</strong> for longer periods of time, in order to measure their long-term effectiveness with respect to business metrics (aka <strong class="markup--li-strong">A/B testing</strong>).</li></ul><p id="a229">Quite a bit of work, then. Let‚Äôs first look at the different ways we could deploy models.</p><h3 id="84a1">Deployment options</h3><h4 id="af33">Standard deployment</h4><p id="b421">In its simplest form, deploying a model usually involves building a bespoke <strong class="markup--p-strong">web application hosting your model</strong> and receiving prediction requests. Testing is what you would expect: sending HTTP requests, checking logs and checking metrics.</p><p id="032f">SageMaker <strong class="markup--p-strong">greatly</strong> simplifies this process. With just a few lines of code, the <a href="http://sagemaker.readthedocs.io/en/latest/estimators.html" target="_blank">Estimator</a> object in the <a href="https://github.com/aws/sagemaker-python-sdk" target="_blank">SageMaker SDK</a> (or its subclasses for built-in algos, TensorFlow, etc.) lets you deploy a model to an HTTPS <strong class="markup--p-strong">endpoint</strong> and run prediction tests. No need to write any app. In addition, technical metrics are available out of the box in <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html" target="_blank">CloudWatch</a>.</p><blockquote class="graf--blockquote" id="c50e">If you‚Äôre interested in load testing and sizing endpoints, this nice <a class="markup--blockquote-anchor" href="https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/" target="_blank">AWS blog post</a> will show you how to do it.</blockquote><p class="graf-after--blockquote" id="a4b3">I won‚Äôt dwell on this: I‚Äôve covered it several times in previous posts and you‚Äôll also find plenty of examples in the SageMaker <a href="https://github.com/awslabs/amazon-sagemaker-examples$" target="_blank">notebook collection</a>.</p><h4 id="1303">Blue-green deployment</h4><p id="033a">This proven <a href="https://martinfowler.com/bliki/BlueGreenDeployment.html" target="_blank">deployment technique</a> requires <strong class="markup--p-strong">two identical environments</strong>:</p><ul class="postList"><li id="3c3e">the live production environment (‚Äúblue‚Äù) running <strong class="markup--li-strong">version n</strong>,</li><li id="d63c">an exact copy of this environment (‚Äúgreen‚Äù) running <strong class="markup--li-strong">version n+1</strong>.</li></ul><p id="a6f3">First, you <strong class="markup--p-strong">run tests</strong> on the green environment, <strong class="markup--p-strong">monitor</strong> technical and business metrics and check that everything is correct. If it is, you can then <strong class="markup--p-strong">switch traffic</strong> to the green environment‚Ä¶ and check again. If something goes wrong, you can immediately <strong class="markup--p-strong">switch back</strong> to the blue environment and investigate. If everything is fine, you can <strong class="markup--p-strong">delete</strong> the blue environment.</p><p id="ed57">To make this process completely transparent to client applications, a middleman‚Ää‚Äî‚Äälocated between the clients and the environments‚Ää‚Äî‚Ääis in charge of <strong class="markup--p-strong">implementing the switch</strong>: popular choices include load balancers, DNS, etc. This is what it looks like.</p><figure id="d9f8"><img class="graf-image" src="image03.webp"/><figcaption>Blue-green deployment</figcaption></figure><h4 id="2829">Blue-green deployment, the SageMaker way</h4><p id="48b9">The <a href="https://boto3.readthedocs.io/en/latest/reference/services/sagemaker.html" target="_blank">AWS SDK for SageMaker</a> provides the middleman that we need in the form of the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" target="_blank"><strong class="markup--p-strong">endpoint configuration</strong></a>. This resource lets us attach <strong class="markup--p-strong">several models</strong> to the same endpoint, with different weights and different instance configurations (aka <strong class="markup--p-strong">production variants</strong>). The setup may be updated at any time during the life of the endpoint.</p><p id="cdaf">In fact, one could picture an endpoint as a special type of load balancer, using weighted round robin to send prediction requests to instance pools hosting different models. Here‚Äôs the information required to set one up with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" target="_blank"><em class="markup--p-em">CreateEndpointConfig</em></a> API.</p><pre class="graf--pre" id="d27c"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0">"<a class="markup--pre-anchor" href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html#SageMaker-CreateEndpointConfig-request-ProductionVariants" target="_blank">ProductionVariants</a>": [ <br/>      { <br/>         "<a class="markup--pre-anchor" href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InitialInstanceCount" target="_blank">InitialInstanceCount</a>": <em class="markup--pre-em">number</em>,<br/>         "<a class="markup--pre-anchor" href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InitialVariantWeight" target="_blank">InitialVariantWeight</a>": <em class="markup--pre-em">number</em>,<br/>         "<a class="markup--pre-anchor" href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-InstanceType" target="_blank">InstanceType</a>": "<em class="markup--pre-em">string</em>",<br/>         "<a class="markup--pre-anchor" href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-ModelName" target="_blank">ModelName</a>": "<em class="markup--pre-em">string</em>",<br/>         "<a class="markup--pre-anchor" href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html#SageMaker-Type-ProductionVariant-VariantName" target="_blank">VariantName</a>": "<em class="markup--pre-em">string</em>"<br/>      }<br/>   ]</code></pre><p class="graf-after--pre" id="3e43">Implementing blue-green deployment now goes like this:</p><ul class="postList"><li id="e587">create a <strong class="markup--li-strong">new endpoint configuration</strong>, holding the <strong class="markup--li-strong">production variants</strong> for the existing live model and for the new model.</li><li id="5a7a">update the <strong class="markup--li-strong">existing live endpoint</strong> with the new endpoint configuration (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpoint.html" target="_blank"><em class="markup--li-em">UpdateEndpoint</em></a> API): SageMaker creates the required infrastructure for the new production variant and update weights <strong class="markup--li-strong">without any downtime.</strong></li><li id="004f"><strong class="markup--li-strong">switch traffic</strong> to the new model (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html" target="_blank"><em class="markup--li-em">UpdateEndpointWeightAndCapacities</em></a> API),</li><li id="b383">create a new endpoint configuration holding <strong class="markup--li-strong">only the new production variant</strong> and apply it to the endpoint: SageMaker terminates the infrastructure for the previous production variant.</li></ul><p id="9946">This is what it looks like.</p><figure id="c17e"><img class="graf-image" src="image08.webp"/><figcaption>Blue-green deployment with a single SageMaker endpoint</figcaption></figure><h4 id="4cdd"><strong class="markup--h4-strong">Canary testing</strong></h4><p id="20a4">Canary testing lets you validate a new release with <strong class="markup--p-strong">minimal risk</strong> by deploying it first for a <strong class="markup--p-strong">fraction of your users</strong>: everyone else keeps using the previous version. This user split can be done in many ways: random, geolocation, specific user lists, etc. Once you‚Äôre satisfied with the release, you can gradually roll it out to <strong class="markup--p-strong">all users</strong>.</p><p id="ae08">This requires ‚Äú<strong class="markup--p-strong">stickiness</strong>‚Äù: for the duration of the test, designated users must be routed to servers running the <strong class="markup--p-strong">new release</strong>. This could be achieved by setting a specific cookie for these users, allowing the web application to identify them and send their traffic to the proper servers.</p><p id="858f">You could implement this logic either in the application itself or in a dedicated web service. The latter would be in charge of receiving prediction requests and invoking the appropriate endpoint.</p><p id="e800">This feels like extra work, but chances are you‚Äôll need a web service anyway for data pre-processing (normalization, injecting extra data in the prediction request, etc.) and post-processing (filtering prediction results, logging, etc.). Lambda feels like a good way to do this: easy to deploy, easy to scale, built-in high-availability, etc.: here‚Äôs an <a href="https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033" target="_blank">example</a> implemented with AWS Chalice.</p><p id="2599">This what it would look like with two endpoints.</p><figure id="4c1a"><img class="graf-image" src="image01.webp"/><figcaption>Using a ‚Äúswitch‚Äù web service and two single-model endpoints for canary¬†testing.</figcaption></figure><p id="05d5">Once we‚Äôre happy that the new model works, we can gradually roll it out to all users, scaling endpoints up and down accordingly.</p><figure id="92c9"><img class="graf-image" src="image09.webp"/><figcaption>Gradually switching all users to the new¬†models.</figcaption></figure><h4 id="5eef">A/B testing</h4><p id="197f">A/B testing is about <strong class="markup--p-strong">comparing the performance of different versions of the same feature</strong> while monitoring a high-level <strong class="markup--p-strong">metric</strong> (e.g. click-through rate, conversion rate, etc.). In this context, this would mean predicting with different models for different users and analysing results.</p><p id="b0d1">Technically speaking, A/B testing is similar to canary testing with <strong class="markup--p-strong">larger user groups</strong> and a <strong class="markup--p-strong">longer time-scale</strong> (days or even weeks). Stickiness is essential and the technique mentioned above would certainly work: building user buckets, sticking them to different endpoints and logging results.</p><p id="c1d8">As you can see, the ability to deploy multiple models to the same endpoint is an important requirement for validation and testing. Let‚Äôs see how this works.</p><h3 id="9844">Deploying multiple models to the same¬†endpoint</h3><p id="9ada">Imagine we‚Äôd like to compare different models trained with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html" target="_blank">built-in algorithm for image classification</a> using different hyper-parameters.</p><p id="c5d5">These are the steps we need to take (<a href="https://gitlab.com/juliensimon/dlnotebooks/blob/master/sagemaker/05-Image-classification-two-models.ipynb" target="_blank">full notebook</a> available on Gitlab):</p><ol class="postList"><li id="bbd2">train model A.</li><li id="97f5">train model B.</li><li id="046c">create models A and B, i.e. registering them in SageMaker.</li><li id="87d1">create an endpoint configuration with the two production variants.</li><li id="cd50">create the endpoint.</li><li id="e128">send traffic and look at CloudWatch metrics.</li></ol><p id="d5b7">We‚Äôve trained this algo in a <a href="https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54" target="_blank">previous post</a>, so I won‚Äôt go into details: in a nutshell, we‚Äôre simply training two models with different learning rates.</p><h4 id="9e80">Creating the endpoint configuration</h4><p id="4f99">This is where we define our <strong class="markup--p-strong">two production variants</strong>: one for model A and one for model B. To begin with, we‚Äôll assign them <strong class="markup--p-strong">equal weights</strong>, in order to balance traffic 50/50. We‚Äôll also use <strong class="markup--p-strong">identical instance configurations</strong>.</p><figure id="f3ba"><script src="https://gist.github.com/juliensimon/9d571ed98862c038e91d5f4a0e1cbcbc.js"></script></figure><h4 id="8a32">Creating the¬†endpoint</h4><p id="6005">Pretty straightforward: all it takes is calling the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html" target="_blank"><em class="markup--p-em">CreateEndpoint</em></a> API, which builds all infrastructure required to support the production variants defined in the endpoint configuration.</p><figure id="c24e"><script src="https://gist.github.com/juliensimon/75b72a45ce5a219c34cbcb7e46ddbd79.js"></script></figure><p id="1fe4">After a few minutes, we can see the endpoint settings in the SageMaker console.</p><figure id="6735"><img class="graf-image" src="image07.webp"/></figure><h4 id="df5b">Monitoring traffic</h4><p id="f6fc">Let‚Äôs send some traffic and monitor the endpoint in <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html" target="_blank">CloudWatch</a>. After a few more minutes, we can see that traffic is nicely balanced between the two production variants.</p><figure id="b067"><img class="graf-image" src="image04.webp"/></figure><p id="4e13">Let‚Äôs update the weights in the AWS console: model A now gets <strong class="markup--p-strong">10%</strong> of traffic and model B gets <strong class="markup--p-strong">90%</strong>. As mentioned above, you could also do this programmaticall with the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html" target="_blank"><em class="markup--p-em">UpdateEndpointWeightAndCapacities</em></a> API.</p><figure id="659d"><img class="graf-image" src="image05.webp"/></figure><p id="3172">Almost immediately, we see most of the traffic now going model B.</p><figure id="9a9a"><img class="graf-image" src="image10.webp"/></figure><h3 id="09d6">Wrapping up</h3><p id="fdc7">As you can see, it‚Äôs pretty easy to manage multiple models on the same prediction endpoint. This lets use different techniques to safely test new models before deploying them with <strong class="markup--p-strong">minimal risk</strong> to client applications¬†:)</p><p id="4ae8">That‚Äôs it for today. Thank you for reading. As always, please feel free to ask your questions here or on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="e9f0"><a href="https://medium.com/@julsimon/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579" target="_blank"><strong class="markup--p-strong">Part 2</strong></a><strong class="markup--p-strong"> is available: deploying Amazon SageMaker endpoints with AWS CloudFormation</strong></p><figure id="ee2e"><img class="graf-image" src="image06.webp"/></figure><p id="32cf"><strong class="markup--p-strong">Join our community Slack and read our weekly Faun topics ‚¨á</strong></p><div class="graf--mixtapeEmbed" id="a9fa"><a class="markup--mixtapeEmbed-anchor" href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium" title="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium"><strong class="markup--mixtapeEmbed-strong">Join a Community of Aspiring Developers.Get must-read articles, learn new technologies for free‚Ä¶</strong><br/><em class="markup--mixtapeEmbed-em">Join thousands of developers and IT experts, get must-read articles, chat with like-minded people, get job offers and‚Ä¶</em>www.faun.dev</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="196e30881c9915d1a1d318225381bca9" data-thumbnail-img-id="0*AWWZdqNDfqshVEPJ" href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmedium" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*AWWZdqNDfqshVEPJ);"></a></div><h4 class="graf-after--mixtapeEmbed" id="cb07">If this post was helpful, please click the clap üëè button below a few times to show your support for the author!¬†‚¨á</h4></div></div></section>
</section>
</article></body></html>