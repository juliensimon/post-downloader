<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Apache Spark and Amazon SageMaker, the Infinity Gems of analytics</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="7070">Apache Spark and Amazon SageMaker, the Infinity Gems of analytics</h3><p id="300e">In a previous post, I showed you <a href="https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f" target="_blank">how to build a spam classifier</a> by running <a href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank"><strong class="markup--p-strong">PySpark</strong></a> on an <a href="https://aws.amazon.com/sagemaker" target="_blank"><strong class="markup--p-strong">Amazon SageMaker</strong></a><strong class="markup--p-strong"> </strong>notebook instance.</p><p id="62b7">This is a fine setup for experimentation but neither scalable nor automated enough for production. Now, let’s go all the way and use a proper <a href="https://aws.amazon.com/emr" target="_blank"><strong class="markup--p-strong">Amazon EMR</strong></a> cluster running Spark and using the <a href="https://github.com/aws/sagemaker-spark/" target="_blank"><strong class="markup--p-strong">SageMaker Spark SDK</strong></a> to fire up training jobs.</p><figure id="9e01"><img class="graf-image" src="image02.webp"/><figcaption>Data Science Thanos: “With the SageMaker and Spark gems, even the mightiest data sets will bow to my will!”.</figcaption></figure><blockquote class="graf--blockquote" id="04b0">We also <a class="markup--blockquote-anchor" href="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7" target="_blank">briefly touched upon the “why?”</a>. There’s more to it and I just recorded a webinar on this topic… but I don’t want to spoil it :) Just rest assured that half the living universe *doesn’t* die at the end. I’ll share the video when it’s live on YouTube.</blockquote><h4 class="graf-after--blockquote" id="ea1c">SDK examples</h4><p id="2672">The <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/apache-spark.html" target="_blank">SageMaker documentation</a> includes the following examples of using SageMaker and Spark:</p><ul class="postList"><li id="f04b">Classifying MNIST with the built-in <strong class="markup--li-strong">XGBoost</strong> algorithm (PySpark).</li><li id="5c9c">Clustering MNIST with the built-in <strong class="markup--li-strong">K-Means</strong> algorithm (PySpark and Scala).</li><li id="0b0a">Clustering MNIST with a <a href="https://spark.apache.org/docs/2.2.0/ml-pipeline.html" target="_blank"><strong class="markup--li-strong">Spark pipeline</strong></a>, running the <strong class="markup--li-strong">PCA</strong> algorithm in MLlib and the built-in <strong class="markup--li-strong">K-Means</strong> algorithm in SageMaker (Scala).</li></ul><p id="9abb">Copying and pasting from web pages is unpleasant, so I did it for you. I’m happy to share these examples in <strong class="markup--p-strong">text and Zeppelin format on </strong><a href="https://gitlab.com/juliensimon/dlnotebooks/tree/master/spark/sagemaker-spark" target="_blank"><strong class="markup--p-strong">Github</strong></a><strong class="markup--p-strong">.</strong></p><blockquote class="graf--blockquote graf--hasDropCapModel" id="0b33"><a class="markup--blockquote-anchor" href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-zeppelin.html" target="_blank">Zeppelin is pre-installed on EMR</a>. Here’s how to <a class="markup--blockquote-anchor" href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html" target="_blank">set up remote connectivity</a> from your machine to the EMR web applications.</blockquote><p class="graf-after--blockquote" id="fb38">Just make sure you update the IAM role used in the code (account number, role name) and you’re all set. Don’t thank me: I’m just doing my job :*)</p><div class="graf--mixtapeEmbed" id="0170"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/juliensimon/dlnotebooks/tree/master/spark" title="https://github.com/juliensimon/dlnotebooks/tree/master/spark"><strong class="markup--mixtapeEmbed-strong">juliensimon/dlnotebooks</strong><br/><em class="markup--mixtapeEmbed-em">dlnotebooks - Machine Learning &amp; Deep Learning notebooks</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="0c28d497215555e65872f07e493c5dbe" data-thumbnail-img-id="0*IAgrSvYRSb3de8Li." href="https://github.com/juliensimon/dlnotebooks/tree/master/spark" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*IAgrSvYRSb3de8Li.);"></a></div><p class="graf-after--mixtapeEmbed" id="5d1a">I would suggest running all these examples and reading the <a href="https://readthedocs.org/projects/sagemaker-pyspark/" target="_blank"><strong class="markup--p-strong">SageMaker Spark SDK documentation</strong></a> to making sure you understand the finer points. Not that there are many: this felt pretty straightforward when I did, so it shouldn’t take you long either.</p><h4 id="7ac0">Revisiting our spam classifier</h4><p id="394c">We’ll use the exact same technique as in the previous post (shamelessly copying and pasting here, ha!).</p><blockquote class="graf--blockquote" id="ae81">Our raw data set is composed of <strong class="markup--blockquote-strong">1-line messages</strong> stored in <strong class="markup--blockquote-strong">two files</strong>:</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="39c2">- the ‘<a class="markup--blockquote-anchor" href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/ham" rel="nofollow noopener noopener" target="_blank">ham</a>’ file: 4827 valid messages,</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="023b">- the ‘<a class="markup--blockquote-anchor" href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/spam" rel="nofollow noopener noopener" target="_blank">spam</a>’ file: 747 messages.</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="cbd6">In order to classify these messages, we need to build an intermediate data set with <strong class="markup--blockquote-strong">two classes</strong>. For this purpose, we’re going to use a simple but efficient technique called <a class="markup--blockquote-anchor" href="https://en.wikipedia.org/wiki/Feature_hashing" rel="nofollow noopener noopener" target="_blank"><strong class="markup--blockquote-strong">Feature Hashing</strong></a>:</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="b7b7">For each message in the data set, we first <strong class="markup--blockquote-strong">hash</strong> its words into a <strong class="markup--blockquote-strong">fixed</strong> number of buckets.</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="d38c">Then, we build a <strong class="markup--blockquote-strong">vector</strong> indicating non-zero occurrences for each word: these are the <strong class="markup--blockquote-strong">features</strong> that will be used to decide whether a message is spam or not.</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="6b8e">For a valid message, the corresponding <strong class="markup--blockquote-strong">label</strong> will be zero, i.e. the message is not spam. Accordingly, for a spam message, the label will be one.</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="a358">Finally, we split the data set<strong class="markup--blockquote-strong"> 80/20</strong> for training and validation</blockquote><p class="graf-after--blockquote" id="7124">Nothing new here. I just added is a bit of clean-up: convert everything to lower case, remove all punctuation and numbers, trim white spaces. In real-life, you’d certainly want to do more (stemming, etc.).</p><figure id="5451"><script src="https://gist.github.com/juliensimon/1337dae92f0c927ef06859845ff56f84.js"></script></figure><p id="8aa5">Once we’re done, the data set looks like this: each line holds a label and a feature vector.</p><figure id="276b"><script src="https://gist.github.com/juliensimon/275cc49415f0d0c2e9f54d2543deff34.js"></script></figure><h4 id="4eeb">Training with XGBoost</h4><p id="b448">Now we get to the good stuff: let’s use the <strong class="markup--p-strong">high-level API</strong> in the SageMaker Spark SDK to train with XGBoost.</p><p id="8c1e">First, we have to convert the training set to <strong class="markup--p-strong">libsvm</strong> format, which is what XGBoost <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html" target="_blank">expects</a>. Fortunately, Spark has a friendly API for this.</p><blockquote class="graf--blockquote" id="8150">Most if not all other built-in algorithms can train on protobuf data: the SageMaker Spark SDK will <strong class="markup--blockquote-strong">automatically</strong> handle conversion between DataFrames and protobuf. XGBoost is an open-source algorithm, which is why it’s a bit different in this respect.</blockquote><p class="graf-after--blockquote" id="0d45">Then, we set up our training and prediction infrastructure in a <strong class="markup--p-strong">single API call</strong>: this is even simpler than the regular <a href="https://github.com/aws/sagemaker-python-sdk" target="_blank">SageMaker SDK</a>.</p><p id="2b94">Finally, we set up hyper parameters and start training.</p><figure id="48d6"><script src="https://gist.github.com/juliensimon/21c7e246e41947b9c4f3c01dcd2d4714.js"></script></figure><p id="2305">This fires up our training instance from Spark and after a few minutes, voila! Our model has been trained…</p><figure id="be79"><img class="graf-image" src="image03.webp"/></figure><p id="a19d">… and deployed.</p><figure id="de79"><img class="graf-image" src="image01.webp"/></figure><p id="c4d4">Isn’t this the <strong class="markup--p-strong">simplest</strong> integration ever? :) If you needed to train on 100 instances, things wouldn’t be any different.</p><h4 id="1283">Predicting with XGBoost</h4><p id="e588">Predicting is simple too: we just need convert the test data set to libsvm and predict.</p><figure id="a189"><script src="https://gist.github.com/juliensimon/dc43521368c3cd50dca10fc0aa5bd0ba.js"></script></figure><p id="f2e0">Here’s what the predicted data and the accuracy look like.</p><figure id="1fe7"><script src="https://gist.github.com/juliensimon/5105eeb63fc618ec953626d661e30b8b.js"></script></figure><p id="6210">What about the EMR cluster? Well, it didn’t even blink. It takes more than this tiny data set to put four m4.2xlarge instances to work :)</p><figure id="4cb6"><img class="graf-image" src="image04.webp"/></figure><p id="a7bc">Still, you could be processing <strong class="markup--p-strong">huge data sets</strong> in exactly the same way:</p><ul class="postList"><li id="5343"><strong class="markup--li-strong">ETL</strong> at scale on Spark,</li><li id="51db"><strong class="markup--li-strong">Store</strong> processed data in <em class="markup--li-em">DataFrames</em> or in S3,</li><li id="4b26"><strong class="markup--li-strong">Train</strong> at scale on SageMaker, possibly using one of the so-called “<a href="https://www.allthingsdistributed.com/2018/03/Infinitely-scalable-machine-learning-with-Amazon-SageMaker.html" target="_blank">infinitely scalable algorithms</a>” (my <a href="https://www.youtube.com/watch?v=IeIUr78OrE0" target="_blank">session from the Tel Aviv Summit</a> will also tell you more).</li><li id="aaa3"><strong class="markup--li-strong">Deploy</strong> at scale on SageMaker with <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html" target="_blank">auto scaling</a>.</li></ul><p id="183d">You don’t usually get the best of both worlds in life (most of us would settle for the best of *one* world, right?). However, when it comes to analytics at scale, this Spark-SageMaker combo has <strong class="markup--p-strong">a lot</strong> going for it.</p><p id="1351">Very curious to see what you will build with this, so go and <strong class="markup--p-strong">shake the universe</strong>, my friends!</p><p id="e64e">That’s it for today. As always, thank you for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon/" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="9514"><em class="markup--p-em">I guess this advice also stands for analytics at scale \m/</em></p><figure id="0273"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/CTt1vk9nM9c?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>