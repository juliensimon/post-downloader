<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Johnny Pi, I am your father — part 5: adding MXNet for local image classification</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="6a72">Johnny Pi, I am your father — part 5: adding MXNet for local image classification</h3><p id="fe27">In the <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" target="_blank">previous post</a>, we learned how to use <a href="https://aws.amazon.com/rekognition/" target="_blank">Amazon Rekognition</a> to let our robot detect faces and labels in pictures taken with its own camera. This is a very cool feature, but could we do the same thing with <strong class="markup--p-strong">local resources </strong>only, i.e. without relying on a network connection and a cloud-based API?</p><p id="4ead">Or course we can. In this post, I’ll show you how to classify images using a local Deep Learning model. We’ll use the <a href="http://mxnet.io" target="_blank">Apache MXNet</a> library, which I’ve covered extensively in <a href="https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4" target="_blank">another series</a>. Let’s get to work.</p><figure id="564c"><img class="graf-image" src="image04.webp"/></figure><h4 id="cdc8">Sending commands</h4><p id="1352">We’re going to use the same MQTT topic as for Rekognition, namely <em class="markup--p-em">JohnnyPi/see</em>: we’ll just add an extra word to specify whether Rekognition or MXNet should be used.</p><p id="6405">Since we’re not invoking any additional AWS API, there is no additional IAM setup required (and there was much rejoicing!).</p><h4 id="e2ec">Installing Apache MXNet on the Raspberry Pi.</h4><p id="9dcd">We have two options: build from source (as explained in <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" target="_blank">this post</a>) or install directly with <em class="markup--p-em">pip</em>. Let’s go for the second option, which is obviously easier and faster.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="f44a">If you’re interested in building from source, <a class="markup--blockquote-anchor" href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" target="_blank">here</a> are the instructions.</blockquote><figure class="graf-after--blockquote" id="f449"><script src="https://gist.github.com/juliensimon/f0aa2459836af9283228ad1a95d00903.js"></script></figure><h4 id="d097">Picking a Deep Learning model for image classification</h4><p id="e9ed">Now what about the model? Given the limited processing power and storage capabilities of the Pi, it’s obvious we’re never be able to train a model with it. Fortunately, we can pick pre-trained models from MXNet’s <a href="https://mxnet.incubator.apache.org/model_zoo/" target="_blank">model zoo</a>. As we found <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" target="_blank">earlier</a>, we need a model that is <strong class="markup--p-strong">small enough</strong> to fit in the Pi’s memory (only 1GB). <a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank">Inception v3</a> is such a model.</p><blockquote class="graf--blockquote" id="1115">Published in December 2015, <a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1512.00567" rel="nofollow noopener noopener" target="_blank">Inception v3</a> is an evolution of the <a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1409.4842" rel="nofollow noopener noopener" target="_blank">GoogleNet</a> model (which won the <a class="markup--blockquote-anchor" href="http://image-net.org/challenges/LSVRC/2014/" rel="nofollow noopener noopener" target="_blank">2014 ImageNet challenge</a>). Inception v3 is <strong class="markup--blockquote-strong">15–25% more accurate</strong> than the best models available at the time, while being <strong class="markup--blockquote-strong">six times cheaper computationally</strong> and using at least <strong class="markup--blockquote-strong">five times less parameters</strong> (i.e. less RAM is required to use the model).</blockquote><p class="graf-after--blockquote" id="d7b8">Our version of Inception v3 has been pre-trained on the <a href="http://image-net.org" target="_blank">ImageNet</a> dataset, which holds over 1.2 million pictures of animals and objects classified in 1,000 categories. Let’s fetch it from the model zoo.</p><figure id="255f"><script src="https://gist.github.com/juliensimon/e9e6ef71b3a84e18bd2537639e75b7a8.js"></script></figure><p id="b833">OK, now it’s time to write some code!</p><h4 id="d844">Code overview</h4><p id="d60b">With MXNet added into the mix, here’s what should now happen when we send an MQTT message to the <em class="markup--p-em">JohnnyPi/see</em> topic.</p><p id="cb40">First, we’ll take a picture with the Pi camera. If the message starts with ‘<em class="markup--p-em">reko</em>’, we’ll use Rekognition just like we did in the previous post. No changes here.</p><p id="1b1d">If the message starts with ‘<em class="markup--p-em">mxnet</em>’, then we will:</p><ul class="postList"><li id="5728">load the image and convert it in a format than can be fed to the MXNet model,</li><li id="9f8e">use the model the predict the categories for this image,</li><li id="3331">extract the 5 most likely categories (aka ‘top 5’),</li><li id="2a47">build a text message with the top 1 category and use Polly to play it,</li><li id="223e">if the message ends with ‘tweet’, send a tweet containing both the image and the message.</li></ul><p id="6e17">This is what the updated callback looks like.</p><blockquote class="graf--blockquote" id="1380">As usual, you’ll find the full code on <a class="markup--blockquote-anchor" href="https://github.com/juliensimon/johnnypi/tree/master/part5" target="_blank">Github</a>.</blockquote><figure class="graf-after--blockquote" id="f05f"><script src="https://gist.github.com/juliensimon/8325e7b9ef65e8de58423296631dd9d9.js"></script></figure><h4 id="fa8f">Loading and classifying the image</h4><p id="b360">I’ve already covered these exact steps in full detail in a <a href="https://medium.com/towards-data-science/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" target="_blank">previous post</a>, so I’ll stick to a quick summary here.</p><p id="d810">In <em class="markup--p-em">inception.load_image()</em>, we use <a href="http://opencv.org" target="_blank">OpenCV</a> and <a href="http://www.numpy.org" target="_blank">Numpy</a> to load the image and transform it a 4-dimension array corresponding to the input layer on the Inception v3 model: (1L, 3L, 224L, 224L) : one sample with three 224x244 images (red, green and blue).</p><p id="7924">In <em class="markup--p-em">inception.predict()</em>, we forward the sample through the model and receive a vector of 1,000 probabilities, one for each class.</p><p id="7f9e">In <em class="markup--p-em">inception.get_top_categories(), </em>we sort these probabilities and find the top 5 classes.</p><h4 id="2e8a">Speaking and tweeting</h4><p id="4d0a">This part is almost identical to what we did with Rekognition, so no need to repeat ourselves.</p><h4 id="045d">Testing</h4><p id="8c5a">Once again, we’ll use <a href="http://www.mqttfx.org/" target="_blank">MQTT.fx</a> to send commands to the <em class="markup--p-em">JohnnyPi/see</em> topic:</p><ul class="postList"><li id="4e0f">two for Rekognition: ‘<em class="markup--li-em">reko</em>’ and ‘<em class="markup--li-em">reko tweet</em>’). We’ve already tested Rekognition in the previous post, so let’s skip these here.</li><li id="39ce">two for MXNet: ‘<em class="markup--li-em">mxnet</em>’ and ‘<em class="markup--li-em">mxnet tweet</em>’.</li></ul><p id="d6fb">Let’s try a first object.</p><figure id="2030"><img class="graf-image" src="image01.webp"/></figure><p id="25e6">The output is:</p><pre class="graf--pre" id="777e">Topic=JohnnyPi/see<br/>Message=mxnet<br/>forward pass in 3.14714694023<br/>probability=0.467848, class=n03085013 computer keyboard, keypad<br/>probability=0.156991, class=n04152593 screen, CRT screen<br/>probability=0.090207, class=n03782006 monitor<br/>probability=0.079132, class=n03793489 mouse, computer mouse<br/>probability=0.048446, class=n03642806 laptop, laptop computer<br/>[(0.46784759, 'n03085013 computer keyboard, keypad'), (0.15699127, 'n04152593 screen, CRT screen'), (0.090206854, 'n03782006 monitor'), (0.079132304, 'n03793489 mouse, computer mouse'), (0.0484455, 'n03642806 laptop, laptop computer')]<br/>I'm 46% sure that this is a computer keyboard, keypad.</pre><p class="graf-after--pre" id="ba72">Let’s try another one.</p><figure id="a029"><img class="graf-image" src="image03.webp"/></figure><p id="3016">The output is:</p><pre class="graf--pre" id="5d7c">Topic=JohnnyPi/see<br/>Message=mxnet<br/>forward pass in 3.1429579258<br/>probability=0.511771, class=n03602883 joystick<br/>probability=0.185790, class=n03584254 iPod<br/>probability=0.099805, class=n03691459 loudspeaker, speaker, speaker unit, loudspeaker system, speaker system<br/>probability=0.022552, class=n04074963 remote control, remote<br/>probability=0.017473, class=n03891332 parking meter<br/>[(0.5117712, 'n03602883 joystick'), (0.18578961, 'n03584254 iPod'), (0.09980496, 'n03691459 loudspeaker, speaker, speaker unit, loudspeaker system, speaker system'), (0.022552039, 'n04074963 remote control, remote'), (0.017472528, 'n03891332 parking meter')]<br/>I'm 51% sure that this is a joystick.</pre><p class="graf-after--pre" id="3337">And let’s tweet a last one:</p><figure id="79c5"><img class="graf-image" src="image02.webp"/></figure><p id="68cf">The output is indeed:</p><pre class="graf--pre" id="c0b0">Topic=JohnnyPi/see<br/>Message=mxnet tweet<br/>forward pass in 3.00172805786<br/>probability=0.460716, class=n04557648 water bottle<br/>probability=0.098160, class=n01990800 isopod<br/>probability=0.079460, class=n04116512 rubber eraser, rubber<br/>probability=0.055721, class=n04286575 spotlight, spot<br/>probability=0.032628, class=n02823750 beer glass<br/>[(0.46071553, 'n04557648 water bottle'), (0.098159559, 'n01990800 isopod'), (0.079459898, 'n04116512 rubber eraser, rubber, pencil eraser'), (0.055721201, 'n04286575 spotlight, spot'), (0.03262835, 'n02823750 beer glass')]<br/>I'm 46% sure that this is a water bottle.</pre><h4 class="graf-after--pre" id="7731">What’s next</h4><p id="9578">Our robot is now able to move, measure distance to objects, detect faces and classify objects. In the next part, I’ll show you how to use an <a href="https://aws.amazon.com/iotbutton/" target="_blank">IoT button</a> to send commands to the robot. Once this this complete, we’ll move on to building an Amazon Echo skill for voice control. Stay tuned :)</p><p id="1f31">As always, thanks for reading.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="5457">Part 0: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36" target="_blank">a sneak preview</a></p><p id="4525">Part 1: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce" rel="noopener nofollow nofollow noopener noopener" target="_blank">moving around</a></p><p id="58a9">Part 2: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86" rel="nofollow noopener nofollow noopener noopener" target="_blank">the joystick</a></p><p id="a175">Part 3: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76" target="_blank">cloud-based speech</a></p><p id="7aa6">Part 4: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" target="_blank">cloud-based vision</a></p></div></div></section><section class="section"><div><hr/></div><div><div><p id="846f"><em class="markup--p-em">This article was written while listening to Pink Floyd’s “Dark Side of the Moon”, which is pretty much the only thing that my jet-lagged brain can take right now :)</em></p></div></div></section>
</section>
</article></body></html>