<!DOCTYPE html>
<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>An introduction to the MXNet API — part 3</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">


<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="050c">An introduction to the MXNet API — part 3</h3><p id="7ea0"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-2-ce761513124e" target="_blank">In part 2</a>, we discussed how Symbols allow us to define computation graphs processing data stored in NDArrays (which we studied in <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" target="_blank">part 1</a>).</p><p id="a26c">In this article, we’re going to use what we learned on <em class="markup--p-em">Symbols</em> and <em class="markup--p-em">NDArrays</em> to prepare some data and build a neural network. Then, we’ll use the <a href="http://mxnet.io/api/python/module.html" target="_blank">Module API</a> to train the network and predict results.</p><h4 id="649f">Defining our data set</h4><p id="9a08">Our (imaginary) data set is composed of <strong class="markup--p-strong">1000 data samples</strong></p><ul class="postList"><li id="980e">Each sample has <strong class="markup--li-strong">100 features</strong>.</li><li id="90b8">A feature is represented by a <strong class="markup--li-strong">float value between 0 and 1</strong>.</li><li id="55e6">Samples are split in <strong class="markup--li-strong">10 categories</strong>. The purpose of the network will be to predict the correct category for a given sample.</li><li id="ed61">We’ll use 800 samples for <strong class="markup--li-strong">training</strong> and 200 samples for <strong class="markup--li-strong">validation</strong>.</li><li id="d78a">We’ll use a <strong class="markup--li-strong">batch size</strong> of 10 for training and validation</li></ul><pre class="graf--pre" id="df70">import mxnet as mx<br/>import numpy as np<br/>import logging</pre><pre class="graf--pre graf-after--pre" id="ee6f">logging.basicConfig(level=logging.INFO)</pre><pre class="graf--pre graf-after--pre" id="1b16">sample_count = 1000<br/>train_count = 800<br/>valid_count = sample_count - train_count</pre><pre class="graf--pre graf-after--pre" id="19d3">feature_count = 100<br/>category_count = 10<br/>batch=10</pre><h4 class="graf-after--pre" id="74bf">Generating the data set</h4><p id="c1e8">Let’s use a uniform distribution to generate the 1000 samples. They are stored in an <em class="markup--p-em">NDArray</em> named ‘X’: <strong class="markup--p-strong">1000 lines, 100 columns</strong>.</p><pre class="graf--pre" id="ece8">X = mx.nd.uniform(low=0, high=1, shape=(sample_count,feature_count))</pre><pre class="graf--pre graf-after--pre" id="b5b2">&gt;&gt;&gt; X.shape<br/>(1000L, 100L)<br/>&gt;&gt;&gt; X.asnumpy()<br/>array([[ 0.70029777,  0.28444085,  0.46263582, ...,  0.73365158,<br/>         0.99670047,  0.5961988 ],<br/>       [ 0.34659418,  0.82824177,  0.72929877, ...,  0.56012964,<br/>         0.32261589,  0.35627609],<br/>       [ 0.10939316,  0.02995235,  0.97597599, ...,  0.20194994,<br/>         0.9266268 ,  0.25102937],<br/>       ...,<br/>       [ 0.69691515,  0.52568913,  0.21130568, ...,  0.42498392,<br/>         0.80869114,  0.23635457],<br/>       [ 0.3562004 ,  0.5794751 ,  0.38135922, ...,  0.6336484 ,<br/>         0.26392782,  0.30010447],<br/>       [ 0.40369365,  0.89351988,  0.88817406, ...,  0.13799617,<br/>         0.40905532,  0.05180593]], dtype=float32)</pre><p class="graf-after--pre" id="d5c0">The categories for these 1000 samples are represented as <strong class="markup--p-strong">integers</strong> in the 0–9 range. They are randomly generated and stored in an <em class="markup--p-em">NDArray</em> named ‘Y’.</p><pre class="graf--pre" id="60cf">Y = mx.nd.empty((sample_count,))<br/>for i in range(0,sample_count-1):<br/>  Y[i] = np.random.randint(0,category_count)</pre><pre class="graf--pre graf-after--pre" id="827a">&gt;&gt;&gt; Y.shape<br/>(1000L,)<br/>&gt;&gt;&gt; Y[0:10].asnumpy()<br/>array([ 3.,  3.,  1.,  9.,  4.,  7.,  3.,  5.,  2.,  2.], dtype=float32)</pre><h4 class="graf-after--pre" id="396f">Splitting the data set</h4><p id="8136">Next, we’re splitting the data set <strong class="markup--p-strong">80/20</strong> for <strong class="markup--p-strong">training</strong> and <strong class="markup--p-strong">validation</strong>. We use the <em class="markup--p-em">NDArray.crop</em> function to do this. Here, the data set is completely random, so we can use the top 80% for training and the bottom 20% for validation. In real life, we’d probably <strong class="markup--p-strong">shuffle</strong> the data set first, in order to avoid potential bias on sequentially-generated data.</p><pre class="graf--pre" id="6487">X_train = mx.nd.crop(X, begin=(0,0), end=(train_count,feature_count-1))</pre><pre class="graf--pre graf-after--pre" id="95ca">X_valid = mx.nd.crop(X, begin=(train_count,0), end=(sample_count,feature_count-1))</pre><pre class="graf--pre graf-after--pre" id="0f8c">Y_train = Y[0:train_count]</pre><pre class="graf--pre graf-after--pre" id="e0c3">Y_valid = Y[train_count:sample_count]</pre><p class="graf-after--pre" id="886a">Our data is now ready!</p><h4 id="5541">Building the network</h4><p id="6e1d">Our network is pretty simple. Let’s look at each layer:</p><ul class="postList"><li id="2890">The <strong class="markup--li-strong">input layer</strong> is represented by a <em class="markup--li-em">Symbol</em> named ‘data’. We’ll bind it to the actual input data later on.</li></ul><pre class="graf--pre" id="70f8">data = mx.sym.Variable('data')</pre><ul class="postList"><li class="graf-after--pre" id="1ab4"><em class="markup--li-em">fc1</em>, the <strong class="markup--li-strong">first hidden layer</strong> is built from <strong class="markup--li-strong">64 fully-connected neurons</strong>, i.e. each feature in the input layer is connected to all 64 neurons. As you can see, we use the high-level <em class="markup--li-em">Symbol.FullyConnected</em> function, which is much more convenient than building each connection manually!</li></ul><pre class="graf--pre" id="5582">fc1 = mx.sym.FullyConnected(data, name='fc1', num_hidden=64)</pre><ul class="postList"><li class="graf-after--pre" id="48b5">Each output of <em class="markup--li-em">fc1</em> goes through an <a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank"><strong class="markup--li-strong">activation function</strong></a>. Here we use a <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" target="_blank">rectified linear unit</a>, aka ‘relu’. I promised minimal theory, so let’s just say that an activation function is how we decide whether a neuron should “fire” or not, i.e. whether its inputs are meaningful enough in predicting the correct result.</li></ul><pre class="graf--pre" id="7f79">relu1 = mx.sym.Activation(fc1, name='relu1', act_type="relu")</pre><ul class="postList"><li class="graf-after--pre" id="4ffe"><em class="markup--li-em">fc2</em>, the <strong class="markup--li-strong">second hidden layer</strong> is built from <strong class="markup--li-strong">10 fully-connected neurons</strong>, which map to our <strong class="markup--li-strong">10 categories</strong>. Each neuron outputs a float value of arbitrary scale. The largest of the 10 values represents the <strong class="markup--li-strong">most likely category</strong> for the data sample.</li></ul><pre class="graf--pre" id="6cfc">fc2 = mx.sym.FullyConnected(relu1, name='fc2', num_hidden=category_count)</pre><ul class="postList"><li class="graf-after--pre" id="a6e6">The <strong class="markup--li-strong">output layer</strong> applies the <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank"><strong class="markup--li-strong">Softmax function</strong></a> to the 10 values coming from the <em class="markup--li-em">fc2</em> layer: they are transformed into 10 values between 0 and 1 that add up to 1. Each value represents the <strong class="markup--li-strong">predicted probability for each category</strong>, the largest one pointing at the most likely category.</li></ul><pre class="graf--pre" id="e2c5">out = mx.sym.SoftmaxOutput(fc2, name='softmax')<br/>mod = mx.mod.Module(out)</pre><h4 class="graf-after--pre" id="7a89">Building the data iterator</h4><p id="fb89">In part 1, we saw that neural networks not trained one sample at a time, as this is quite inefficient from a performance point of view. Instead, we use <strong class="markup--p-strong">batches</strong>, i.e. <strong class="markup--p-strong">a fixed number of samples</strong>.</p><p id="70d3">In order to deliver these batches to the network, we need to build an <strong class="markup--p-strong">iterator</strong> using the <em class="markup--p-em">NDArrayIter</em> function. Its parameters are the <strong class="markup--p-strong">training data</strong>, the categories (MXNet calls these <strong class="markup--p-strong">labels</strong>) and the <strong class="markup--p-strong">batch size</strong>.</p><p id="096a">As you can see, we can indeed iterate on the data set, 10 samples and 10 labels at a time. We then call the <em class="markup--p-em">reset()</em> function to restore the iterator to its original state.</p><pre class="graf--pre" id="9cbc">train_iter = mx.io.NDArrayIter(data=X_train,label=Y_train,batch_size=batch)</pre><pre class="graf--pre graf-after--pre" id="426b">&gt;&gt;&gt; for batch in train_iter:<br/>...   print batch.data<br/>...   print batch.label<br/>...<br/>[&lt;NDArray 10x99 <a class="markup--pre-anchor" href="http://twitter.com/cpu" target="_blank" title="Twitter profile for @cpu">@cpu</a>(0)&gt;]<br/>[&lt;NDArray 10 <a class="markup--pre-anchor" href="http://twitter.com/cpu" target="_blank" title="Twitter profile for @cpu">@cpu</a>(0)&gt;]<br/>[&lt;NDArray 10x99 <a class="markup--pre-anchor" href="http://twitter.com/cpu" target="_blank" title="Twitter profile for @cpu">@cpu</a>(0)&gt;]<br/>[&lt;NDArray 10 <a class="markup--pre-anchor" href="http://twitter.com/cpu" target="_blank" title="Twitter profile for @cpu">@cpu</a>(0)&gt;]<br/>[&lt;NDArray 10x99 <a class="markup--pre-anchor" href="http://twitter.com/cpu" target="_blank" title="Twitter profile for @cpu">@cpu</a>(0)&gt;]<br/>[&lt;NDArray 10 <a class="markup--pre-anchor" href="http://twitter.com/cpu" target="_blank" title="Twitter profile for @cpu">@cpu</a>(0)&gt;]<br/><em class="markup--pre-em">&lt;edited for brevity&gt;<br/>&gt;&gt;&gt; </em>train_iter.reset()</pre><p class="graf-after--pre" id="6155">Our network is now ready for training!</p><h4 id="5a39">Training the model</h4><p id="db4b">First, let’s <strong class="markup--p-strong">bind</strong> the input symbol to the actual data set (samples and labels). This is where the iterator comes in handy.</p><pre class="graf--pre" id="dbdb">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)</pre><p class="graf-after--pre" id="dd56">Next, let’s <strong class="markup--p-strong">initialize</strong> the neuron weights in the network. This is actually a very important step: initializing them with the “right” technique will help the network learn <strong class="markup--p-strong">much faster</strong>. The Xavier initializer (named after his inventor, Xavier Glorot —<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank"> PDF</a>) is one of these techniques.</p><pre class="graf--pre" id="7dc8"># Allowed, but not efficient<br/>mod.init_params()<br/># Much better<br/>mod.init_params(initializer=mx.init.Xavier(magnitude=2.))</pre><p class="graf-after--pre" id="7812">Next, we need to define the <strong class="markup--p-strong">optimization</strong> parameters:</p><ul class="postList"><li id="1f56">we’re using the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank"><strong class="markup--li-strong">Stochastic Gradient Descent</strong></a> algorithm (aka SGD), which has long been used for Machine Learning and Deep Learning application.</li><li id="e2cc">we’re setting the <strong class="markup--li-strong">learning rate</strong> to 0.1, a pretty typical value for SGD.</li></ul><pre class="graf--pre" id="fa49">mod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 0.1), ))</pre><p class="graf-after--pre" id="18d9">And finally, we can train the network! We’re doing it over 50 <strong class="markup--p-strong">epochs</strong>, which means the full data set will flow 50 times through the network (in batches of 10 samples).</p><pre class="graf--pre" id="3abf">mod.fit(train_iter, num_epoch=50)</pre><pre class="graf--pre graf-after--pre" id="2222">INFO:root:Epoch[0] Train-accuracy=0.097500<br/>INFO:root:Epoch[0] Time cost=0.085<br/>INFO:root:Epoch[1] Train-accuracy=0.122500<br/>INFO:root:Epoch[1] Time cost=0.074<br/>INFO:root:Epoch[2] Train-accuracy=0.153750<br/>INFO:root:Epoch[2] Time cost=0.087<br/>INFO:root:Epoch[3] Train-accuracy=0.162500<br/>INFO:root:Epoch[3] Time cost=0.082<br/>INFO:root:Epoch[4] Train-accuracy=0.192500<br/>INFO:root:Epoch[4] Time cost=0.094<br/>INFO:root:Epoch[5] Train-accuracy=0.210000<br/>INFO:root:Epoch[5] Time cost=0.108<br/>INFO:root:Epoch[6] Train-accuracy=0.222500<br/>INFO:root:Epoch[6] Time cost=0.104<br/>INFO:root:Epoch[7] Train-accuracy=0.243750<br/>INFO:root:Epoch[7] Time cost=0.110<br/>INFO:root:Epoch[8] Train-accuracy=0.263750<br/>INFO:root:Epoch[8] Time cost=0.101<br/>INFO:root:Epoch[9] Train-accuracy=0.286250<br/>INFO:root:Epoch[9] Time cost=0.097<br/>INFO:root:Epoch[10] Train-accuracy=0.306250<br/>INFO:root:Epoch[10] Time cost=0.100<br/>...<br/>INFO:root:Epoch[20] Train-accuracy=0.507500<br/>...<br/>INFO:root:Epoch[30] Train-accuracy=0.718750<br/>...<br/>INFO:root:Epoch[40] Train-accuracy=0.923750<br/>...<br/>INFO:root:Epoch[50] Train-accuracy=0.998750<br/>INFO:root:Epoch[50] Time cost=0.077</pre><p class="graf-after--pre" id="3d9b">As we can see, the training accuracy rises rapidly and reaches <strong class="markup--p-strong">99+%</strong> after 50 epochs. It looks like our network was able to learn the training set. That’s pretty impressive!</p><p id="a801">But how does it perform against the validation set?</p><h4 id="f6e8">Validating the model</h4><p id="1e6a">Now we’re going to throw new data samples at the network, i.e. the 20% that <strong class="markup--p-strong">haven’t</strong> been used for training.</p><p id="0ac9">First, we’re building an iterator. This time, we’re using the <strong class="markup--p-strong">validation</strong> samples and labels.</p><pre class="graf--pre" id="7809">pred_iter = mx.io.NDArrayIter(data=X_valid,label=Y_valid, batch_size=batch)</pre><p class="graf-after--pre" id="2bc0">Next, using the <em class="markup--p-em">Module.iter_predict</em>() function, we’re going to run these samples through the network. As we do this, we’re going to compare the <strong class="markup--p-strong">predicted label</strong> with the <strong class="markup--p-strong">actual label</strong>. We’ll keep track of the score and display the <strong class="markup--p-strong">validation accuracy</strong>, i.e. how well the network did on the validation set.</p><pre class="graf--pre" id="4d78">pred_count = valid_count<br/>correct_preds = total_correct_preds = 0</pre><pre class="graf--pre graf-after--pre" id="46f8">for preds, i_batch, batch in mod.iter_predict(pred_iter):<br/>    label = batch.label[0].asnumpy().astype(int)<br/>    pred_label = preds[0].asnumpy().argmax(axis=1)<br/>    correct_preds = np.sum(pred_label==label)<br/>    total_correct_preds = total_correct_preds + correct_preds</pre><pre class="graf--pre graf-after--pre" id="2b72">print('Validation accuracy: %2.2f' % (1.0*total_correct_preds/pred_count))</pre><p class="graf-after--pre" id="c742">There is quite a bit happening here :)</p><p id="910d"><em class="markup--p-em">iter_predict</em>() returns:</p><ul class="postList"><li id="ce01"><em class="markup--li-em">i_batch</em>: the batch number</li><li id="3c44"><em class="markup--li-em">batch</em>: an array of NDArrays. Here, it holds a single <em class="markup--li-em">NDArray</em> storing the current batch. We’re using it to find the labels of the 10 data samples in the current batch. We store them in the <em class="markup--li-em">label</em> numpy array (10 elements).</li><li id="b238"><em class="markup--li-em">preds</em>: an array of <em class="markup--li-em">NDArrays</em>. Here, it holds a single <em class="markup--li-em">NDArray</em> storing predicted labels for the current batch: for each sample, we have <strong class="markup--li-strong">predicted probabilities for all 10 categories </strong>(10x10 matrix). Thus, we’re using <em class="markup--li-em">argmax</em>() to find the <strong class="markup--li-strong">index</strong> of the highest value, i.e. the <strong class="markup--li-strong">most likely category</strong>. Thus, <em class="markup--li-em">pred_label</em> is a 10-element array holding the predicted category for each data sample in the current batch.</li></ul><p id="d2e3">Then, we’re comparing the number of equal values in <em class="markup--p-em">label</em> and <em class="markup--p-em">pred_label</em> using <em class="markup--p-em">Numpy.sum()</em>.</p><p id="bbfa">Finally, we compute and display the validation accuracy.</p><pre class="graf--pre" id="de32">Validation accuracy: 0.09</pre><p class="graf-after--pre" id="c5d0">What? 9%? <strong class="markup--p-strong">This is really bad!</strong> If you needed proof that our data set was random, there you have it!</p><p id="9cbb">The bottom line is that you can indeed train a neural network to learn <strong class="markup--p-strong">anything</strong>, but if your data is <strong class="markup--p-strong">meaningless</strong> (like ours here), it won’t be able to predict anything. <strong class="markup--p-strong">Garbage in, garbage out!</strong></p><p id="4a20">If you read this far, I guess you deserve to get the full code for this example ;) Please take some time to use it on your own data, it’s the best way to learn.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="7ec5">Next :</p><ul class="postList"><li id="8171"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" target="_blank">Part 4</a>: Using a pre-trained model for image classification (Inception v3)</li><li id="1261"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" target="_blank">Part 5</a>: More pre-trained models (VGG16 and ResNet-152)</li><li id="e12d"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" target="_blank">Part 6</a>: Real-time object detection on a Raspberry Pi (and it speaks, too!)</li></ul><figure id="5c99"><script src="https://gist.github.com/juliensimon/7cfef0423b0183e891774a289e156b49.js"></script></figure></div></div></section>
</section>
</article></body></html>
