<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Keras shoot-out, part 3: fine-tuning</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="a34b">Keras shoot-out, part 3: fine-tuning</h3><p id="ff97">Using <a href="http://keras.io" target="_blank">Keras</a> and the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">CIFAR-10</a> dataset, we <a href="https://medium.com/@julsimon/keras-shoot-out-tensorflow-vs-mxnet-51ae2b30a9c0" target="_blank">previously</a> compared the training performance of two Deep Learning libraries, <a href="http://mxnet.io" target="_blank">Apache MXNet</a> and <a href="http://tensorflow.org" target="_blank">Tensorflow</a>.</p><p id="f3cb">In this article, we’ll continue to explore this theme. I’ll show you how to:</p><ul class="postList"><li id="2c14">save and load a trained model,</li><li id="e485">build a subset of CIFAR-10 using samples from two classes,</li><li id="37c4">retrain the model on this subset to optimise prediction (aka “fine-tuning”),</li><li id="c301">improve training time by freezing layers.</li></ul><figure id="5902"><img class="graf-image" src="image01.webp"/><figcaption>Go ahead, global minimum, make my day.</figcaption></figure><h4 id="1631">Preparing our pre-trained models</h4><p id="783b">Using the same script (<em class="markup--p-em">~/keras/examples/cifar10_resnet50.py)</em>, I trained a <a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank">Resnet-50</a> model on CIFAR-10 using first MXNet 0.11, then Tensorflow 1.2. Here is the setup I used:</p><ul class="postList"><li id="30c5">All 8 GPUs on a p2.8xlarge AWS instance,</li><li id="82c4">batch size: 256,</li><li id="08fc">Data augmentation: enabled,</li><li id="7994">Number of epochs: 200.</li></ul><blockquote class="graf--blockquote" id="974d">To save the trained model, I only added one line of code: <em class="markup--blockquote-em">model.save(modelname). </em>That’s all there is to it!</blockquote><p class="graf-after--blockquote" id="b7d4">Even with 8 GPUs, training takes time: about 2h for MXNet and about 3h30 for Tensorflow. Pretty heavy lifting! I uploaded the models to my own personal <a href="http://jsimon-deep-learning-models.s3-website-us-east-1.amazonaws.com" target="_blank">model zoo</a>: feel free to grab them and run your own tests :)</p><ul class="postList"><li id="ca13">MXNet : <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-mxnet011rc3-0200.h5" target="_blank">model</a> (HDF5, 91MB), <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-mxnet011rc3-0200.log" target="_blank">training log</a> (text, 6MB), test accuracy: <strong class="markup--li-strong">82.12%</strong></li><li id="4893">Tensorflow: <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-tensorflow12-0200.h5" target="_blank">model</a> (HDF5, 181MB), <a href="https://jsimon-deep-learning-models.s3.amazonaws.com/resnet50-tensorflow12-0200.log" target="_blank">training log</a> (text, 6MB), test accuracy: <strong class="markup--li-strong">75.48%</strong></li></ul><p id="ee30">Now let’s see how we can load our models.</p><h4 id="476f">Loading a model</h4><p id="a8c8">Depending on the backend configured in <em class="markup--p-em">~/.keras/keras.json</em>, we have to load one model or the other. Using <em class="markup--p-em">keras.backend.backend()</em>, it’s easy to figure which one to pick. Then, we simply call <em class="markup--p-em">keras.models.load_model()</em>.</p><p id="2451">What about setting the number of GPUs using for training? For Tensorflow, we’ll use session parameters. For MXNet, we’ll add an extra parameter to <em class="markup--p-em">model.compile().</em></p><figure id="6375"><script src="https://gist.github.com/juliensimon/52f25c1d6bf995e1860093fff6e086f2.js"></script></figure><p id="1aaf">Ok, now let’s take care of data.</p><h4 id="0da2">Extracting the subset</h4><p id="d3e7">Keras provides convenient functions to load commonly used data sets, including CIFAR-10. Nice!</p><p id="ab23">First, we need to extract a given number of samples from two classes. This is the purpose of the <em class="markup--p-em">get_samples()</em> function:</p><ul class="postList"><li id="a777">find the relevant number of sample indexes matching a given class,</li><li id="f825">extract samples (‘x’) and labels (‘y’) for these indexes,</li><li id="9071">normalise pixel values for samples, since we also did it when we trained the initial model.</li></ul><figure id="1274"><script src="https://gist.github.com/juliensimon/8a55d451b82d5bf4cdd635058b028fca.js"></script></figure><h4 id="feae">Building the new data set</h4><p id="504f">The next step is to build the new training and test sets with the <em class="markup--p-em">prepare_dataset()</em> function:</p><ul class="postList"><li id="a436">Concatenate samples for both categories,</li><li id="2498">Concatenate labels for both categories,</li><li id="91b6">One-hot encode labels, e.g. convert ‘6’ to [0, 0, 0, 0, 0, 0, 1, 0, 0, 0].</li></ul><figure id="478c"><script src="https://gist.github.com/juliensimon/99589ebc877d96675c0f14e1d44040a6.js"></script></figure><p id="c813">The last step before actually retraining is to define the optimizer — for now, we’ll use the same one as for the original training — as well as the number of GPUs — we’ll stick to one, as this is the most likely setup for developers fine-tuning a model on their own machine.</p><figure id="93fa"><script src="https://gist.github.com/juliensimon/db533719a2ed96826965dc4411329d13.js"></script></figure><h4 id="502b">Fine-tuning the model</h4><p id="7a2f">OK, we’re now ready to train the model. First, let’s predict our test set to see what the baseline is. Then, we train the model. Finally, we predict the test set again to see how much the model has improved.</p><figure id="56f0"><script src="https://gist.github.com/juliensimon/79a5b5d1b8f38506cbc1a0938348fe8e.js"></script></figure><p id="5b4b">Full code is available on <a href="https://github.com/juliensimon/aws/tree/master/mxnet/keras" target="_blank">Github</a>. Now, let’s run it!</p><h4 id="7ff9">Is it a horse or a car?</h4><p id="4f31">Let’s fine-tune the model on cars and horses (classes 1 and 7) for 10 epochs. Here are the results.</p><ul class="postList"><li id="344f">MXNet: <strong class="markup--li-strong">31 seconds</strong> per epoch, test accuracy: <strong class="markup--li-strong">98.79%</strong> (<a href="https://gist.github.com/juliensimon/0ea6c0301e8f813f6f55c8715f07a4f6" target="_blank">log</a>).</li><li id="7ac4">Tensorflow: <strong class="markup--li-strong">44 seconds</strong> per epoch, test accuracy: <strong class="markup--li-strong">98.55%</strong> (<a href="https://gist.github.com/juliensimon/51e78fda60f1d50583febd3e8560afa8" target="_blank">log</a>).</li></ul><p id="37e9">This is a MASSIVE accuracy improvement after <strong class="markup--p-strong">just a few minutes</strong> of training. Just wait, we can do even better :)</p><h4 id="ad56">Freezing layers</h4><p id="ea8f">The many layers in our model have already learned the car and horse classes. So, it’s probably a waste of time to potentially retrain <strong class="markup--p-strong">all</strong> of them. Maybe it would just be enough to retrain <strong class="markup--p-strong">the last layer</strong>, i.e. the one that actually outputs the probability for the 10 classes.</p><p id="eb2f">Keras includes a very nice feature that lets us decide which layers of a model are trainable and which aren’t. Let’s use it to freeze <strong class="markup--p-strong">all layers but the last one</strong> and try again.</p><figure id="acc8"><script src="https://gist.github.com/juliensimon/8861a66e4666cd8c7d02f0a8c384f850.js"></script></figure><p id="3af9">Here are the results.</p><ul class="postList"><li id="29be">MXNet: <strong class="markup--li-strong">12 seconds</strong> per epoch, test accuracy: <strong class="markup--li-strong">97.29%</strong> (<a href="https://gist.github.com/juliensimon/d1e9b2cacc5ced3ed1a8a081d62105f3" target="_blank">log</a>).</li><li id="d55b">Tensorflow: <strong class="markup--li-strong">13 seconds</strong> per epoch, test accuracy: <strong class="markup--li-strong">98.35%</strong> (<a href="https://gist.github.com/juliensimon/da4e78116c883abcf11a0af831a81cd9" target="_blank">log</a>).</li></ul><p id="778c">As expected, freezing layers <strong class="markup--p-strong">significantly</strong> reduces training time.</p><p id="15e9">Training time is almost identical for both libraries. I guess the work boils down to running backpropagation on a single layer, which both libraries can do equally well on a single GPU. Scaling doesn’t seem to come into play here.</p><p id="4d7d">Accuracy is hardly impacted for Tensorflow, but there is a slight hit for MXNet. Hmmm. Maybe our optimisation parameters are not optimal. Let’s try one last thing :)</p><h4 id="4ebf">Tweaking the optimiser</h4><p id="6516">Picking the right hyper parameters for SGD(learning rate, etc.) is tricky. Here, we’re only training for 10 epochs, which probably makes it even more difficult. One way out of this problem may be to use the <strong class="markup--p-strong">AdaGrad</strong> optimizer, which <strong class="markup--p-strong">automatically</strong> adapts the learning rate</p><blockquote class="graf--blockquote" id="2f42">For an excellent overview of SGD and its variants, please read <a class="markup--blockquote-anchor" href="http://ruder.io/optimizing-gradient-descent/" target="_blank">this post</a> by Sebastian Ruder. It’s by far the best I’ve seen.</blockquote><p class="graf-after--blockquote" id="728c">Here are the results.</p><ul class="postList"><li id="e456">MXNet: <strong class="markup--li-strong">12 seconds</strong> per epoch, test accuracy: <strong class="markup--li-strong">98.90% </strong>after 9 epochs (<a href="https://gist.github.com/juliensimon/1e7edf23993cf36657c349fc15b5cbed" target="_blank">log</a>).</li><li id="f303">Tensorflow: <strong class="markup--li-strong">13 seconds</strong> per epoch, test accuracy: <strong class="markup--li-strong">98.75% </strong>after 8 epochs (<a href="https://gist.github.com/juliensimon/963522ca6db35431544cf88bb130e8a4" target="_blank">log</a>).</li></ul><p id="5338">AdaGrad works its magic indeed. It helps MXNet improve its accuracy and deliver the best score in this test. Tensorflow improves as well.</p><h4 id="c185">Going further</h4><p id="4ede">Here, we fine-tuned the model for two classes, but we still output 10 probabilities (one for each class). The next step would be to add an extra layer with <strong class="markup--p-strong">only two outputs</strong>, forcing the model to decide on two categories only (and not ten).</p><p id="b015">We would need to modify our subset labels, i.e. one-hot encode them with only two values. Let’s keep this for a future article :)</p><h4 id="e233">Conclusion</h4><p id="1b98">Fine-tuning is a very powerful way to improve the accuracy of a model in a <strong class="markup--p-strong">very short period of time </strong>compared to training. It’s a great technique when you can’t or don’t want to spend time and money (re)training complex models on large data sets. Just make sure you understand how the model was trained (data set, data format, etc.) in order to retrain it appropriately.</p><p id="c749">That’s it for today. Thank you for reading.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="9181"><em class="markup--p-em">No animals were harmed during the writing of this article, but there sure was a whole lot of creative swearing and keyboard smashing during the coding phase.</em></p></div></div></section>
</section>
</article></body></html>
