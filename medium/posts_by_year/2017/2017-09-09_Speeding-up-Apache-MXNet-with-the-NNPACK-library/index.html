<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Speeding up Apache MXNet with the NNPACK library</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="5d25">Speeding up Apache MXNet with the NNPACK library</h3><p id="ad39"><a href="http://mxnet.io" target="_blank">Apache MXNet</a> is an Open Source library helping developers to build, train and re-use Deep Learning networks. In this article, I’ll show you to speed up predictions thanks to the <a href="https://github.com/Maratyszcza/NNPACK" target="_blank">NNPACK</a> library. Before we dive in, let’s first discuss why we would want to do this.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="11cf"><strong class="markup--blockquote-strong">The original post was based on MXNet 0.11. It was updated on April 14th, 2018 and it will now let you successfully build MXNet 1.1 with NNPACK. For more information on why this isn’t as easy as it should be: </strong><a class="markup--blockquote-anchor" href="https://github.com/Maratyszcza/NNPACK/issues/135" rel="nofollow noopener" target="_blank">https://github.com/Maratyszcza/NNPACK/issues/135</a> + <a class="markup--blockquote-anchor" href="https://github.com/apache/incubator-mxnet/pull/9860" rel="nofollow noopener" target="_blank">https://github.com/apache/incubator-mxnet/pull/9860</a></blockquote><figure class="graf-after--blockquote" id="5a38"><img class="graf-image" src="image02.webp"/><figcaption>Speed… what else?</figcaption></figure><h4 id="6c4b">Training</h4><p id="0833">Training is the step where a neural network learns how to correctly predict the right output for each sample in the data set. One batch at a time (typically from 32 to 256 samples), the data set is fed into the network, which then proceeds to minimise total error by adjusting weights (and sometimes hyper parameters) thanks to the <a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank">backpropagation</a> algorithm.</p><p id="8df0">Going through the full data set is called an “epoch”: large networks may be trained for hundreds of epochs to reach the highest accuracy possible. This may take days or even weeks, which is where GPUs step in: thanks to their formidable parallel processing power, training time can be significantly cut down, compared to even the most powerful of CPUs.</p><h4 id="db31">Inference</h4><p id="99be">Inference is the step where you actually use the trained network to predict a result from new data samples. You could be predicting with one sample at a time, for example trying to identify objects in a single picture like <a href="https://aws.amazon.com/rekognition/" target="_blank">Amazon Rekognition</a> does, or with multiple samples, for example trying to track a moving object in a video stream.</p><p id="4f3c">Of course, GPUs are equally efficient at inference. However, many systems are not able to accommodate them because of cost, power consumption or form factor constraints (just think of embedded systems).</p><blockquote class="graf--blockquote" id="6139">Being able to run fast CPU-based inference is an important topic.</blockquote><p class="graf-after--blockquote" id="e2f7">This is where the NNPACK library comes into play, as it will help us speed up CPU inference in Apache MXNet.</p><h4 id="069f">The NNPACK Library</h4><p id="3a36">NNPACK is an Open Source library available on <a href="https://github.com/Maratyszcza/NNPACK" target="_blank">Github</a>. It implements operations like matrix multiplication, convolution and pooling in a highly optimised fashion.</p><blockquote class="graf--blockquote" id="2212">These operations are at the core of neural networks, in particular of <a class="markup--blockquote-anchor" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank">Convolution Neural Networks</a> which are predominantly used to detect features in an image.</blockquote><p class="graf-after--blockquote" id="2f25">If you’re curious about the theory and math that help make these operations very fast, please refer to the research papers mentioned by the author in this <a href="https://www.reddit.com/r/MachineLearning/comments/4bswi6/nnpack_acceleration_package_for_neural_networks/" target="_blank">Reddit post</a>.</p><p id="9ad8">NNPACK is available for Linux and MacOS X platforms. It’s optimised for the Intel x64 processor with the AVX2 instruction set, as well as the ARMv7 processor with the NEON instruction set and the ARM v8.</p><p id="05d2">In this post, I will use a c4.8xlarge instance running the <a href="https://aws.amazon.com/marketplace/pp/B06VSPXKDX" target="_blank">Deep Learning AMI</a>, which already includes some of the dependencies we need. Here’s what we going to do:</p><ul class="postList"><li id="3772">Build the NNPACK library from source,</li><li id="d7ba">Build the cpuinfo library from source,</li><li id="46a6">Build Apache MXNet from source with NNPACK and cpuinfo,</li><li id="242f">Run some image classification benchmarks using a variety of networks.</li></ul><p id="14dc">Let’s get to work!</p><h4 id="0430">Building NNPACK</h4><p id="626c">NNPACK uses the <a href="http://ninja-build.org/" target="_blank">ninja</a> build tool. Unfortunately, the Ubuntu repository does not host the latest version, so we need to build it from source as well.</p><figure id="4fae"><script src="https://gist.github.com/juliensimon/767105398d5665ff361197ee6dbdc3c3.js"></script></figure><p id="711d">Now let’s prepare the NNPACK build, as per <a href="https://github.com/Maratyszcza/NNPACK" target="_blank">instructions</a>.</p><figure id="f98f"><script src="https://gist.github.com/juliensimon/b048e1058042787ec726f929bf6dd2ea.js"></script></figure><p id="91f7">Before we actually build, we need to tweak the configuration file a bit. The reason for this is that NNPACK only builds as a static library whereas MXNET builds as a dynamic library: thus, they won’t link properly. The MXNet documentation suggests to use an older version of NNPACK, but there’s another way ;)</p><p id="bc09">We need to edit the build.ninja file and the ‘-fPIC’ flag, in order to build C and C++ files as position-independent code, which is really all we need to link with the MXNet shared library.</p><figure id="d6ac"><script src="https://gist.github.com/juliensimon/f790f6de6bc70243e37d5950d2036d9b.js"></script></figure><p id="9224">Now, let’s build NNPACK and run some basic tests.</p><figure id="fc2b"><script src="https://gist.github.com/juliensimon/73b93853a364528d3ef4747e572b2085.js"></script></figure><p id="64e3">We’re done with NNPACK: you should see the library in <strong class="markup--p-strong">~/NNPACK/lib</strong>.</p><h4 id="e6de">Building cpuinfo</h4><p id="6dd1">NNPACK relies on this library to accurately detect CPU information. Build instructions are very similar.</p><figure id="82ab"><script src="https://gist.github.com/juliensimon/634678be884528588895f82c74545570.js"></script></figure><p id="9443">Then, we need to edit build.ninja and update <em class="markup--p-em">cflags</em> and <em class="markup--p-em">cxxflags</em> again.</p><figure id="1f68"><script src="https://gist.github.com/juliensimon/f790f6de6bc70243e37d5950d2036d9b.js"></script></figure><p id="d87a">Now, let’s build cpuinfo and run some basic tests.</p><figure id="4171"><script src="https://gist.github.com/juliensimon/73b93853a364528d3ef4747e572b2085.js"></script></figure><p id="d615">We’re also done with cpuinfo: you should see the library in <strong class="markup--p-strong">~/cpuinfo/lib</strong>.</p><h4 id="ef30">Building Apache MXNet with NNPACK and cpuinfo</h4><p id="c1e9">First, let’s install dependencies as well as the latest MXNet sources (1.1 at the time of writing). Detailed build instructions are available on the <a href="https://mxnet.incubator.apache.org/get_started/install.html" target="_blank">MXNet website</a>.</p><figure id="64ed"><script src="https://gist.github.com/juliensimon/7722360217dd7afecd9e1db1e09d6a2e.js"></script></figure><p id="a30b">Now, we need to configure the MXNet build. You should edit the make/config.mk file and set the variables below in order to include NNPACK in the build, as well as the dependencies we installed earlier.</p><figure id="b67a"><script src="https://gist.github.com/juliensimon/a0902ab0babefe598c7a57ad8a1baaf5.js"></script></figure><p id="5068">Now, we’re ready to build MXNet. Our instance has 36 vCPUs, so let’s put them to good use.</p><figure id="90dc"><script src="https://gist.github.com/juliensimon/9a70d08b26b766c3a4217dbbdad14228.js"></script></figure><p id="220e">A few minutes later, the build is complete. Let’s install our new MXNet library and its Python bindings.</p><figure id="60ce"><script src="https://gist.github.com/juliensimon/4042f8836af3780f11a2794f396b55d2.js"></script></figure><p id="5019">We can quickly check that we have the proper version by importing MXNet in Python.</p><figure id="78c2"><script src="https://gist.github.com/juliensimon/1be7ffb73e9aebf192df3a244a2b4a18.js"></script></figure><p id="e758">We’re all set. Time to run some benchmarks.</p><h3 id="fe10">Benchmarks</h3><p id="6566">Benchmarking with a couple of images isn’t going to give us a reliable view on whether NNPACK makes a difference. Fortunately, the MXNet sources include a benchmarking script which feeds randomly generated images in a variety of batch sizes through the following models: <strong class="markup--p-strong">Alexnet, VGG16, Inception-BN, Inception v3, Resnet-50 and Resnet-152</strong>.</p><blockquote class="graf--blockquote" id="9764">April 14th, 2018: these numbers have not been updated. They’re still the old MXNet 0.11 numbers.</blockquote><p class="graf-after--blockquote" id="1224">As nothing is ever simple, we need to fix a line of code in the script. C4 instances don’t have any GPU installed (which is the whole point here) and the script is unable to properly detect that fact. Here’s the modification you need to apply to<em class="markup--p-em">~/incubator-mxnet/example/image-classification/benchmark_score.py</em>. While we’re at it, let’s add additional batch sizes.</p><figure id="e84c"><script src="https://gist.github.com/juliensimon/1f95775151558a7592cf58d84f9e599b.js"></script></figure><p id="dd4e">Time to run some benchmarks. Let’s give 8 threads to NNPACK, which is the largest recommended value.</p><figure id="a181"><script src="https://gist.github.com/juliensimon/3e2c3c54c5763b9b0fcc52e9ee8ad575.js"></script></figure><p id="b89f">Full results are available <a href="https://gist.github.com/juliensimon/9aa7868e8f0fb98ef5e4a05ea03d1b06" target="_blank">here</a>. As a reference, I also ran the same script on an identical instance running the vanilla 0.11.0-rc3 MXNet (full results are available <a href="https://gist.github.com/juliensimon/c3394ca697e5b8692a1956027afd34d1" target="_blank">here</a>).</p><figure id="82c8"><img class="graf-image" src="image01.webp"/><figcaption>Number of images per second vs. batch size</figcaption></figure><p id="0e85">As we can see on the graph above, <strong class="markup--p-strong">NNPACK delivers a significant speedup for Alexnet (up to 2x), VGG (up to 3x) and Inception-BN (almost 2x).</strong></p><p id="16c3">For reasons beyond the scope of this article, NNPACK doesn’t (yet?) deliver any speedup for Inception v3 and Resnet.</p><h4 id="bc4c">Conclusion</h4><p id="580f">When GPU inference is not available, adding NNPACK to Apache MXNet may be an easy option to extract more performance from your network.</p><blockquote class="graf--blockquote" id="f973">As always, your mileage may vary and you should always run your own tests.</blockquote><p class="graf-after--blockquote" id="de18">It would definitely be interesting to run the same benchmark on the upcoming <a href="https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/" target="_blank">c5 instances</a> (based on the latest Intel Skylake architecture) and on a Raspberry Pi. More articles to be written :)</p><p id="50a6">Thanks for reading!</p></div></div></section>
</section>
</article></body></html>