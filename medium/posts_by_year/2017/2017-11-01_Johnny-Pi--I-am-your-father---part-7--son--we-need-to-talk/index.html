<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Johnny Pi, I am your father — part 7: son, we need to talk</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="f294">Johnny Pi, I am your father — part 7: son, we need to talk</h3><p id="e96d"><a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74" target="_blank">Previously</a>, we used an <a href="https://aws.amazon.com/iotbutton/" target="_blank">AWS IoT button</a> to trigger object detection on our robot. However, this is a pretty poor way to interact. Wouldn’t it be just simpler and more fun to <strong class="markup--p-strong">speak</strong> to it? Of course!</p><p id="15f9">In this post, I’ll walk you through an <a href="https://developer.amazon.com/alexa-skills-kit" target="_blank">Alexa skill</a> that I wrote to send voice commands to the robot.</p><figure id="008c"><img class="graf-image" src="image01.webp"/></figure><h4 id="4042">Why not Alexa on the Pi?</h4><p id="03e9">You can definitely set up Alexa on the Pi itself: the <a href="https://github.com/alexa-pi/AlexaPi" target="_blank"><strong class="markup--p-strong">AlexaPi</strong></a> project lets you connect a Pi to the <a href="https://developer.amazon.com/alexa-voice-service" target="_blank">Alexa Voice Service</a> (AVS) and process voice commands, provided that you connected a microphone on one of the USB ports (duh).</p><p id="c2bf">The setup process is a little hairy, but I did get things to work. Here’s a quick demo.</p><figure id="7611"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/nPc1PiG72GY?feature=oembed" width="700"></iframe></figure><p id="3302">However, the overall results were pretty poor because my cheap USB microphone is absolute crap!</p><blockquote class="graf--blockquote" id="2763">As a side note, this made me appreciate how amazingly good the Amazon Echo microphones are.</blockquote><p class="graf-after--blockquote" id="1c39">Anyway, even with a decent mike, the robot obviously needs to be close enough to hear you… and I wanted to be able to talk to a remote robot as well :) For these reasons, I decided not to go the AlexaPi way. Instead, I opted for an Amazon Echo device with a custom skill.</p><h4 id="cf99">Designing the skill</h4><p id="6d81">As you probably know, an <strong class="markup--p-strong">Alexa skill</strong> requires two components:</p><ul class="postList"><li id="7c0a">An <a href="https://developer.amazon.com/docs/custom-skills/custom-interaction-model-reference.html" target="_blank"><strong class="markup--li-strong">interaction model</strong></a>, where you will focus on the conversation itself (utterances, intents, slots).</li><li id="6825">An <a href="https://aws.amazon.com/lambda/" target="_blank"><strong class="markup--li-strong">AWS Lambda</strong></a> function, which will be invoked during the conversation to maintain the session, perform input validation, trigger actions, etc.</li></ul><p id="e394">Here, the interaction model is pretty simple. I’ll either ask the robot to <strong class="markup--p-strong">move around</strong> or to <strong class="markup--p-strong">look at objects or faces</strong>. Thus, we’ll need to define two <strong class="markup--p-strong">intents</strong>: <strong class="markup--p-strong">DirectionIntent</strong> and <strong class="markup--p-strong">SeeIntent</strong>.</p><p id="31e2">As far as the Lambda function is concerned, it will use <a href="https://aws.amazon.com/iot" target="_blank">AWS IoT</a> to send the proper MQTT messages to the robot, just like we’ve done with the joystick and the IoT button. It worked well so far and if it ain’t broken, I sure ain’t gonna try to fix it :*)</p><h4 id="5857">Interaction model: defining custom slots</h4><p id="b9cb">The movement commands we’d like to send the robot are what you’d expect: right, left, forward, backward, hold, slower, faster. Let’s define a <strong class="markup--p-strong">custom slot</strong>, called <strong class="markup--p-strong">{Direction}</strong>, with all these values.</p><p id="4a6d">We’ll also ask the robot to look at objects and faces. For this purpose, let’s create a second <strong class="markup--p-strong">custom slot</strong>, called <strong class="markup--p-strong">{Target}</strong>, holding two possible values: ‘object’ and ‘faces’.</p><figure id="c1b1"><img class="graf-image" src="image02.webp"/></figure><p id="4b85">Once you’re done, this is how your intent schema should look.</p><figure id="ded0"><script src="https://gist.github.com/juliensimon/33e94bb0a7a18bfb669662e09f3cf8db.js"></script></figure><h4 id="7f04">Interaction model: defining utterances for DirectionIntent</h4><p id="37e8">Then, we need to come up with a number of different <strong class="markup--p-strong">utterances</strong> that we’re likely to use. Here are some examples:</p><pre class="graf--pre" id="3204">DirectionIntent move {Direction}<br/>DirectionIntent go {Direction}<br/>DirectionIntent turn {Direction}<br/>DirectionIntent now move {Direction}<br/>DirectionIntent now go {Direction}<br/>DirectionIntent now turn {Direction}<br/>DirectionIntent just go {Direction}<br/>DirectionIntent just move {Direction}<br/>DirectionIntent just turn {Direction}<br/>DirectionIntent I want you to go {Direction}<br/>DirectionIntent I want you to move {Direction}<br/>DirectionIntent I want you to turn {Direction}</pre><p class="graf-after--pre" id="e3cf">Some combinations do sound a little weird, such as “just turn forward”, but nothing that would really require us to create different intents for left/right and forward/backward.</p><h4 id="b091">Interaction model: defining utterances for SeeIntent</h4><p id="7c63">Again, we have to create a <strong class="markup--p-strong">custom slot</strong>, called <strong class="markup--p-strong">{Target}</strong>, holding two possible values: ‘object’ and ‘faces’. Here are some of the utterances:</p><pre class="graf--pre" id="86b9">SeeIntent Look at the {Target}<br/>SeeIntent Take a look at the {Target}<br/>SeeIntent Just look at the {Target}<br/>SeeIntent Tell me about the {Target} you see<br/>SeeIntent Tell me about the {Target} in front of you<br/>SeeIntent What is the {Target} in front of you<br/>SeeIntent Do you see the {Target} in front of you<br/>SeeIntent Do you see an {Target}<br/>SeeIntent Do you see {Target}<br/>SeeIntent How many {Target} do you see<br/>SeeIntent Describe the {Target} you see</pre><p class="graf-after--pre" id="f0f8">I’m sure you’ll have additional ideas, just add them to your list.</p><h4 id="60f3">Implementing the Lambda function</h4><p id="fefb">We’re finished with the interaction model. Let’s now take care of the <strong class="markup--p-strong">Lambda function</strong> which will perform the actual processing.</p><p id="cd15">Starting from a vanilla skill and customizing is the simplest way to get this done. Thus, I will only highlight the parts that are specific to this skill.</p><h4 id="d620">Lambda function: connecting to AWS IoT</h4><p id="498d">First, we need to create AWS <strong class="markup--p-strong">IoT credentials</strong> for the skill (certificate, key pair, IAM policy). We’ve done this many times before and <a href="https://docs.aws.amazon.com/iot/latest/developerguide/iot-gs.html" target="_blank">the process</a> is always the same: just repeat the same steps and download all credentials on your local machine.</p><p id="e0df">Here’s the <strong class="markup--p-strong">IAM policy</strong> you should attach to the skill thing in AWS IoT and to the Lambda function itself.</p><figure id="df13"><script src="https://gist.github.com/juliensimon/5e6eb17ca2884a02e473d2856f1f7e3a.js"></script></figure><p id="1431">Time to write the function itself. First, we have import the <strong class="markup--p-strong">AWS IoT SDK</strong> and the <strong class="markup--p-strong">IoT credentials</strong>. Let’s also define a couple of helper functions.</p><figure id="1da6"><script src="https://gist.github.com/juliensimon/0a611aebf587a75d162d5c1f693b497d.js"></script></figure><h4 id="5d39">Lambda function: defining messages</h4><p id="6c56">This is pretty easy :) Just change the messages in the vanilla skill functions.</p><figure id="a824"><script src="https://gist.github.com/juliensimon/7e88b5ba7625dd266521e798c9e613ab.js"></script></figure><h4 id="8852">Lambda function: handling DirectionIntent</h4><p id="4ed7">Nothing really complicated here:</p><ul class="postList"><li id="ae1a">Check that the <strong class="markup--li-strong">Direction</strong> slot is present and valid,</li><li id="a7aa">Connect to AWS IoT,</li><li id="c075">Publish an <strong class="markup--li-strong">MQTT message</strong> containing the direction in the ‘JohnnyPi/move’ topic,</li><li id="b3ea">Disconnect from AWS IoT.</li></ul><p id="7a76">In this context, I found that it was more reliable to connect and disconnect every time. Not sure why, although the latency is hardly noticeable. Feel free to try something different.</p><p id="dfde">There’s a tiny hack in this code that I was too lazy to fix. We can’t use the word ‘stop’, as it’s reserved by Alexa to stop the skill. That’s why I use ‘hold’ instead: although I’m actually sending ‘stop’ to the robot as this is the command I’ve implemented there ;)</p><figure id="7ef2"><script src="https://gist.github.com/juliensimon/283442847e16894b0f05e697bb9031e9.js"></script></figure><h4 id="ac6c">Lambda function: handling SeeIntent</h4><p id="63b5">The handler function for SeeIntent has a similar structure, but it is a little more complex. Indeed, we need to handle two different cases:</p><ul class="postList"><li id="855f">looking at objects: we’ll perform a call to <a href="https://aws.amazon.com/rekognition/" target="_blank"><strong class="markup--li-strong">Amazon Rekognition</strong></a>.</li><li id="e4be">looking at faces: we’ll use a local <a href="http://mxnet.incubator.apache.org/" target="_blank"><strong class="markup--li-strong">Apache MXNet</strong></a><strong class="markup--li-strong"> model</strong>.</li></ul><p id="5bc2">In both cases, we’re also instruction the robot to tweet the picture. You can see past pictures <a href="https://twitter.com/@callmejohnnypi" target="_blank">here</a>.</p><figure id="8366"><script src="https://gist.github.com/juliensimon/f46c57873051d88792b8ae1b2be6de12.js"></script></figure><h4 id="1609">Lambda function: dispatching the intents</h4><p id="dab7">The last bit we need to implement is the <strong class="markup--p-strong">intent dispatcher</strong>. Pretty straightforward.</p><figure id="86aa"><script src="https://gist.github.com/juliensimon/449c2e84461bed9032d7cdeab5ff2ad3.js"></script></figure><p id="669d">We’re now done with the Lambda function. Time to <strong class="markup--p-strong">package</strong> and <strong class="markup--p-strong">deploy</strong> it.</p><h4 id="25e9">Lambda function: packaging and deploying</h4><p id="fd0b">Three actions are required:</p><ul class="postList"><li id="d532">install the AWS IoT SDK,</li><li id="41cf">create a ZIP file holding the function and the SDK,</li><li id="fdfc">create the function in AWS Lambda.</li></ul><p id="cb4d">Here are the corresponding commands, which you need to run in the directory holding the function code.</p><pre class="graf--pre" id="af3d">$ pip install  AWSIoTPythonSDK -t .</pre><pre class="graf--pre graf-after--pre" id="62ea">$ ls -p<br/>AWSIoTPythonSDK/<br/>AWSIoTPythonSDK-1.2.0.dist-info/                 <br/>certs/                           <br/>lambda.py<br/>iot_config.py</pre><pre class="graf--pre graf-after--pre" id="95bf">$ zip -9 lambda.zip .</pre><pre class="graf--pre graf-after--pre" id="d206">$ aws lambda create-function --function-name alexaJohnnyPi \<br/>--handler lambda.lambda_handler --zip-file fileb://lambda.zip \<br/>--runtime python2.7 --memory-size 128 --region REGION_NAME \<br/>--role arn:aws:iam::ACCOUNT_NAME:role/ROLE_NAME</pre><h4 class="graf-after--pre" id="278e">Demo time!</h4><p id="5b6a">All right, all the pieces are in place, let’s test them. Here’s a recent video from the AWS Loft in London. My session was an introduction to Deep Learning and the actual demo starts at the 59:30 mark.</p><figure id="4d29"><iframe frameborder="0" height="281" scrolling="no" src="https://player.twitch.tv/?video=v176362879&amp;!branding&amp;autoplay=false" width="500"></iframe></figure><p id="516e">Pretty cool, don’t you think? As usual, you’ll find all code on <a href="https://github.com/juliensimon/johnnypi/tree/master/part7" target="_blank">Github</a>.</p><p id="3f93">That’s it for today. In the next and possibly last part of this story, we’ll have the robot send text strings back to the Alexa skill instead of speaking through Amazon Polly. Until then, thank you for reading and keep building.</p><p id="b5aa">Part 0: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36" target="_blank">a sneak preview</a></p><p id="4525">Part 1: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">moving around</a></p><p id="58a9">Part 2: <a href="https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">the joystick</a></p><p id="a175">Part 3: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76" target="_blank">cloud-based speech</a></p><p id="7aa6">Part 4: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" target="_blank">cloud-based vision</a></p><p id="8772">Part 5: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113" target="_blank">local vision</a></p><p id="7ba1">Part 6: <a href="https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74" target="_blank">the IoT button</a></p></div></div></section><section class="section"><div><hr/></div><div><div><p id="66a3">This post was written while listening to this <a href="https://open.spotify.com/album/09ZD2k6FieGjzoVGrQdsAC" target="_blank">Gamma Ray best-of</a>. German Power Metal be blessed \m/</p></div></div></section>
</section>
</article></body></html>
