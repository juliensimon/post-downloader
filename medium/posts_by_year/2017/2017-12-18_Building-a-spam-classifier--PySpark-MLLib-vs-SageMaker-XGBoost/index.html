<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Building a spam classifier: PySpark+MLLib vs SageMaker+XGBoost</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="9afb">Building a spam classifier: PySpark+MLLib vs SageMaker+XGBoost</h3><p id="2b08">In this article, I will first show you how to build a <strong class="markup--p-strong">spam classifier</strong> using <a href="https://spark.apache.org/" target="_blank"><strong class="markup--p-strong">Apache Spark</strong></a>, its Python API (aka PySpark) and a variety of Machine Learning algorithms implemented in <a href="https://spark.apache.org/mllib/" target="_blank"><strong class="markup--p-strong">Spark MLLib</strong></a>.</p><p id="bee9">Then, we will use the new <a href="https://aws.amazon.com/blogs/aws/sagemaker/" target="_blank"><strong class="markup--p-strong">Amazon Sagemaker</strong></a> service to train, save and deploy an <strong class="markup--p-strong">XGBoost</strong> model trained on the same data set.</p><figure id="6348"><img class="graf-image" src="image01.webp"/><figcaption>“I must break you”</figcaption></figure><p id="60da">All code runs in a Jupyter notebook, available on <a href="https://github.com/juliensimon/dlnotebooks/tree/master/spark" target="_blank">Github</a> :)</p></div></div></section><section class="section"><div><hr/></div><div><div><h3 id="bf39">PySpark + MLLib</h3><h4 id="bca4">The big picture</h4><p id="849e">Our raw data set is composed of <strong class="markup--p-strong">1-line messages</strong> stored in <strong class="markup--p-strong">two files</strong>:</p><ul class="postList"><li id="39c2">the ‘<a href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/ham" target="_blank">ham</a>’ file: 4827 valid messages,</li><li id="023b">the ‘<a href="https://github.com/juliensimon/dlnotebooks/blob/master/spark/spam" target="_blank">spam</a>’ file: 747 messages.</li></ul><p id="cbd6">In order to classify these messages, we need to build an intermediate data set with <strong class="markup--p-strong">two classes</strong>. For this purpose, we’re going to use a simple but efficient technique called <a href="https://en.wikipedia.org/wiki/Feature_hashing" target="_blank"><strong class="markup--p-strong">Feature Hashing</strong></a>:</p><ul class="postList"><li id="b7b7">For each message in the data set, we first <strong class="markup--li-strong">hash</strong> its words into a <strong class="markup--li-strong">fixed</strong> number of buckets (say, 1000).</li><li id="b08b">Then, we build a <strong class="markup--li-strong">vector</strong> indicating non-zero occurrences for each word: these are the <strong class="markup--li-strong">features</strong> that will be used to decide whether a message is spam or not.</li><li id="6b8e">For a valid message, the corresponding <strong class="markup--li-strong">label</strong> will be zero, i.e. the message is not spam. Accordingly, for a spam message, the label will be one.</li></ul><p id="32ce">Once we’re done, our intermediate data set will be:</p><ul class="postList"><li id="0985"><strong class="markup--li-strong">4827 word vectors labeled with a zero</strong>,</li><li id="ebeb"><strong class="markup--li-strong">747 word vectors labeled with a one</strong>.</li></ul><p id="ed68">We’ll split it <strong class="markup--p-strong">80/20</strong> for training and validation and run in through a number of <strong class="markup--p-strong">classification</strong> algorithms.</p><p id="e0cb">For prediction, the process will be similar: hash the message, send the word vector to the model and get the predicted result.</p><p id="d1d6">Not that difficult, hey? Let’s get to work!</p><h4 id="ed4c">Building the intermediate data set</h4><p id="4a01">Our first step is to load both files and split the messages into <strong class="markup--p-strong">words</strong>.</p><figure id="e5d5"><script src="https://gist.github.com/juliensimon/4e28509ca2a5a7501b8bdf537abe49bb.js"></script></figure><p id="5119">Then, we’re hashing each message into 1,000 <strong class="markup--p-strong">word buckets</strong>. As you can see, each message is turned into a <strong class="markup--p-strong">sparse vector</strong> holding bucket numbers and occurrences.</p><figure id="fb68"><script src="https://gist.github.com/juliensimon/8727c4e5798c1228a78476e3a8e17bf8.js"></script></figure><p id="65f4">The next step is to <strong class="markup--p-strong">label</strong> our features: 1 for spam, 0 for non-spam. The result is a collected of <strong class="markup--p-strong">labeled samples</strong> which are ready for use.</p><figure id="ce12"><script src="https://gist.github.com/juliensimon/7d4743dbf913c1760cbc785b8c536937.js"></script></figure><p id="46be">Finally, we split the data set 80/20 for <strong class="markup--p-strong">training</strong> and <strong class="markup--p-strong">test</strong> and cache both RDDs as we will use them repeatedly.</p><figure id="4516"><script src="https://gist.github.com/juliensimon/1722c3663f793f33b90224dfceb61795.js"></script></figure><p id="992d">Now we’re going to train a number of models with this data set. To measure their accuracy, here’s the <strong class="markup--p-strong">scoring function</strong> we’re going to use: simply predict all samples in the test set, compare the predicted label with the real label and compute accuracy.</p><figure id="bab5"><script src="https://gist.github.com/juliensimon/46e5dc9b688e64c799b3054a4baaa380.js"></script></figure><h4 id="3d3d">Classifying the data set with Spark MLLib</h4><p id="4d45">We’re going to use the following classification algorithms:</p><ul class="postList"><li id="e73e"><a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank">Logistic regression</a> with the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">SGD</a> optimizer,</li><li id="5070">Logistic regression with the <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank">LBFGS</a> optimizer,</li><li id="b2e7"><a href="https://en.wikipedia.org/wiki/Support_vector_machine" target="_blank">Support Vector Machines</a>,</li><li id="c0ed"><a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank">Decision Trees</a>,</li><li id="82d3"><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting" target="_blank">Gradient Boosted Trees</a>,</li><li id="2a96"><a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank">Random Forests</a>,</li><li id="65eb"><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank">Naive Bayes</a>.</li></ul><h4 id="8933">Logistic Regression</h4><p id="d4fb">Let’s start with Logistic Regression, the mother of all classifiers.</p><figure id="71c0"><script src="https://gist.github.com/juliensimon/5896647f67994781c2cffa1ec0d5a792.js"></script></figure><h4 id="e2be">Support Vector Machines</h4><p id="b173">What about SVMs, another popular algorithm?</p><figure id="412b"><script src="https://gist.github.com/juliensimon/8f18b17cd9011a0cf3768bd0a33c37f6.js"></script></figure><h4 id="309b">Trees</h4><p id="505d">Now let’s try three variants of tree-based classification. The API is slightly different from previous algos.</p><figure id="9197"><script src="https://gist.github.com/juliensimon/f31b69c7c91ebbb4177032e3b9d1db9e.js"></script></figure><h4 id="5840">Naive Bayes</h4><p id="7833">Last but not least, let’s try the Naives Bayes classifier.</p><figure id="0a79"><script src="https://gist.github.com/juliensimon/313fbcce4ca5d215779b818a17b5c34d.js"></script></figure><p id="80ac">It is vastly superior to all other algos. Let’s try to predict a couple of real-life samples.</p><figure id="17ae"><script src="https://gist.github.com/juliensimon/b679cfc760668571f464fe2efbb4e684.js"></script></figure><p id="f179">They were predicted correctly. This looks like a pretty good model. Now why don’t try to improve these scores? I’ve used default parameters for most of the algorithms, surely there is room for improvement :) You’ll find links to all APIs in the notebook, so feel free to tweak away!</p><h4 id="9591">This is great, but…</h4><p id="aade">So far, we’ve only worked locally. This raises some questions:</p><ol class="postList"><li id="8143">how would we train on a <strong class="markup--li-strong">much larger data set</strong>?</li><li id="5c0e">how would we deploy our model to <strong class="markup--li-strong">production</strong>?</li><li id="bf32">how could we know if our model would <strong class="markup--li-strong">scale</strong>?</li></ol><p id="7fb2">These questions — scalability and deployment — are often the <strong class="markup--p-strong">bane</strong> of Machine Learning projects. Going from “it works on my machine” to “it works in production at scale 24/7” usually requires <strong class="markup--p-strong">a lot</strong> of work.</p><p id="dbe8">There is hope. Read on :)</p></div></div></section><section class="section"><div><hr/></div><div><div><h3 id="266c">SageMaker + XGBoost</h3><p id="5713"><strong class="markup--p-strong">Solving these pain points is at the core of </strong><a href="http://aws.amazon.com/sagemaker" target="_blank"><strong class="markup--p-strong">Amazon SageMaker</strong></a>. Let’s revisit our use case.</p><h4 id="2602"><strong class="markup--h4-strong">Built-in algorithms</strong></h4><p id="3b2a">As we saw previously, there are plenty of classification algorithms. Picking the “right” one and its “best” implementation (good luck trying to define “right” and “best”) is not an easy task. Fortunately, SageMaker provides you with several <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" target="_blank"><strong class="markup--p-strong">built-in algorithms</strong></a>. They have been implemented by Amazon, so I guess you can expect them to perform and scale correctly :)</p><blockquote class="graf--blockquote" id="d468">You can also bring your own code, your own pre-trained model, etc. To be discussed in future articles! More SageMaker examples on <a class="markup--blockquote-anchor" href="https://github.com/awslabs/amazon-sagemaker-examples" target="_blank">Github</a>: regression, multi-class classification, image classification, etc.</blockquote><p class="graf-after--blockquote" id="bb87">Here, we’re going to use <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html" target="_blank"><strong class="markup--p-strong">XGBoost</strong></a>, a popular implementation of Gradient Boosted Trees to build a binary classifier.</p><p id="d929">In a nutshell, the <a href="https://github.com/aws/sagemaker-python-sdk" target="_blank"><strong class="markup--p-strong">SageMaker SDK</strong></a> will let us:</p><ul class="postList"><li id="8d38">create managed infrastructure to <strong class="markup--li-strong">train</strong> XGBoost on our data set,</li><li id="722a"><strong class="markup--li-strong">store</strong> the model in SageMaker,</li><li id="ffb8"><strong class="markup--li-strong">configure</strong> a REST endpoint to serve our model,</li><li id="8e1d">create managed infrastructure to <strong class="markup--li-strong">deploy</strong> the model to the REST endpoint,</li><li id="1f4a"><strong class="markup--li-strong">invoke</strong> the model on a couple of samples.</li></ul><p id="a562">Let’s do this!</p><h4 id="4789">Setting up storage and data</h4><p id="7628">First things first: S3 will be used to store the data set and all artifacts (what a surprise). Let’s declare a few things, then. Hint: the S3 bucket <strong class="markup--p-strong">must</strong> be in the same region as SageMaker.</p><figure id="1cf2"><script src="https://gist.github.com/juliensimon/8757751b07f8aa49e809075b0a7290a5.js"></script></figure><p id="e52b">This implementation of XGBoost requires data to be either in <strong class="markup--p-strong">CSV</strong> or <strong class="markup--p-strong">libsvm</strong> format. Let’s try the latter, copy the resulting files to S3 and grab the SageMaker IAM role.</p><figure id="40c4"><script src="https://gist.github.com/juliensimon/ae3200662999b0b83a7fe048fe62cbfc.js"></script></figure><p id="07a5">Looking good. Now let’s set up the training job.</p><h4 id="42bb">Setting up the training job</h4><p id="c7d7">Amazon SageMaker uses Docker containers to run training jobs. We need to pick the container name corresponding to the region we’re running in.</p><figure id="2611"><script src="https://gist.github.com/juliensimon/847d92171270311f63bcd89374902e3e.js"></script></figure><p id="0720">Easy enough. Time to configure training. We’re going to:</p><ul class="postList"><li id="ea32"><strong class="markup--li-strong">Build a binary classifier,</strong></li><li id="ee15"><strong class="markup--li-strong">Fetch the training and validation data sets in libsvm format from S3,</strong></li><li id="3aff"><strong class="markup--li-strong">Train for 100 iterations a single m4.4xlarge instance.</strong></li></ul><figure id="02b7"><script src="https://gist.github.com/juliensimon/347efe35de1fdc5a965a376e2a91460c.js"></script></figure><p id="29f5">That’s quite a mouthful, but don’t panic:</p><ul class="postList"><li id="e7a5">Parameters common to all algorithms are defined in the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html" target="_blank">CreateTrainingJob API</a> documentation.</li><li id="8929">Algorithm-specific parameters are defined on the algorithm page, e.g. <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html" target="_blank">XGBoost</a>.</li></ul><h4 id="9647">Training and saving the model</h4><p id="7131">OK, let’s get this party going. Time to start training.</p><figure id="f276"><script src="https://gist.github.com/juliensimon/a9658a80d70a798957785894a1e32469.js"></script></figure><p id="5d15">6 minutes later, our model is ready. Of course, this is a bit long for such a small data set :) However, if we had millions of lines, <strong class="markup--p-strong">we could have started a</strong> <strong class="markup--p-strong">training job on multiple instances with the exact same code</strong>. Pretty cool, huh?</p><p id="ed3c">OK, let’s <strong class="markup--p-strong">save this model</strong> in SageMaker. Pretty straightforward with the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html" target="_blank">CreateModel API</a>.</p><figure id="fbdc"><script src="https://gist.github.com/juliensimon/7f2950edf4a3815ba6129507004c4a56.js"></script></figure><h4 id="13db">Creating the endpoint</h4><p id="3706">Here comes the <strong class="markup--p-strong">really</strong> good part. We’re going to deploy this model and invoke it. Yes, just like that.</p><p id="7451">First, we need to create an endpoint configuration with the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html" target="_blank">CreateEndpointConfig API</a>: we’ll use a <strong class="markup--p-strong">single m4.xlarge for inference</strong>, with <strong class="markup--p-strong">100% of traffic going to our model</strong> (we’ll look at A/B testing in a future post).</p><figure id="8340"><script src="https://gist.github.com/juliensimon/7edf42a19cdd4f827ad3fc3fb5358fe3.js"></script></figure><h4 id="7a5c">Deploying the model</h4><p id="3eff">Now we can deploy our trained model on this endpoint with the <a href="http://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html" target="_blank">CreateEndpoint API</a>.</p><figure id="e2ca"><script src="https://gist.github.com/juliensimon/540590eb5557250a7dc1d9f2c53e256e.js"></script></figure><h4 id="878d">Invoking the endpoint</h4><p id="baee">We’re now ready to invoke the endpoint. Let’s grab a couple of samples (in libsvm format) from the data set and predict them.</p><figure id="a88c"><script src="https://gist.github.com/juliensimon/329a976570464143314b9862168428f0.js"></script></figure><p id="a944">Both samples are predicted correctly. Woohoo.</p><p id="848c"><strong class="markup--p-strong">Conclusion</strong></p><p id="f8da">As you can see, SageMaker helps you run your Machine Learning projects end to end: notebook experimentation, model training, model hosting, model deployment.</p><p id="e8b2">If you’re curious about other ways you can use SageMaker (and if you can’t wait for the inevitable future posts!), here’s a overview I recorded recently.</p><figure id="d261"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/ym7NEYEx9x4?feature=oembed" width="700"></iframe></figure><p id="b76b">That’s it for today. Thank you very much for reading.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="573b"><em class="markup--p-em">This monster post was written while listening over and over (it WAS a long post) to this legendary Foreigner show from 1981.</em></p><figure id="04c0"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/yTkHV3hD010?feature=oembed" width="640"></iframe></figure></div></div></section>
</section>
</article></body></html>
