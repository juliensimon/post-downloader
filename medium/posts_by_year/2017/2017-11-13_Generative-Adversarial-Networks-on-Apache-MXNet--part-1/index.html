<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Generative Adversarial Networks on Apache MXNet, part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="cf59">Generative Adversarial Networks on Apache MXNet, part 1</h3><p id="65fe">In several previous posts, I’ve shown you how to classify images using a variety of Convolution Neural Networks. Using a <strong class="markup--p-strong">labeled training set</strong> and applying a <strong class="markup--p-strong">supervised learning</strong> process, AI delivers fantastic results on this problem and on similar ones, such as object detection or object segmentation.</p><p id="f6c5">Impressive as it is, this form of intelligence only deals with <strong class="markup--p-strong">understanding</strong> representations of our world as it is (text, images, etc). What about <strong class="markup--p-strong">inventing</strong> new representations? Could AI be able to generate <strong class="markup--p-strong">brand new images</strong>, convincing enough to fool the human eye? Well, yes.</p><p id="8a9d">In this post, we’ll start to explore how!</p><h4 id="3b8c">Generative Adversarial Networks</h4><p id="bfc1">A breakthrough happened in 2014, with the publication of “<a href="https://arxiv.org/abs/1406.2661" target="_blank"><em class="markup--p-em">Generative Adversarial Networks</em></a>”, by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio.</p><blockquote class="graf--blockquote" id="eca1">In my crystal ball, I see Ian Goodfellow winning a Turing Award for this. It might take 30 years, but mark my words.</blockquote><p class="graf-after--blockquote" id="99e1">Relying on an <strong class="markup--p-strong">unlabeled data set</strong> and an <strong class="markup--p-strong">unsupervised learning process</strong>, GANs are able to <strong class="markup--p-strong">generate new images</strong> (people, animals, landscapes, etc.) or even <strong class="markup--p-strong">alter parts of an existing image</strong> (like adding a smile to a person’s face).</p><h4 id="431f">An intuitive explanation</h4><p id="b3d6">The original Goodfellow article uses the <strong class="markup--p-strong">art forge</strong>r vs <strong class="markup--p-strong">art expert</strong> analogy, which has been rehashed to death on countless blogs. I’ll let you read the original version and I’ll try to use a different analogy: <strong class="markup--p-strong">cooking</strong>.</p><p id="1dda">You’re the <strong class="markup--p-strong">apprentice</strong> and I’m the <strong class="markup--p-strong">chef</strong> (obviously!). Your goal would to cook a really nice <a href="http://www.marmiton.org/recettes/recette_boeuf-bourguignon_18889.aspx" target="_blank">Boeuf Bourguignon</a>, but I wouldn’t give you <strong class="markup--p-strong">any</strong> instructions. No list of ingredients, no recipe, nothing. My only request would be: “<strong class="markup--p-strong">cook something with 20 ingredients</strong>”.</p><p id="4b62">You’d go the pantry, pick 20 <strong class="markup--p-strong">random</strong> ingredients, mix them together in a pot and show me the result. I’d look at it and of course the result would be nothing like what I expected</p><p id="a52f">For each of the ingredients you selected, I’d give you a <strong class="markup--p-strong">hint</strong> which would help you get a little bit closer to the actual recipe. For example, if you picked chicken, I could tell you: “Well, there is no chicken in this recipe but there is meat”. And if you used grape juice, I may say: “Hmm, the color is right but this is the wrong liquid” (red wine is required).</p><p id="417c">Resolved to improve, you’d go back to the pantry and try to make <strong class="markup--p-strong">slightly better choices</strong>. The result would still be far off, but a <strong class="markup--p-strong">little bit closer</strong> anyway. I’d give you more hints, you’d cook again and so on. After a number of <strong class="markup--p-strong">iterations</strong> (and a massive waste of food), chances are you’d get very close to the <strong class="markup--p-strong">actual recipe</strong> — assuming that I wouldn’t have lost my temper by then :D</p><h4 id="0b95">A (slightly) more scientific explanation</h4><p id="cecd">Let’s replace the apprentice by the <strong class="markup--p-strong">Generator</strong> and the chef by the <strong class="markup--p-strong">Discriminator</strong>. Here is how GANs work.</p><ol class="postList"><li id="94ed">The <strong class="markup--li-strong">Generator</strong> model has <strong class="markup--li-strong">no access to the data set</strong>. Using random data, it creates an image that is forwarded through the Detector model.</li><li id="83d9">The <strong class="markup--li-strong">Discriminator</strong> model learns how to recognise <strong class="markup--li-strong">valid</strong> data samples (the ones included in the data) from <strong class="markup--li-strong">invalid</strong> data samples (the ones computed by the Generator). The training process uses traditional techniques like gradient descent and back propagation.</li><li id="1921">The <strong class="markup--li-strong">Generator</strong> model also learns, but in a different way. First, it treats its samples as <strong class="markup--li-strong">valid</strong> (it’s trying to fool the Discriminator after all). Second, weights are updated using the <strong class="markup--li-strong">gradients computed by the Discriminator</strong>.</li><li id="04f1">Repeat!</li></ol><blockquote class="graf--blockquote" id="edf0">This is the key to understanding GANs: by treating its samples as valid and by applying the Discriminator weight updates, <strong class="markup--blockquote-strong">the Generator progressively learns how to generate data samples that are closer and closer to the ones that the Discriminator considers as valid</strong>, i.e. the ones that are part of the data set.</blockquote><p class="graf-after--blockquote" id="3b6f">Brilliant, brilliant idea (Turing award, I’m telling you).</p><h4 id="72dd">Deep Convolutional GANs</h4><p id="c8b3">GANs may be implemented using a number of different model architectures. Here, we will study a GAN based on <strong class="markup--p-strong">Convolutional Neural Networks</strong>, as published in “<a href="https://arxiv.org/abs/1511.06434" target="_blank"><em class="markup--p-em">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em></a>”, by Alec Radford, Luke Metz and Soumith Chintala (2016).</p><p id="ab84">Let’s take a look at the <strong class="markup--p-strong">Generator</strong> model. I’ve slightly updated the illustration included in the original article to reflect the exact code that we will use later on.</p><figure id="369c"><img class="graf-image" src="image02.webp"/><figcaption>The Generator network (data flows from left to right)</figcaption></figure><p id="5923">Don’t panic, it’s not as bad as you think. We start from a random vector of 100 values. Using 5 transposed convolution operations (more on this in a minute), this vector is turned into an RGB 64x64 image (hence the 3 channels).</p><p id="b615">Now let’s look at the <strong class="markup--p-strong">Discriminator</strong>. Wait, it’s almost identical (don’t forget to start from the left this time). Using 5 convolution operations, we turn an RGB 64x64 image into a probability: true for valid samples, false for invalid samples.</p><figure id="6f5f"><img class="graf-image" src="image07.webp"/><figcaption>The Discriminator network (data flows from right to left)</figcaption></figure><p id="8213">Still with me? Good. Now what about this convolution / transposed convolution thing?</p><h4 id="dec9">A look at convolution</h4><p id="0093">There are plenty of great tutorials out there. The best I’m aware of is part of the Theano documentation. Extremely detailed with <strong class="markup--p-strong">beautiful</strong> animations. Read it and words like “kernel”, “padding” and “stride” will become crystal clear.</p><div class="graf--mixtapeEmbed" id="b197"><a class="markup--mixtapeEmbed-anchor" href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" title="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html"><strong class="markup--mixtapeEmbed-strong">Convolution arithmetic tutorial - Theano 0.9.0 documentation</strong><br/><em class="markup--mixtapeEmbed-em">Learning to use convolutional neural networks (CNNs) for the first time is generally an intimidating experience. A…</em>deeplearning.net</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="389a4b19e8d31f54a7ce1bce73679fd1" data-thumbnail-img-id="0*K2OlEWy2aJRkBnnF." href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*K2OlEWy2aJRkBnnF.);"></a></div><p class="graf-after--mixtapeEmbed" id="8b28">In a nutshell, convolution is typically used to <strong class="markup--p-strong">reduce dimensions</strong>. This is why this operation is at the core of Convolutional (duh) Neural Networks: they start from a full image (say 224x224) and gradually <strong class="markup--p-strong">shrink</strong> it through a series of convolutions which will only preserve the features that are meaningful for classification.</p><p id="430e">The formula to compute the size of the <strong class="markup--p-strong">output image</strong> is actually quite simple.</p><figure id="4bef"><img class="graf-image" src="image06.webp"/><figcaption>i: input, o: output, k: kernel, p: padding, s: stride</figcaption></figure><p id="c53b">We can apply it to the Discriminator network above and yes, it works. Woohoo.</p><figure id="f509"><img class="graf-image" src="image04.webp"/></figure><h4 id="2f71">A look at transposed convolution</h4><p id="6a49">Transposed convolution is the reverse process, i.e. it <strong class="markup--p-strong">increases dimensions</strong>. Don’t call it “Deconvolution”, it seems to aggravate some people ;)</p><p id="7cf1">The formula to compute the size of the <strong class="markup--p-strong">output image</strong> is as follows.</p><figure id="01f4"><img class="graf-image" src="image05.webp"/><figcaption>i’: input, o’: output, k: kernel, p: padding, s: stride</figcaption></figure><p id="b11e">Applying it to the Generator network gives us the correct results too. Hopefully, this is starting to make sense and you now understand how it’s possible to generate a picture from a vector of random values :)</p><figure id="38f4"><img class="graf-image" src="image01.webp"/></figure><h4 id="05ca">Coding the Discriminator network</h4><p id="3b22">Apache MXNet has a couple of nice examples implementing this network architecture in R and Python. I’ll use Python for the rest of the post, but I’m sure R users will follow along.</p><div class="graf--mixtapeEmbed" id="2588"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" title="https://github.com/apache/incubator-mxnet/tree/master/example/gan"><strong class="markup--mixtapeEmbed-strong">apache/incubator-mxnet</strong><br/><em class="markup--mixtapeEmbed-em">incubator-mxnet — Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware…</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="a7a19f3155ed8677ae5dd1822f931430" data-thumbnail-img-id="0*13DYtb3j5zwVmwk_." href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*13DYtb3j5zwVmwk_.);"></a></div><p class="graf-after--mixtapeEmbed" id="c2d2">Here’s the code for the <strong class="markup--p-strong">Discriminator</strong> network, based on the illustration above. You’ll find extra details in the research article, e.g. why they use the LeakyRelu activation function and so on.</p><figure id="3cbf"><script src="https://gist.github.com/juliensimon/cdba4eb4359c6820862ce7b91565d30b.js"></script></figure><h4 id="53fa">Coding the Generator network</h4><p id="7ffc">Here’s the code for the <strong class="markup--p-strong">Generator</strong> network, based on the illustration above.</p><figure id="226b"><script src="https://gist.github.com/juliensimon/758e69a2805728d703a21090ff53dcba.js"></script></figure><h4 id="9f6b">Preparing MNIST</h4><p id="2140">OK, now let’s take care of the data set. As you probably know, the <strong class="markup--p-strong">MNIST</strong> data set contains 28x28 black and white images. We need to:</p><ul class="postList"><li id="3726"><strong class="markup--li-strong">reshape</strong> them to 64x64 images,</li><li id="c8f6"><strong class="markup--li-strong">normalize</strong> pixel values between 0 and 1,</li><li id="dcbe">add <strong class="markup--li-strong">two extra channels</strong> (identical to the original image),</li><li id="d3de">set 10,000 samples aside to <strong class="markup--li-strong">validate</strong> the Discriminator.</li></ul><p id="7400">Nothing MXNet-specific here, just good old Python data manipulation.</p><p id="e18e">During Discriminator training, this data set will be served by a standard <em class="markup--p-em">NDArray</em> <strong class="markup--p-strong">iterator</strong>.</p><figure id="5596"><script src="https://gist.github.com/juliensimon/5759a813f2a7cb7322d0627feb985b7c.js"></script></figure><h4 id="44a1">Preparing random data</h4><p id="beba">We also need to provide random data to the Generator. We’ll do this with a <strong class="markup--p-strong">custom iterator</strong>.</p><figure id="0e4d"><script src="https://gist.github.com/juliensimon/c3fec702b301cf17caaaeafa5d4783b0.js"></script></figure><p id="b737">When <em class="markup--p-em">getdata</em>() is called, this iterator will return an <em class="markup--p-em">NDArray</em> shaped <strong class="markup--p-strong">(batch size, random vector size, 1, 1)</strong>. We’ll use a 100-element random vector, so through multiple transposed convolutions, the Generator will indeed build a picture from a (100, 1, 1) sample.</p><h4 id="08cd">The training loop</h4><p id="7677">Time to look at the training code. This time, we cannot use the <em class="markup--p-em">Module.fit()</em> API. We have to write a <strong class="markup--p-strong">custom training loop</strong> taking into account the fact that we’re going to use the Discriminator gradients to update the Generator.</p><p id="5205">Here are the steps:</p><ol class="postList"><li id="792c">Generate a batch of <strong class="markup--li-strong">random samples</strong> (line 4).</li><li id="e3ed">Forward the batch through the <strong class="markup--li-strong">Generator</strong> and grab the resulting <strong class="markup--li-strong">images</strong> (lines 6–7).</li><li id="062a">Label these images as <strong class="markup--li-strong">fake</strong>, forward them through the <strong class="markup--li-strong">Discriminator</strong> and run back propagation (lines 10–12) : this lets the Discriminator learn to detect <strong class="markup--li-strong">fake</strong> images.</li><li id="77f5">Save the Discriminator <strong class="markup--li-strong">gradients</strong> but do not update the Discriminator weights at the moment (line 13).</li><li id="e070">Label the current <strong class="markup--li-strong">MNIST</strong> batch of images as <strong class="markup--li-strong">real</strong> images, forward them through the <strong class="markup--li-strong">Discriminator</strong> and run back propagation (lines 16–19) : this lets the Discriminator learn to detect <strong class="markup--li-strong">real</strong> images.</li><li id="2ad4">Add the “fake images” <strong class="markup--li-strong">gradients</strong> to the “real images” <strong class="markup--li-strong">gradients</strong> and update the <strong class="markup--li-strong">Discriminator</strong> <strong class="markup--li-strong">weights</strong> (lines 20–23).</li><li id="30a9">Label the Generator images as <strong class="markup--li-strong">real </strong>this time, forward them through the Discriminator again and run back propagation (lines 26–28).</li><li id="b66e">Get the Discriminator <strong class="markup--li-strong">gradients</strong>: they would normally help the <strong class="markup--li-strong">Discriminator</strong> learn how real images look like. However, we’re applying them to the <strong class="markup--li-strong">Generator</strong> network instead, effectively helping it to <strong class="markup--li-strong">forge</strong> better fake images (lines 29–31).</li></ol><figure id="a1ca"><script src="https://gist.github.com/juliensimon/b8fc358471fb9bead98c9e6b327d4e30.js"></script></figure><p id="9958">Quite a mouthful! Congratulations if you got this far: you understood the core concepts of GANs.</p><h4 id="f2c3">Let’s run this thing</h4><p id="f1b8">The <a href="https://github.com/apache/incubator-mxnet/tree/master/example/gan" target="_blank">MXNet sample</a> includes code to visualize the images coming out of the Generator . The simplest way to view them is to copy the code in a Jupyter notebook and run it :)</p><p id="cc21">After a few minutes (especially if you use a Volta-powered <a href="https://aws.amazon.com/about-aws/whats-new/2017/10/introducing-amazon-ec2-p3-instances/" target="_blank">p3 instance</a>), you should see something similar to this.</p><figure id="492a"><img class="graf-image" src="image03.webp"/></figure><p id="8f30">As you can see, random noise gradually turns into well-formed digits. It’s just math, but it’s still amazing…</p><blockquote class="graf--pullquote" id="5f53">In all chaos there is a cosmos, in all disorder a secret order— Carl Jung</blockquote><h4 class="graf-after--pullquote" id="6ba2">So when do we stop training?</h4><p id="8f81">Common training metrics like accuracy mean nothing here. We have no way of knowing whether Generator images are getting better… except by looking at them.</p><p id="4dba">An alternative would be to generate only fives (or any other digit), to run them through a <a href="https://medium.com@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62" target="_blank">proper MNIST classifier</a> and to measure accuracy.</p><p id="7103">There is also ongoing research to use <strong class="markup--p-strong">new metrics</strong> for GANs, such as the <a href="https://arxiv.org/abs/1701.07875" target="_blank">Wasserstein distance</a>. Let’s keep this topic for another article :)</p><p id="fcda">Thanks for reading. This is definitely a deeper dive than usual, but I hope you enjoyed it.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="635e"><em class="markup--p-em">Only one song is worthy here. Generator vs Discriminator, may the best model win!</em></p><figure id="0272"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/ty66q5RAL3E?feature=oembed" width="640"></iframe></figure></div></div></section>
</section>
</article></body></html>