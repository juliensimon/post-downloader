<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Exploring (ahem) AWS DeepLens</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="2b3f">Exploring (ahem) AWS DeepLens</h3><blockquote class="graf--blockquote graf--hasDropCapModel" id="eb87">Before you ask: everything in this post in based on publicly available information. No secrets, animals or Deep Learning hardware have been harmed during the writing process :-P</blockquote><p class="graf-after--blockquote" id="9f32"><a href="https://aws.amazon.com/deeplens/" target="_blank">AWS DeepLens</a> was one of the most surprising launches at at re:Invent 2017. Built on top of AWS services such as <a href="http://aws.amazon.com/lambda" target="_blank">Lambda</a> and <a href="http://aws.amazon.com/greengrass" target="_blank">Greengrass</a>, this Intel-powered camera lets developers experiment with Deep Learning in a fun and practical way.</p><p id="bb38">Out of the box, <a href="http://docs.aws.amazon.com/deeplens/latest/dg/deeplens-templated-projects-overview.html" target="_blank">a number of projects</a> can be deployed to the camera in just a few clicks: face detection, object recognition, activity detection, neural art, etc.</p><figure id="0382"><img class="graf-image" src="image01.webp"/><figcaption>Try explaining to your kids that this *is* serious work…</figcaption></figure><p id="f648">The overall process looks like this:</p><ol class="postList"><li id="7081"><strong class="markup--li-strong">Train an Deep Learning model</strong> in the cloud with <a href="https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4" target="_blank">Apache MXNet</a>.</li><li id="8cee"><strong class="markup--li-strong">Write a Lambda function</strong> using the <a href="http://docs.aws.amazon.com/deeplens/latest/dg/deeplens-device-library.html" target="_blank">DeepLens SDK</a> to run inference on images coming from the camera.</li><li id="df13"><strong class="markup--li-strong">Bundle both</strong> in a DeepLens project.</li><li id="9f9f"><strong class="markup--li-strong">Deploy the project</strong> to your DeepLens camera (using Greengrass, although this is completely transparent)</li><li id="183a"><strong class="markup--li-strong">Run the project</strong> on your DeepLens camera, view the project video stream and receive <a href="https://aws.amazon.com/iot/" target="_blank">AWS IoT</a> messages sent by the Lambda function.</li></ol><p id="38f9">This is seriously cool and fun, but I want to know how this really works. Don’t you? Yeah, I thought so :)</p><h4 id="733a">Models are not always what they seem</h4><p id="9499">In the DeepLens console, we can easily see that <strong class="markup--p-strong">project models are deployed from S3</strong>. Here’s how it looks for the face detection project.</p><figure id="189c"><img class="graf-image" src="image06.webp"/></figure><p id="77b1">Let’s take a look, then.</p><pre class="graf--pre" id="03ef">$ aws s3 ls s3://deeplens-managed-resources/models/SSDFacialDetect/<br/>2017-11-23 08:57:20   54559092 mxnet_deploy_ssd_FP16_FUSED.bin<br/>2017-11-23 08:57:20     127913 mxnet_deploy_ssd_FP16_FUSED.xml</pre><p class="graf-after--pre" id="7e1e">Huh? This is <strong class="markup--p-strong">not</strong> what an MXNet looks like. <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-4-df22560b83fe" target="_blank">As explained before</a>, we should see a <strong class="markup--p-strong">JSON file</strong> holding the model definition and <strong class="markup--p-strong">PARAMS file</strong> storing model weights.</p><figure id="846d"><img class="graf-image" src="image02.webp"/></figure><p id="06f9">Let’s <em class="markup--p-em">ssh</em> to the camera and try to figure this out.</p><h4 id="24a0">Exploring DeepLens</h4><p id="3552">After a few minutes, well… bingo.</p><pre class="graf--pre" id="5b78">aws_cam@Deepcam:/opt/intel$ ls<br/>deeplearning_deploymenttoolkit  deeplearning_deploymenttoolkit_2017.1.0.5675  <br/>intel_sdp_products.db  <br/>intel_sdp_products.tgz.db  <br/>ism  <br/>opencl</pre><p class="graf-after--pre" id="59cd"><strong class="markup--p-strong">Intel Deep Learning Deployment Toolkit</strong>. This sounds exciting. A few seconds of googling later, we learn <a href="https://software.intel.com/en-us/inference-engine-devguide-introduction" target="_blank">here</a> that this SDK includes:</p><ul class="postList"><li id="fffa">A <strong class="markup--li-strong">Model Optimizer</strong>, which converts our trained model into an optimized <strong class="markup--li-strong">Intermediate Representation</strong> (IR).</li><li id="2956">An <strong class="markup--li-strong">Inference Engine</strong> optimized for the underlying hardware platform.</li></ul><figure id="9456"><img class="graf-image" src="image04.webp"/><figcaption>Source: Intel</figcaption></figure><p id="1b5e">This makes <strong class="markup--p-strong">a lot</strong> of sense. Although it’s perfectly capable to run in resource-constrained environments — as demonstrated by my <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" target="_blank">Raspberry Pi experiment </a>— Apache MXNet is not the best option here. First, it carries a lot of code (training, data loading, etc.) which is useless in an <strong class="markup--p-strong">inference context</strong>. Second, it simply cannot compete with a platform-specific implementation making full use of <strong class="markup--p-strong">dedicated hardware</strong>, <strong class="markup--p-strong">special instructions</strong> and so on.</p><p id="0f6a">So now, the model files in S3 make sense. The XML file is the <strong class="markup--p-strong">model description</strong> and the BIN file is the <strong class="markup--p-strong">model in IR form</strong>.</p><p id="5b4d">What kind of <strong class="markup--p-strong">hardware</strong> is it optimized for ? Let’s look at the hardware for a second.</p><h4 id="8eba">Under the hood</h4><p id="2100">The CPU is pretty obvious. It’s a <a href="https://ark.intel.com/products/96486/Intel-Atom-x5-E3930-Processor-2M-Cache-up-to-1_80-GHz" target="_blank"><strong class="markup--p-strong">dual-core Atom E3930</strong></a>.</p><pre class="graf--pre" id="d596">$ dmesg |grep Intel<br/>[    0.108336] smpboot: CPU0: Intel(R) Atom(TM) Processor E3930 @ 1.30GHz (family: 0x6, model: 0x5c, stepping: 0x9)</pre><p class="graf-after--pre" id="1349">This baby comes with an <strong class="markup--p-strong">Intel HD Graphics 500</strong> chip, so we have a GPU in there too. This one has 12 “execution units” capable of running 7 threads each (SIMD architecture). 84 “cores”, then: not a monster, but surely better than running inference on the Atom itself.</p><p id="bf2c">Now it’s starting to make sense. The Inference Engine is certainly able to leverage <strong class="markup--p-strong">specific instructions on the Atom</strong> (with <a href="https://software.intel.com/en-us/mkl" target="_blank">Intel MKL</a>, no doubt) as well as the <strong class="markup--p-strong">GPU architecture</strong> (with <a href="https://software.intel.com/en-us/iocl_rt_ref" target="_blank">OpenCL</a>).</p><p id="df6c">Now what about the model optimizin’ thing?</p><h4 id="fd75">Model optimization</h4><p id="d456">Says the Intel doc: “(the model optimizer) <em class="markup--p-em">performs static model analysis and automatically adjusts deep learning models for optimal execution on end-point target device</em>”.</p><p id="866d">OK. It optimizes. Nice job explaining it :-/ Let’s figure it out.</p><p id="dc40">After a bit of installing (and cursing at python), we’re able to run the optimizer.</p><figure id="460a"><script src="https://gist.github.com/juliensimon/99b3af914a13048b9096c02f8186fa2c.js"></script></figure><p id="97e4">Most parameters make sense, but two are a bit intriguing.</p><ul class="postList"><li id="12fd"><code class="markup--code markup--li-code">--precision</code> - A precision of the output model. Valid values: <strong class="markup--li-strong">FP32</strong> (by default) or <strong class="markup--li-strong">FP16</strong>. Depending on the selected precision, weights would be aligned accordingly.</li><li id="edf9"><code class="markup--code markup--li-code">--fuse</code> - flag which enables <strong class="markup--li-strong">fusion</strong> (combination) of layers to boost topology execution. The idea is to join layers to reduce calculations during inference. Valid values: ON (by default) or OFF.</li></ul><p id="3789">OK, this explains the model name we saw earlier.</p><pre class="graf--pre" id="1dd0">mxnet_deploy_ssd_FP16_FUSED.bin</pre><p class="graf-after--pre" id="8828">This model uses <strong class="markup--p-strong">16-bit floating values for weights </strong>(and probably activation functions too). Obviously, 16-bit arithmetic is both <strong class="markup--p-strong">faster</strong> and more <strong class="markup--p-strong">energy-efficient</strong> than 32-bit arithmetic, so this makes sense.</p><blockquote class="graf--blockquote" id="27fc">As a side note, it’s possible to train MXNet directly with FP16 precision. My gut feeling tells me that this will probably yield more accurate models than training with FP32 and then converting to FP16, but who knows. More information on this <a class="markup--blockquote-anchor" href="http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank">NVIDIA page</a>.</blockquote><p class="graf-after--blockquote" id="8d38">OK, now let’s run this thing on <strong class="markup--p-strong">existing models</strong>. First, we’ll download Inception v3 and VGG-16 from the <a href="https://mxnet.incubator.apache.org/model_zoo/" target="_blank">MXNet model zoo</a>. Then, we’ll convert them.</p><figure id="fad6"><script src="https://gist.github.com/juliensimon/5e6db2afcb418c5c7a744c5022138107.js"></script></figure><p id="7019">Let’s now compare the original models to their optimized version.</p><figure id="45ed"><script src="https://gist.github.com/juliensimon/f77d44d90267caa105384e542bfdd207.js"></script></figure><p id="410a">As you can see, moving to <strong class="markup--p-strong">FP16 definitely halves model size</strong>. Less storage, less RAM, less compute!</p><p id="8e51">Now there’s only question? Do these models work? Should we give this Inference Engine a spin? Of course we should.</p><h4 id="9fdc">Predicting with IR models</h4><p id="18ab">The toolkit comes with a bunch of <strong class="markup--p-strong">samples</strong>, let’s build them.</p><figure id="a488"><script src="https://gist.github.com/juliensimon/7dd4ee1a9e091e0f1332a18fd3464af5.js"></script></figure><p id="acb4">Our models are <strong class="markup--p-strong">classification models</strong>, so let’s try this.</p><figure id="e00b"><script src="https://gist.github.com/juliensimon/a32a44147ff78deac59edb5d907cb99b.js"></script></figure><p id="96b0">Let’s grab an image and <strong class="markup--p-strong">resize</strong> it to 224x224.</p></div><div><figure id="7216"><img class="graf-image" src="image03.webp"/></figure></div><div><p id="989c">How well do our networks do on this one?</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="c382">At the moment, I haven’t figured out how to get this code to run on the GPU (but I will). My best guess is that the GPU is already busy running the actual DeepLens stuff. Since I’m not going to delete it, I’m running on the CPU instead with FP32 precision. Grumble grumble.</blockquote><figure class="graf-after--blockquote" id="9ff9"><script src="https://gist.github.com/juliensimon/ab3f718050e2f47d06594b99c830086e.js"></script></figure><p id="e87c">Both networks report <strong class="markup--p-strong">category #292</strong> as the top one. Let’s check the <strong class="markup--p-strong">ImageNet categories</strong> to find out whether this prediction is correct. Line numbers start at one, so category #292 is on line 293 ;)</p><figure id="a6cc"><img class="graf-image" src="image05.webp"/></figure><p id="9963">Good call :)</p><p id="4a2d">So there you go. We answered a lot of questions:</p><ul class="postList"><li id="6b35">We now know that DeepLens is not running MNXet itself, but the <strong class="markup--li-strong">Intel Inference Engine</strong> on an optimized model.</li><li id="6ba6">We know how to convert models using the <strong class="markup--li-strong">Model Optimizer</strong>.</li><li id="1b1a">We know how to run image classification models on the <strong class="markup--li-strong">Inference Engine</strong>.</li></ul><p id="192e">Next step? How about deploying these tools on a SageMaker instance, training an MXNet model, converting it and deploying it to DeepLens? This should keep me busy during the holidays (#NoLifeTillDeath).</p><p id="20ec">I hope you liked this crazy post. Thanks for reading!</p></div></div></section>
</section>
</article></body></html>
