<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Fascinating Tales of a Strange Tomorrow</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="f0d0">Fascinating Tales of a Strange Tomorrow</h3><h3 id="a4da">AI: Science vs. Fiction</h3><p id="0d92">Our trip begins in March 1956 with the release of the “Forbidden Planet” movie, which featured <strong class="markup--p-strong">Robbie the Robot</strong>, commonly acknowledged as the first Science Fiction robot on screen. A few months later, a small group of Computer Scientists led by <strong class="markup--p-strong">John McCarthy</strong><a href="https://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29" target="_blank">[1]</a> held a 6-week workshop<a href="https://en.wikipedia.org/wiki/Dartmouth_workshop" target="_blank">[2]</a> at Dartmouth College in New Hampshire.</p></div><div><figure id="618a" style="width: 58.207%;"><img class="graf-image" src="image02.webp"/></figure><figure class="graf--layoutOutsetRowContinue" id="0bcc" style="width: 41.793%;"><img class="graf-image" src="image01.webp"/><figcaption style="width: 239.275%; left: -139.275%;">John McCarthy (Turing Award 1971) &amp; Robbie the Robot</figcaption></figure></div><div><p id="a9ab">The topic of this workshop was “<strong class="markup--p-strong">Artificial Intelligence</strong>”, a term coined by McCarthy himself, which he defined this way:</p><blockquote class="graf--blockquote" id="d475"><em class="markup--blockquote-em">“Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it”</em>.</blockquote><p class="graf-after--blockquote" id="9750">Wouldn’t it be great if the Dartmouth workshop had actually be triggered by John McCarthy seeing the “Forbidden Planet” and then going home thinking: “Let’s build Robbie”? That’s probably not true at all, though. Oh well.</p><p id="34b1">Anyway, the group got to work and laid the foundations of Artificial Intelligence as we know it. In fact, most of the participants devoted their entire career to furthering the state of the art on AI, receiving no less than four Turing Awards in the process: <strong class="markup--p-strong">Marvin Minsky</strong><a href="https://en.wikipedia.org/wiki/Marvin_Minsky" target="_blank">[3]</a> in 1969, John McCarthy in 1971, <strong class="markup--p-strong">Herbert Simon</strong><a href="https://en.wikipedia.org/wiki/Herbert_A._Simon" target="_blank">[4]</a> &amp; <strong class="markup--p-strong">Allen Newell</strong><a href="https://en.wikipedia.org/wiki/Allen_Newell" target="_blank">[5]</a> in 1975.</p></div><div><figure id="724c" style="width: 40.579%;"><img class="graf-image" src="image05.webp"/></figure><figure class="graf--layoutOutsetRowContinue" id="6441" style="width: 59.421%;"><img class="graf-image" src="image07.webp"/><figcaption style="width: 168.291%; left: -68.291%;">Herbert Simon (Turing Award 1975, Nobel Prize in Economics 1978) ad Allen Newell (Turing Award 1975)</figcaption></figure></div><div><p id="43a8">During the early years of AI, these bright scientists made predictions, such as:</p><p id="27c1">· 1958, Herbert Simon and Allen Newell: “<strong class="markup--p-strong"><em class="markup--p-em">Within 10 years a digital computer will be the world’s chess champion.</em></strong>”</p><p id="1b0c">· 1965, Herbert Simon: “<strong class="markup--p-strong"><em class="markup--p-em">Machines will be capable, within 20 years, of doing any work a man can do.</em></strong>”</p><p id="08df">· 1967 Marvin Minsky: “<strong class="markup--p-strong"><em class="markup--p-em">Within a generation, the problem of creating ‘artificial intelligence’ will substantially be solved</em>.</strong>”</p><p id="7d37">· 1970 Marvin Minsky: “<strong class="markup--p-strong"><em class="markup--p-em">In from 3 to 8 years, we will have a machine with the general intelligence of an average human being.</em></strong>”</p><p id="6c94">Oops.</p><h3 id="c139">The AI Winter is Coming</h3><p id="5cfa">Predicting the future is always risky business, but still… <strong class="markup--p-strong">This raises an daunting question: how could such brilliant minds be so awfully wrong about what AI would (or wouldn’t) achieve in a reasonable time frame? </strong>Don’t worry, we’ll answer this question later on.</p><blockquote class="graf--blockquote" id="25d9">Unfortunately, repeated failures to achieve significant progress became a trademark of Artificial Intelligence.</blockquote><p class="graf-after--blockquote" id="68e3">Expectations were high, little or no results were delivered, funds were cut and projects were abandoned. Unsurprisingly, these multiple “<strong class="markup--p-strong">AI winters</strong>” discouraged all but the most hardcore supporters.</p><p id="1c86">The most glaring symbol of this disillusion came from Marvin Minsky himself. In 2001, he gave a talk named “<strong class="markup--p-strong">It’s 2001: Where is HAL?</strong>” referring of course to the HAL computer in Stanley Kubrick’s movie “2001: A Space Odyssey”. This is all the more significant that back in 1968, Minsky actually advised Kubrick during the making of the movie. In this talk, he notably addresses the “<strong class="markup--p-strong">Common Sense issue</strong>” in non-ambiguous terms: <em class="markup--p-em">“No program today can distinguish a dog from a cat, or recognize objects in typical rooms, or answer questions that 4-year-olds can!”</em></p></div><div><figure id="d8b5" style="width: 59.219%;"><img class="graf-image" src="image04.webp"/></figure><figure class="graf--layoutOutsetRowContinue" id="1feb" style="width: 40.781%;"><img class="graf-image" src="image09.webp"/><figcaption style="width: 245.212%; left: -145.212%;">Marvin Minsky (Turing Award 1969) &amp; HAL 9000</figcaption></figure></div><div><p id="10c1">Bottom line: AI is cool to play with in a lab environment, but it will never achieve anything in the real world. Case closed.</p><h3 id="175c">Meanwhile, on the US West Coast…</h3><p id="9ebc">While AI researchers despaired in their labs, a number of startups were reinventing the world: Amazon, Google, Yahoo, later joined by Facebook and a few others were growing their web platforms at a frantic pace. In the process, they were acquiring users by the millions and piling up mountains of data. It soon became clear that this data was a goldmine, <strong class="markup--p-strong">if it could actually be mined</strong>!</p><p id="52d9">Using commodity hardware, these companies’ engineers set on a quest to design and build data processing platforms that would allow them to <strong class="markup--p-strong">crunch raw data and extract business value that could turn into revenue</strong>… always a key goal for fast-growing startups!</p><p id="f747">A major milestone was reached in December 2004, when Google released the famous <strong class="markup--p-strong">Map Reduce</strong> paper<a href="https://research.google.com/archive/mapreduce.html" target="_blank">[6]</a>, where they described « <em class="markup--p-em">a programming model and an associated implementation for processing and generating large data sets </em>». Not to be outdone, Yahoo implemented the ideas described in this paper and released a first version of their project in April 2006: <strong class="markup--p-strong">Hadoop</strong><a href="https://hadoop.apache.org/" target="_blank">[7]</a> was born.</p><blockquote class="graf--blockquote" id="a3d3">Gasoline waiting for a match: the Machine Learning explosion happened and the rest, as they say, is <strong class="markup--blockquote-strong">history</strong>.</blockquote><h3 class="graf-after--blockquote" id="9973">Fast-forward a few years</h3><p id="5f32">2010 or so: <strong class="markup--p-strong">Machine Learning is now a commodity</strong>. Customers have a wide range of options, from DIY to Machine Learning as a Service. Everything is great in Data World. But is it really? Yes, Machine Learning helped us make a lot of applications “smarter” but <strong class="markup--p-strong">did we make significant progress on Artificial Intelligence?</strong> In other words, are we any closer to “building HAL”? Well… no. Let’s try to understand why.</p><p id="a57a">One of the first steps in building a Machine Learning application is called “<strong class="markup--p-strong">feature extraction</strong>”. In a nutshell, this is a step where Data Scientists explore the data set to figure out which variables are meaningful in predicting or classifying data and which aren’t. Although this is still mostly a lengthy manual process, it’s now well understood and works nicely on structured or semi-structured data such as web logs or sales data.</p><p id="d23a">However, <strong class="markup--p-strong">it doesn’t work for complex AI problems</strong> such as computer vision or computer speech, simply because it’s quite impossible to define <strong class="markup--p-strong">formally</strong> what the features are: for example, what makes a cat a cat? And how is a cat different from a dog? Or from a lion?</p><blockquote class="graf--blockquote" id="67ca">To put it simply, traditional Machine Learning doesn’t solve this kind of problem, which is why new tools are needed. Enter neural networks!</blockquote><h3 class="graf-after--blockquote" id="99fb">Back To The Future</h3><p id="6d0c">New tools? Hardly! In 1957, Frank Rosenblatt designed an electro-mechanical neural network, the Perceptron<a href="https://en.wikipedia.org/wiki/Perceptron" target="_blank">[8]</a>, which he trained to recognize images (20x20 “pixels”). In 1975, Paul Werbos published a article describing “backpropagation”<a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank">[9]</a>, an algorithm allowing better and faster training of neural networks.</p><p id="488b">So, <strong class="markup--p-strong">if neural networks have been around for so long, surely they must be partly responsible for failed AI attempts, right?</strong> Should they really be resurrected? Why would they suddenly be successful?</p><p id="001a">Very valid questions indeed. Let’s first take a quick look at how neural networks work. A <strong class="markup--p-strong">neuron</strong> is a simple construct, which sums multiple <strong class="markup--p-strong">weighted inputs</strong> to produce an <strong class="markup--p-strong">output</strong>. Neurons are organized in <strong class="markup--p-strong">layers</strong>, where the output of each neuron in layer ’n’ serves as an input to each neuron in layer ‘n+1’. The first layer is called the <strong class="markup--p-strong">input layer</strong> and is fed with the input data, say the pixel values of an image. The last layer is called the <strong class="markup--p-strong">output layer</strong> and produces the result, say a category number for the image (“this is a dog”).</p></div><div><figure id="3c2b" style="width: 64.244%;"><img class="graf-image" src="image11.webp"/></figure><figure class="graf--layoutOutsetRowContinue" id="303c" style="width: 35.756%;"><img class="graf-image" src="image03.webp"/><figcaption style="width: 279.673%; left: -179.673%;">The basic structure of a neural network (Source: “Deep Learning”, Goodfellow &amp; Bengio, 2016)</figcaption></figure></div><div><blockquote class="graf--blockquote" id="2d18">The beauty of neural networks is that they’re able to <strong class="markup--blockquote-strong">self-organize</strong>: given a large enough data set (say, images as inputs and category labels as outputs), a neural network is able to learn <strong class="markup--blockquote-strong">automatically</strong> how to produce correct answers</blockquote><p class="graf-after--blockquote" id="7b70">Thanks to an iterative training process, it’s able to <strong class="markup--p-strong">discover the features</strong> which allow images to be categorized, and adjusts weights repeatedly to reach the best result, i.e. the one with the smallest error rate.</p><p id="f0e5">The training phase and its automatic feature discovery are well adapted to solving informal problems, but here’s the catch: they involve <strong class="markup--p-strong">a lot</strong> of math operations which tend to grow exponentially as <strong class="markup--p-strong">data size</strong> increases (think high-resolution pictures) and as the <strong class="markup--p-strong">number of layers</strong> increases. This problem is called the “<strong class="markup--p-strong">Curse of Dimensionality</strong>” and it’s one of the major reasons why neural networks stagnated for decades: there was simply not enough <strong class="markup--p-strong">computing power</strong> available to run them at scale.</p><p id="a6ca">Nor was enough <strong class="markup--p-strong">data</strong> available. Neural networks need a lot of data to learn properly. The more data, the better! Until recently, it was simply not possible to gather and store vast amounts of digital data. Do you remember punch cards or floppy disks?</p><p id="f2df">A significant breakthrough happened in 1998 when Yann Le Cun invented <strong class="markup--p-strong">Convolutional Neural Networks</strong><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank">[10]</a>, a new breed of multi-layered networks (hence the term “Deep Learning”).</p><blockquote class="graf--blockquote" id="fc4f">In a nutshell, CNNs are able to extract features efficiently while reducing the size of input data: this allows smaller networks to be used for classification, which dramatically reduces the computing cost of network training.</blockquote><p class="graf-after--blockquote" id="da83">This approach was so successful that banks adopted CNN-driven systems to automate handwriting recognition for checks. This was an encouraging accomplishment for neural networks… but the best was still to come!</p><figure id="7d8d"><img class="graf-image" src="image08.webp"/><figcaption>Architecture of a Convolutional Neural Network (Source: NVIDIA blog)</figcaption></figure><h3 id="b95c">The Neural Empire Strikes Back</h3><p id="1dd1">By the late 2000s, three quasi-simultaneous events made large-scale neural networks possible.</p><p id="65da">First, <strong class="markup--p-strong">large data sets</strong> became widely available. Text, pictures, movies, music: everything was suddenly digital and could be used to train neural networks. Today, the ImageNet<a href="http://image-net.org" target="_blank">[11]</a> database holds over 14 million labeled images and researchers worldwide use it to compete every year<a href="http://image-net.org/challenges/LSVRC/" target="_blank">[12]</a> in build the most successful image detection and classification network (more on this later).</p><p id="57d2">Then, researchers were able to leverage the <strong class="markup--p-strong">spectacular parallel processing power</strong> of <strong class="markup--p-strong">Graphics Processing Units</strong> (GPUs) to train large neural networks. Can you believe that the ones that won the 2015 and 2016 ImageNet competition have respectively 152 and 269 layers?</p><p id="704e">Last but not least, <strong class="markup--p-strong">Cloud computing</strong> brought <strong class="markup--p-strong">elasticity</strong> and <strong class="markup--p-strong">scalability</strong> to developers and researchers, allowing them to use as much infrastructure as needed for training… without having to build, run or pay for it long term.</p><blockquote class="graf--blockquote" id="4451">The combination of these three factors helped neural networks deliver on their 60-year old promise.</blockquote><p class="graf-after--blockquote" id="88c2">State of the art networks are now able to <strong class="markup--p-strong">classify images</strong> <strong class="markup--p-strong">faster</strong> and <strong class="markup--p-strong">more accurately</strong> than any human (less than 3% error vs. 5% for humans). Devices like the Amazon Echo understand <strong class="markup--p-strong">natural language</strong> and speak back at us. Autonomous cars are becoming a reality. And the list of AI applications grows every day.</p><p id="942e"><strong class="markup--p-strong">Wouldn’t you like to add yours?</strong></p><figure id="0eff"><img class="graf-image" src="image10.webp"/><figcaption>Number of layers and error rate of ILSVRC winners</figcaption></figure><h3 id="2bcb">How AWS can help you build Deep Learning applications</h3><p id="0461">AWS provides everything you need to start building Deep Learning applications:</p><p id="ba63">· A wide range of <strong class="markup--p-strong">Amazon EC2 instances</strong> to build and train your models, with your choice of <strong class="markup--p-strong">CPU</strong><a href="https://aws.amazon.com/about-aws/whats-new/2016/11/coming-soon-amazon-ec2-c5-instances-the-next-generation-of-compute-optimized-instances/" target="_blank">[13]</a>, <strong class="markup--p-strong">GPU</strong><a href="https://aws.amazon.com/blogs/aws/in-the-work-amazon-ec2-elastic-gpus/" target="_blank">[14]</a> <a href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" target="_blank">[15]</a> or even <strong class="markup--p-strong">FPGA</strong><a href="https://aws.amazon.com/blogs/aws/developer-preview-ec2-instances-f1-with-programmable-hardware/" target="_blank">[16]</a>.</p><p id="8b7f">· The <strong class="markup--p-strong">Deep Learning Amazon Machine Image</strong><a href="https://aws.amazon.com/marketplace/pp/B01M0AXXQB" target="_blank">[17]</a>, a collection of pre-installed tools and libraries: mxnet<a href="http://mxnet.io/" target="_blank">[18]</a> (which AWS officially supports), Theano, Caffe, TensorFlow, Torch, Anaconda and more.</p><p id="cfe7">· High-level <strong class="markup--p-strong">AI services</strong><a href="https://aws.amazon.com/amazon-ai/" target="_blank">[19]</a> for image recognition (<strong class="markup--p-strong">Amazon Rekognition</strong>), speech to text (<strong class="markup--p-strong">Amazon Polly</strong>) and chatbots (<strong class="markup--p-strong">Amazon Lex</strong>).</p><blockquote class="graf--blockquote" id="7809">The choice is yours, <strong class="markup--blockquote-strong">just get started</strong> and help Science catch up with Fiction!</blockquote><h3 class="graf-after--blockquote" id="b112">A New Hope?</h3><figure id="1b54"><img class="graf-image" src="image06.webp"/></figure><p id="d628">Artificial Intelligence is making progress every day. One can only wonder what is coming next!</p><p id="013a">Will machines learn how to understand humans — not the other way around?</p><p id="b3e3">Will they help humans understand each other?</p><p id="aa85">Will they end up ruling the world?</p><p id="2cc2">Who knows?</p><p id="3c5a"><strong class="markup--p-strong">Whatever happens, these will be fascinating tales of a strange tomorrow.</strong></p><p id="ff4c"><em class="markup--p-em">Note: this is an edited transcript of one of my current keynote talks. Original slides are available </em><a href="https://www.slideshare.net/JulienSIMON5/fascinating-tales-of-a-strange-tomorrow-74449554" target="_blank"><em class="markup--p-em">here</em></a><em class="markup--p-em">.</em></p></div></div></section>
</section>
</article></body></html>
