<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Training MXNet — part 1: MNIST</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="5efb">Training MXNet — part 1: MNIST</h3><p id="990f">In a <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab" target="_blank">previous series</a>, we discovered how we could use the <a href="http://mxnet.io/" target="_blank">MXNet</a> library and <a href="http://mxnet.io/model_zoo/" target="_blank">pre-trained models</a> for object detection. In this series, we’re going to focus on <strong class="markup--p-strong">training models</strong> with a number of different data sets.</p><p id="9a53">Let’s start with the famous <strong class="markup--p-strong">MNIST</strong> data set.</p><p id="d41a"><em class="markup--p-em">Please note that is an updated and expanded version of this </em><a href="https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb" target="_blank"><em class="markup--p-em">tutorial:</em></a><em class="markup--p-em"> I’m using the Module API (instead of the deprecated Model API) as well as the MNIST data iterator.</em></p><h4 id="6d43">The MNIST data set</h4><p id="d98b">This <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">data</a> is a set of 28x28 greyscale images representing <strong class="markup--p-strong">handwritten digits</strong> (0 to 9).</p><figure id="dd69"><img class="graf-image" src="image04.webp"/><figcaption>Samples from the MNIST data set</figcaption></figure><p id="c1ca">The <strong class="markup--p-strong">training set</strong> has 60,000 samples and the <strong class="markup--p-strong">test set</strong> has 10,000 examples. Let’s download them right away.</p><pre class="graf--pre" id="9edd"># Training set: images and labels<br/>$ wget <a class="markup--pre-anchor" href="http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz</a><br/>$ wget <a class="markup--pre-anchor" href="http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz</a></pre><pre class="graf--pre graf-after--pre" id="4b04"># Validation set: images and labels<br/>$ wget <a class="markup--pre-anchor" href="http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz</a><br/>$ wget <a class="markup--pre-anchor" href="http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz" rel="nofollow noopener" target="_blank">http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz</a></pre><pre class="graf--pre graf-after--pre" id="6992">$ gzip -d *</pre><p class="graf-after--pre" id="85c8">How about we take a look <strong class="markup--p-strong">inside</strong> these files? We’ll start with the labels. They are stored as a <strong class="markup--p-strong">serialized </strong><a href="http://www.numpy.org" target="_blank"><strong class="markup--p-strong">numpy</strong></a><strong class="markup--p-strong"> array</strong> holding 60,000 unsigned bytes.</p><p id="7cb1">The file starts with a big-endian packed structure, holding 2 integers: magic number, number of labels.</p><pre class="graf--pre" id="2e09">&gt;&gt;&gt; import struct<br/>&gt;&gt;&gt; import numpy as np<br/>&gt;&gt;&gt; import cv2</pre><pre class="graf--pre graf-after--pre" id="df93">&gt;&gt;&gt; labelfile = open("train-labels-idx1-ubyte")<br/>&gt;&gt;&gt; magic, num = struct.unpack("&gt;II", labelfile.read(8))<br/>&gt;&gt;&gt; labelarray = np.fromstring(labelfile.read(), dtype=np.int8)</pre><pre class="graf--pre graf-after--pre" id="6e9c">&gt;&gt;&gt; print labelarray.shape<br/>&gt;&gt;&gt; print labelarray[0:10]</pre><pre class="graf--pre graf-after--pre" id="1c8c">(60000,)<br/>[5 0 4 1 9 2 1 3 1 4]</pre><p class="graf-after--pre" id="dcfe">Let’s now extract some images. Again, they are stored as a serialized <a href="http://www.numpy.org" target="_blank">numpy</a> array, which we will <strong class="markup--p-strong">reshape</strong> to build 28x28 images. Each pixel is stored as an unsigned byte (0 for black, 255 for white).</p><p id="d0d0">The file starts with a big-endian packed structure, holding 4 integers: magic number, number of images, number of rows and number of columns.</p><pre class="graf--pre" id="2276">&gt;&gt;&gt; imagefile = open("train-images-idx3-ubyte")<br/>&gt;&gt;&gt; magic, num, rows, cols = struct.unpack("&gt;IIII", imagefile.read(16))<br/>&gt;&gt;&gt; imagearray = np.fromstring(imagefile.read(), dtype=np.uint8)<br/>&gt;&gt;&gt; print imagearray.shape<br/>(47040000,)</pre><pre class="graf--pre graf-after--pre" id="0e87">&gt;&gt;&gt; imagearray = imagearray.reshape(num, rows, cols)<br/>&gt;&gt;&gt; print imagearray.shape<br/>(60000, 28, 28)</pre><p class="graf-after--pre" id="43ba">Let’s save the <strong class="markup--p-strong">first 10 images</strong> to disk.</p><pre class="graf--pre" id="05d9">&gt;&gt;&gt; for i in range(0,10):<br/>        img = imagearray[i]<br/>        imgname = "img"+(str)(i)+".png"<br/>        cv2.imwrite(imgname, img)</pre><pre class="graf--pre graf-after--pre" id="d0f0">$ ls img?.png<br/>img0.png img1.png img2.png img3.png img4.png img5.png img6.png img7.png img8.png img9.png</pre><p class="graf-after--pre" id="8363">This is how they look.</p><figure id="238b"><img class="graf-image" src="image06.webp"/></figure><p id="8b39">Ok, now that we understand the data, let’s build a <strong class="markup--p-strong">model</strong>.</p><h4 id="4dbe">Building a model</h4><p id="046a">We’re going to use a simple <strong class="markup--p-strong">multi-layer perceptron</strong> (similar to what we built <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" target="_blank">here</a>) : 784 → 128 → 64 → 10</p><ul class="postList"><li id="e0ba">Input layer: an array of <strong class="markup--li-strong">784</strong> pixel values (28x28).</li><li id="9aaf">First layer: <strong class="markup--li-strong">128</strong> neurons activated by the <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29" target="_blank">rectifier linear unit</a> function.</li><li id="0a2d">Second layer: <strong class="markup--li-strong">64</strong> neurons activated by the same function.</li><li id="60d4">Output layer: <strong class="markup--li-strong">10</strong> neurons (for our 10 categories), activated by the <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank">softmax function</a> in order to transform the 10 outputs into 10 values between 0 and 1 that add up to 1. Each value represents the <strong class="markup--li-strong">predicted probability for each category</strong>, the largest one pointing at the most likely category.</li></ul><pre class="graf--pre" id="110e">data = mx.sym.Variable('data')<br/>data = mx.sym.Flatten(data=data)<br/>fc1  = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=128)<br/>act1 = mx.sym.Activation(data=fc1, name='relu1', act_type="relu")<br/>fc2  = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden = 64)<br/>act2 = mx.sym.Activation(data=fc2, name='relu2', act_type="relu")<br/>fc3  = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden=10)<br/>mlp  = mx.sym.SoftmaxOutput(data=fc3, name='softmax')mod = mx.mod.Module(mlp)</pre><h4 class="graf-after--pre" id="4441">Building a data iterator</h4><p id="50ed">MXNet conveniently provides a <a href="http://mxnet.io/api/python/io.html#mxnet.io.MNISTIter" target="_blank"><strong class="markup--p-strong">data iterator</strong></a> for the MNIST data set. Thanks to this, we don’t have to open the files, build <em class="markup--p-em">NDArrays</em>, etc. It also has default parameters for filenames and so on. Very cool!</p><pre class="graf--pre" id="1613">train_iter = mx.io.MNISTIter(shuffle=True)<br/>val_iter = mx.io.MNISTIter(image="./t10k-images-idx3-ubyte", label="./t10k-labels-idx1-ubyte")</pre><p class="graf-after--pre" id="7bf3">We can now <strong class="markup--p-strong">bind</strong> the data to our model. Default batch size is 128.</p><pre class="graf--pre" id="697e">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)</pre><p class="graf-after--pre" id="0498">We’re now ready for training.</p><h4 id="240f">Training the model</h4><p id="5186">Let’s start with default settings for weight initialization and optimization (aka <strong class="markup--p-strong">hyperparameters</strong>) and hope for the best. Here we go!</p><pre class="graf--pre" id="1083">&gt;&gt;&gt; import logging<br/>&gt;&gt;&gt; logging.basicConfig(level=logging.INFO)<br/>&gt;&gt;&gt; mod.fit(train_iter, num_epoch=10)</pre><pre class="graf--pre graf-after--pre" id="6f7b">INFO:root:Epoch[0] Train-accuracy=0.111662<br/>INFO:root:Epoch[0] Time cost=1.244<br/>INFO:root:Epoch[1] Train-accuracy=0.112346<br/>INFO:root:Epoch[1] Time cost=1.376<br/>INFO:root:Epoch[2] Train-accuracy=0.112346<br/>INFO:root:Epoch[2] Time cost=1.254<br/>INFO:root:Epoch[3] Train-accuracy=0.112346<br/>INFO:root:Epoch[3] Time cost=1.296<br/>INFO:root:Epoch[4] Train-accuracy=0.112346<br/>INFO:root:Epoch[4] Time cost=1.234<br/>INFO:root:Epoch[5] Train-accuracy=0.112346<br/>INFO:root:Epoch[5] Time cost=1.283<br/>INFO:root:Epoch[6] Train-accuracy=0.112346<br/>INFO:root:Epoch[6] Time cost=1.440<br/>INFO:root:Epoch[7] Train-accuracy=0.112346<br/>INFO:root:Epoch[7] Time cost=1.237<br/>INFO:root:Epoch[8] Train-accuracy=0.112346<br/>INFO:root:Epoch[8] Time cost=1.235<br/>INFO:root:Epoch[9] Train-accuracy=0.112346<br/>INFO:root:Epoch[9] Time cost=1.307</pre><p class="graf-after--pre" id="bbca">Hmm, things are not going well. It looks like the network is <strong class="markup--p-strong">not learning</strong>. Actually, it is learning, but <strong class="markup--p-strong">real slow</strong>: the default learning rate is <strong class="markup--p-strong">0.01</strong>, which is too low. Let’s use a more reasonable value such as 0.1.</p><pre class="graf--pre" id="39c9">&gt;&gt;&gt; mod.init_params()<br/>&gt;&gt;&gt; mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))<br/>&gt;&gt;&gt; mod.fit(train_iter, num_epoch=10)</pre><pre class="graf--pre graf-after--pre" id="20b6">INFO:root:Epoch[0] Train-accuracy=0.111846<br/>INFO:root:Epoch[0] Time cost=1.288<br/>INFO:root:Epoch[1] Train-accuracy=0.427150<br/>INFO:root:Epoch[1] Time cost=1.308<br/>INFO:root:Epoch[2] Train-accuracy=0.842682<br/>INFO:root:Epoch[2] Time cost=1.271<br/>INFO:root:Epoch[3] Train-accuracy=0.900875<br/>INFO:root:Epoch[3] Time cost=1.282<br/>INFO:root:Epoch[4] Train-accuracy=0.928736<br/>INFO:root:Epoch[4] Time cost=1.288<br/>INFO:root:Epoch[5] Train-accuracy=0.944478<br/>INFO:root:Epoch[5] Time cost=1.296<br/>INFO:root:Epoch[6] Train-accuracy=0.953993<br/>INFO:root:Epoch[6] Time cost=1.287<br/>INFO:root:Epoch[7] Train-accuracy=0.960453<br/>INFO:root:Epoch[7] Time cost=1.294<br/>INFO:root:Epoch[8] Train-accuracy=0.965478<br/>INFO:root:Epoch[8] Time cost=1.297<br/>INFO:root:Epoch[9] Train-accuracy=0.969267<br/>INFO:root:Epoch[9] Time cost=1.291</pre><p class="graf-after--pre" id="c8e4">That’s more like it. We get to <strong class="markup--p-strong">96.93%</strong> accuracy after 10 epochs. What about validation accuracy? Let’s create a metric and score our validation data set.</p><pre class="graf--pre" id="11fa">&gt;&gt; metric = mx.metric.Accuracy()<br/>&gt;&gt; mod.score(val_iter, metric)<br/>&gt;&gt; print metric.get()<br/>('accuracy', 0.9654447115384616)</pre><p class="graf-after--pre" id="76b3">Pretty good accuracy at <strong class="markup--p-strong">96.5</strong>%.</p><p id="494b">Still, the first few training epochs are not great: this is caused by default <strong class="markup--p-strong">weight initialization</strong>. Let’s use something <a href="http://mxnet.io/api/python/optimization.html#the-mxnet-initializer-package" target="_blank">smarter</a>, like the Xavier technique.</p><pre class="graf--pre" id="e97b">&gt;&gt;&gt; mod.init_params(initializer=mx.init.Xavier(magnitude=2.))<br/>&gt;&gt;&gt; mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))<br/>&gt;&gt;&gt; mod.fit(train_iter, num_epoch=10)</pre><pre class="graf--pre graf-after--pre" id="dd73">INFO:root:Epoch[0] Train-accuracy=0.860994<br/>INFO:root:Epoch[0] Time cost=1.338<br/>INFO:root:Epoch[1] Train-accuracy=0.935797<br/>INFO:root:Epoch[1] Time cost=1.325<br/>INFO:root:Epoch[2] Train-accuracy=0.951840<br/>INFO:root:Epoch[2] Time cost=1.273<br/>INFO:root:Epoch[3] Train-accuracy=0.961438<br/>INFO:root:Epoch[3] Time cost=1.264<br/>INFO:root:Epoch[4] Train-accuracy=0.968066<br/>INFO:root:Epoch[4] Time cost=1.250<br/>INFO:root:Epoch[5] Train-accuracy=0.973174<br/>INFO:root:Epoch[5] Time cost=1.299<br/>INFO:root:Epoch[6] Train-accuracy=0.976846<br/>INFO:root:Epoch[6] Time cost=1.374<br/>INFO:root:Epoch[7] Train-accuracy=0.979601<br/>INFO:root:Epoch[7] Time cost=1.407<br/>INFO:root:Epoch[8] Train-accuracy=0.982121<br/>INFO:root:Epoch[8] Time cost=1.336<br/>INFO:root:Epoch[9] Train-accuracy=0.983958<br/>INFO:root:Epoch[9] Time cost=1.343</pre><pre class="graf--pre graf-after--pre" id="165b">&gt;&gt; metric = mx.metric.Accuracy()<br/>&gt;&gt; mod.score(val_iter, metric)<br/>&gt;&gt; print metric.get()<br/>('accuracy', 0.9744591346153846)</pre><p class="graf-after--pre" id="cda8">That’s much better: we get to <strong class="markup--p-strong">86%</strong> accuracy after only one epoch. We gain almost <strong class="markup--p-strong">1.5%</strong> training accuracy and <strong class="markup--p-strong">1%</strong> validation accuracy.</p><p id="2cbf">Can we get better results? Well, we could always try to train the model longer. Let’s try 50 epochs.</p><pre class="graf--pre" id="d456">...<br/>INFO:root:Epoch[39] Train-accuracy=0.999950<br/>INFO:root:Epoch[39] Time cost=1.284<br/>INFO:root:Epoch[40] Train-accuracy=0.999967<br/>INFO:root:Epoch[40] Time cost=1.301<br/>INFO:root:Epoch[41] Train-accuracy=0.999967<br/>INFO:root:Epoch[41] Time cost=1.811<br/>INFO:root:Epoch[42] Train-accuracy=1.000000<br/>INFO:root:Epoch[42] Time cost=1.412<br/>INFO:root:Epoch[43] Train-accuracy=1.000000<br/>INFO:root:Epoch[43] Time cost=1.275<br/>INFO:root:Epoch[44] Train-accuracy=1.000000<br/>INFO:root:Epoch[44] Time cost=1.200<br/>...<br/>('accuracy', 0.9785657051282052)</pre><p class="graf-after--pre" id="13e1">As you can see, we hit 100% training accuracy after 42 epochs and there’s no point in going any further. In the process, we only manage to improve validation accuracy by 0.4%.</p><p id="a842">Is this the best we can do? We could try <a href="http://mxnet.io/api/python/model.html#optimizer-api-reference" target="_blank">other optimizers</a>, but unless you really know what you’re doing, it’s probably safer to stick to SGD.</p><p id="7b2e">Maybe we simply need a bigger boat?</p><h4 id="f292">Using a deeper network</h4><p id="4f50">Let’s try this network and see what happens :</p><p id="d871">784 → 256 → 128 → 64 → 10</p><pre class="graf--pre" id="b555">data = mx.sym.Variable('data')<br/>data = mx.sym.Flatten(data=data)</pre><pre class="graf--pre graf-after--pre" id="da1f">fc1  = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=256)<br/>act1 = mx.sym.Activation(data=fc1, name='relu1', act_type="relu")</pre><pre class="graf--pre graf-after--pre" id="6c4a">fc2  = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden = 128)<br/>act2 = mx.sym.Activation(data=fc2, name='relu2', act_type="relu")</pre><pre class="graf--pre graf-after--pre" id="379f">fc3  = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden = 64)<br/>act3 = mx.sym.Activation(data=fc3, name='relu3', act_type="relu")</pre><pre class="graf--pre graf-after--pre" id="defd">fc4  = mx.sym.FullyConnected(data=act3, name='fc4', num_hidden=10)<br/>mlp  = mx.sym.SoftmaxOutput(data=fc4, name='softmax')</pre><pre class="graf--pre graf-after--pre" id="f93f">mod = mx.mod.Module(mlp)</pre><pre class="graf--pre graf-after--pre" id="b69b">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)<br/>mod.init_params(initializer=mx.init.Xavier(magnitude=2.))<br/>mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))<br/>mod.fit(train_iter, num_epoch=50)</pre><p class="graf-after--pre" id="07fb">We hit 100% training accuracy after 25 epochs and get to <strong class="markup--p-strong">97.99%</strong> validation accuracy, a <strong class="markup--p-strong">modest 0.14% increase</strong> compared to the previous model. Clearly, a deeper multi-layer perceptron is not the way to go.</p><p id="95de">We need a better boat, then.</p><h4 id="24ad">Using a Convolutional Neural Network</h4><p id="8669">We’ve seen that these networks work <strong class="markup--p-strong">very well</strong> for image processing. Let’s try a well-known CNN — called <a href="http://yann.lecun.com/exdb/lenet/" target="_blank"><strong class="markup--p-strong">LeNet </strong></a>— on this data set.</p><p id="4da2">Here is the model definition, everything else is identical.</p><pre class="graf--pre" id="aabf">data = mx.symbol.Variable('data')<br/><br/>conv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)<br/>tanh1 = mx.sym.Activation(data=conv1, act_type="tanh")<br/>pool1 = mx.sym.Pooling(data=tanh1, pool_type="max", kernel=(2,2), stride=(2,2))<br/><br/>conv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)<br/>tanh2 = mx.sym.Activation(data=conv2, act_type="tanh")<br/>pool2 = mx.sym.Pooling(data=tanh2, pool_type="max", kernel=(2,2), stride=(2,2))<br/><br/>flatten = mx.sym.Flatten(data=pool2)<br/>fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)<br/>tanh3 = mx.sym.Activation(data=fc1, act_type="tanh")<br/><br/>fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)<br/><br/>lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')</pre><pre class="graf--pre graf-after--pre" id="d402">mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)<br/>mod.init_params(initializer=mx.init.Xavier(magnitude=2.))<br/>mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))<br/>mod.fit(train_iter, num_epoch=10)</pre><p class="graf-after--pre" id="03a6">Let’s train again.</p><pre class="graf--pre" id="6593">INFO:root:Epoch[0] Train-accuracy=0.906634<br/>INFO:root:Epoch[0] Time cost=46.034<br/>INFO:root:Epoch[1] Train-accuracy=0.971989<br/>INFO:root:Epoch[1] Time cost=47.089</pre><p class="graf-after--pre" id="1a44">This is <strong class="markup--p-strong">painfully</strong> slow. About 45 seconds per epoch, about <strong class="markup--p-strong">30 times slower</strong> than the multilayer perceptron. Now would be a good time to try these fancy GPUs, don’t you think?</p><h4 id="87f0"><strong class="markup--h4-strong">Training on a single GPU</strong></h4><p id="b22d">By chance, I’ve running this on a <a href="https://aws.amazon.com/fr/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" target="_blank">g2.8xlarge instance</a>. It has 4 NVidia GPUs ready to crunch data :)</p><pre class="graf--pre" id="f9e1">[ec2-user]$ nvidia-smi -L<br/>GPU 0: GRID K520 (UUID: GPU-5134e206-9b30-e1a8-a949-3d9e637a6465)<br/>GPU 1: GRID K520 (UUID: GPU-221cb85e-2d26-b615-b674-ad596a8c12ee)<br/>GPU 2: GRID K520 (UUID: GPU-ec4584ae-08e9-036f-d94a-ab60c52cf6fd)<br/>GPU 3: GRID K520 (UUID: GPU-9bd3fe35-ac18-5d1a-4fb1-d819c9265ec2)</pre><p class="graf-after--pre" id="7511">All it takes to switch from CPU to GPU is this. Amazing!</p><pre class="graf--pre" id="6318">#mod = mx.mod.Module(lenet)<br/>mod = mx.mod.Module(lenet, context=mx.gpu(0))</pre><p class="graf-after--pre" id="f81a">Here we go again.</p><pre class="graf--pre" id="8a0d">INFO:root:Epoch[0] Train-accuracy=0.906651<br/>INFO:root:Epoch[0] Time cost=3.452<br/>INFO:root:Epoch[1] Train-accuracy=0.972022<br/>INFO:root:Epoch[1] Time cost=3.455<br/>INFO:root:Epoch[2] Train-accuracy=0.980786<br/>INFO:root:Epoch[2] Time cost=3.450<br/>INFO:root:Epoch[3] Train-accuracy=0.985210<br/>INFO:root:Epoch[3] Time cost=3.454<br/>INFO:root:Epoch[4] Train-accuracy=0.987931<br/>INFO:root:Epoch[4] Time cost=3.454<br/>INFO:root:Epoch[5] Train-accuracy=0.989633<br/>INFO:root:Epoch[5] Time cost=3.453<br/>INFO:root:Epoch[6] Train-accuracy=0.991036<br/>INFO:root:Epoch[6] Time cost=3.449<br/>INFO:root:Epoch[7] Train-accuracy=0.992238<br/>INFO:root:Epoch[7] Time cost=3.451<br/>INFO:root:Epoch[8] Train-accuracy=0.993323<br/>INFO:root:Epoch[8] Time cost=3.453<br/>INFO:root:Epoch[9] Train-accuracy=0.994191<br/>INFO:root:Epoch[9] Time cost=3.452<br/>('accuracy', 0.9903846153846154)</pre><p class="graf-after--pre" id="0c9a">Nice! Training time has been <strong class="markup--p-strong">massively</strong> reduced. Accuracy is now <strong class="markup--p-strong">99+%</strong> thanks to the more sophisticated model.</p><p id="a51a">Did I mention that there are four GPUs in this box? How about using <strong class="markup--p-strong">more</strong> than one?</p><h4 id="7530"><strong class="markup--h4-strong">Training on multiple GPUs</strong></h4><p id="e4b9">Once again, this is pretty simple to set up.</p><pre class="graf--pre" id="530f">#mod = mx.mod.Module(lenet, context=mx.gpu(0))<br/>mod = mx.mod.Module(lenet, context=(mx.gpu(0), mx.gpu(1)))</pre><pre class="graf--pre graf-after--pre" id="43ed">INFO:root:Epoch[0] Train-accuracy=0.906701<br/>INFO:root:Epoch[0] Time cost=2.592<br/>INFO:root:Epoch[1] Train-accuracy=0.972055<br/>INFO:root:Epoch[1] Time cost=2.329<br/>INFO:root:Epoch[2] Train-accuracy=0.980819<br/>INFO:root:Epoch[2] Time cost=2.302<br/>INFO:root:Epoch[3] Train-accuracy=0.985193<br/>INFO:root:Epoch[3] Time cost=2.302<br/>INFO:root:Epoch[4] Train-accuracy=0.987981<br/>INFO:root:Epoch[4] Time cost=2.297<br/>INFO:root:Epoch[5] Train-accuracy=0.989583<br/>INFO:root:Epoch[5] Time cost=2.302<br/>INFO:root:Epoch[6] Train-accuracy=0.991119<br/>INFO:root:Epoch[6] Time cost=2.305<br/>INFO:root:Epoch[7] Train-accuracy=0.992238<br/>INFO:root:Epoch[7] Time cost=2.303<br/>INFO:root:Epoch[8] Train-accuracy=0.993273<br/>INFO:root:Epoch[8] Time cost=2.297<br/>INFO:root:Epoch[9] Train-accuracy=0.994174<br/>INFO:root:Epoch[9] Time cost=2.307</pre><p class="graf-after--pre" id="e6e2">We saved <strong class="markup--p-strong">50%</strong> of training time. Let’s go for three GPUs.</p><pre class="graf--pre" id="12a9">#mod = mx.mod.Module(lenet, context=(mx.gpu(0), mx.gpu(1)))<br/>mod = mx.mod.Module(lenet, context=(mx.gpu(0), mx.gpu(1), mx.gpu(2)))</pre><pre class="graf--pre graf-after--pre" id="b637">INFO:root:Epoch[0] Train-accuracy=0.906667<br/>INFO:root:Epoch[0] Time cost=1.938<br/>INFO:root:Epoch[1] Train-accuracy=0.972055<br/>INFO:root:Epoch[1] Time cost=1.924<br/>INFO:root:Epoch[2] Train-accuracy=0.980836<br/>INFO:root:Epoch[2] Time cost=1.916<br/>INFO:root:Epoch[3] Train-accuracy=0.985193<br/>INFO:root:Epoch[3] Time cost=1.903<br/>INFO:root:Epoch[4] Train-accuracy=0.987997<br/>INFO:root:Epoch[4] Time cost=1.910<br/>INFO:root:Epoch[5] Train-accuracy=0.989600<br/>INFO:root:Epoch[5] Time cost=1.910<br/>INFO:root:Epoch[6] Train-accuracy=0.991052<br/>INFO:root:Epoch[6] Time cost=1.912<br/>INFO:root:Epoch[7] Train-accuracy=0.992288<br/>INFO:root:Epoch[7] Time cost=1.921<br/>INFO:root:Epoch[8] Train-accuracy=0.993339<br/>INFO:root:Epoch[8] Time cost=1.934<br/>INFO:root:Epoch[9] Train-accuracy=0.994157<br/>INFO:root:Epoch[9] Time cost=1.937<br/>('accuracy', 0.9904847756410257)</pre><p class="graf-after--pre" id="604d">Another <strong class="markup--p-strong">20%</strong> saved. Training time is now only 50% more than what it was for the CPU-version of the multi-layer perceptron.</p><p id="e155">Adding a fourth GPU won’t help. Yes, I tried :) Anyway, we’re pretty happy with our model, so let’s <strong class="markup--p-strong">save</strong> it for future use.</p><h4 id="7dc1">Saving our model</h4><p id="e94f">Saving a model just requires a file name and an epoch number.</p><pre class="graf--pre" id="450c">mod.save_checkpoint("lenet", 10)</pre><p class="graf-after--pre" id="bb33">This creates two files (which you should now be familiar with):</p><ul class="postList"><li id="6f8f">the <strong class="markup--li-strong">symbol</strong> file, containing the model definition (3.5KB)</li><li id="5f9f">the <strong class="markup--li-strong">parameter</strong> file, containing all our trained parameters (1.7MB)</li></ul><pre class="graf--pre" id="0a9a">$ ls lenet*<br/>lenet-0010.params  lenet-symbol.json</pre><h4 class="graf-after--pre" id="0522">Reusing our model</h4><p id="7bfe">Just like we did in previous articles, we’re now able to <strong class="markup--p-strong">load</strong> this pre-trained model.</p><pre class="graf--pre" id="1f79">lenet, arg_params, aux_params = mx.model.load_checkpoint("lenet", 10)<br/>mod = mx.mod.Module(lenet)<br/>mod.bind(for_training=False, data_shapes=[('data', (1,1,28,28))])<br/>mod.set_params(arg_params, aux_params)</pre><p class="graf-after--pre" id="23da">Here are the ugly digits I created with Paintbrush :)</p><figure id="7171"><img class="graf-image" src="image07.webp"/><figcaption>My home-made digits</figcaption></figure><p id="56d3">I saved them as a 28x28 images, which I can now load as <em class="markup--p-em">numpy</em> arrays. I need to normalize pixels values and add two dimensions to reshape the array from (28, 28) to (1, 1, 28, 28) : batch size of one, one channel (greyscale), 28 x 28 pixels.</p><pre class="graf--pre" id="c166">def loadImage(filename):<br/>        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)<br/>        img = img / 255<br/>        print img<br/>        img = np.expand_dims(img, axis=0)<br/>        img = np.expand_dims(img, axis=0)<br/>        return mx.nd.array(img)</pre><p class="graf-after--pre" id="f7d9">We’ll predict image by image. To avoid building a data iterator, I’ll use the same trick we’ve seen before (using a <em class="markup--p-em">namedtuple</em> to provide a data attribute).</p><pre class="graf--pre" id="7d91">def predict(model, filename):<br/>        array = loadImage(filename)<br/>        Batch = namedtuple('Batch', ['data'])<br/>        mod.forward(Batch([array]))<br/>        pred = mod.get_outputs()[0].asnumpy()<br/>        return pred</pre><p class="graf-after--pre" id="f97d">Now we’re ready. Let check these digits!</p><pre class="graf--pre" id="bcea">np.set_printoptions(precision=3, suppress=True)</pre><pre class="graf--pre graf-after--pre" id="64b4">mod = loadModel()<br/>print predict(mod, "./0.png")<br/>print predict(mod, "./1.png")<br/>print predict(mod, "./2.png")<br/>print predict(mod, "./3.png")<br/>print predict(mod, "./4.png")<br/>print predict(mod, "./5.png")<br/>print predict(mod, "./6.png")<br/>print predict(mod, "./7.png")<br/>print predict(mod, "./8.png")<br/>print predict(mod, "./9.png")</pre><p class="graf-after--pre" id="1c6a">And here are the results.</p><pre class="graf--pre" id="39d6">[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]<br/>[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]<br/>[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]<br/>[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]<br/>[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]<br/>[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]<br/>[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]<br/>[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]<br/>[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]<br/>[[ 0.  0.  0.  0.001  0.009  0.  0. 0.002  0. 0.988]]</pre><p class="graf-after--pre" id="9aae">Wow. Hardly any doubt on the first 9 digits (probabilities are <strong class="markup--p-strong">99.99+%</strong>). Only my ugly 9 scores lower :)</p><p id="9c3c">Well, who thought that we’d have so much fun and that we’d cover so much ground using the MNIST dataset? Code and images are available on <a href="https://github.com/juliensimon/aws/tree/master/mxnet/mnist" target="_blank">Github</a>. Hopefully, this will get you started on building and training networks on your own data.</p><p id="7791">That’s it for today. Stay tuned for part 2 where we’ll look at another data set!</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="155c">Next:</p><ul class="postList"><li id="7aa4"><a href="https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c" target="_blank">Part 2</a> : Training on CIFAR-10</li><li id="1a83"><a href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" target="_blank">Part 3</a> : CIFAR-10 redux</li><li id="d482"><a href="https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7" target="_blank">Part 4</a>: Distributed training</li><li id="c42d"><a href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" target="_blank">Part 5</a>: Distributed training, EFS edition</li></ul><figure id="f741"><iframe frameborder="0" height="350" scrolling="no" src="https://upscri.be/3e59ef?as_embed=true" width="700"></iframe></figure></div><div><figure id="9159"><img class="graf-image" src="image03.webp"/></figure></div><div><figure id="76a5" style="width: 33.333%;"><a href="https://chatbotslife.com/bot-communities-mastermind-group-d2dae9876709#.53x0py6ou" target="_blank"><img class="graf-image" src="image05.webp"/></a></figure><figure class="graf--layoutOutsetRowContinue" id="88b8" style="width: 33.333%;"><a href="https://m.me/ChatbotsLife" target="_blank"><img class="graf-image" src="image02.webp"/></a></figure><figure class="graf--layoutOutsetRowContinue" id="358a" style="width: 33.333%;"><a href="https://chatbotslife.com/how-to-get-a-free-chatbot-b1fb9dfe109#.z9dtp2sy0" target="_blank"><img class="graf-image" src="image01.webp"/></a></figure></div></div></section>
</section>
</article></body></html>