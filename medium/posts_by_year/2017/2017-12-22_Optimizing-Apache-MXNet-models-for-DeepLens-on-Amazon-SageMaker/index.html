<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Optimizing Apache MXNet models for DeepLens on Amazon SageMaker</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="02f1">Optimizing Apache MXNet models for DeepLens on Amazon SageMaker</h3><p id="23fe">In a <a href="https://medium.com/@julsimon/exploring-ahem-aws-deeplens-fcad551886ef" target="_blank">previous post</a>, we explored how <a href="https://mxnet.apache.org/" target="_blank">Apache MXNet</a> models are in fact optimized for <a href="https://aws.amazon.com/deeplens/" target="_blank">AWS DeepLens</a> thanks to the <a href="https://software.intel.com/en-us/inference-engine-devguide" target="_blank">Intel Deep Learning Inference Engine</a>.</p><p id="8fac">In this article, I will show you how to <strong class="markup--p-strong">automate</strong> this process during a typical training process we would perform on Amazon SageMaker.</p><p id="f38b">As usual, all code is available in a <a href="https://github.com/juliensimon/dlnotebooks/blob/master/mxnet/04%20-%20Optimizing%20an%20MXNet%20model%20for%20AWS%20DeepLens.ipynb" target="_blank">Jupyter notebook</a> on Github.</p><figure id="2d61"><img class="graf-image" src="image01.webp"/></figure><h4 id="b590">Downloading the Intel Deep Learning Deployment Toolkit</h4><p id="2b93">First of all, we need to <a href="https://software.seek.intel.com/deep-learning-deployment" target="_blank">register</a> and get a download link for the toolkit. Then, let’s grab the toolkit and extract it.</p><pre class="graf--pre" id="9133">$ wget TOOLKIT_NAME.tgz<br/>$ tar xvfz TOOLKIT_NAME.tgz<br/>$ cd l_deeplearning_deploymenttoolkit_2017.1.0.5852</pre><p class="graf-after--pre" id="e9b4">In order to be able to perform <strong class="markup--p-strong">unattended installation</strong>, let’s edit the configuration file named <em class="markup--p-em">silent.cfg</em>. We simply need to set:</p><pre class="graf--pre" id="99a8">ACCEPT_EULA=accept</pre><p class="graf-after--pre" id="e59b">Now, let’s archive the toolkit and copy it to an S3 bucket, which we’ll use for deployment to SageMaker instances.</p><pre class="graf--pre" id="38fb">cd ..<br/>tar cvfz toolkit.tgz l_deeplearning_deploymenttoolkit_2017.1.0.5852<br/>aws s3 cp toolkit.tgz s3://jsimon-public-us</pre><p class="graf-after--pre" id="f175">Ok, now we’re ready to deploy the toolkit to SageMaker and convert models. Let’s open a notebook instance and start a new Jupyter notebook.</p><h4 id="9d33">Defining paths and parameters</h4><p id="6f06">First, let’s define S3 locations for the toolkit and the model to convert: if you used Amazon SageMaker to train the model (and why wouldn’t you?), your model is already in S3 anyway :)</p><blockquote class="graf--blockquote" id="f032">In order to make these variables visible from all cells, we’re using the %env Jupyter magic.</blockquote><pre class="graf--pre graf-after--blockquote" id="896e">%env TOOLKIT_BUCKET=s3://jsimon-public-us/<br/>%env TOOLKIT_NAME=toolkit.tgz<br/>%env TOOLKIT_DIR=l_deeplearning_deploymenttoolkit_2017.1.0.5852</pre><pre class="graf--pre graf-after--pre" id="bc48">%env MODEL_BUCKET=s3://jsimon-public-us/<br/>%env MODEL_NAME=Inception-BN</pre><p class="graf-after--pre" id="f84d">Next, we have to set the location where the Intel Toolkit will be installed, as well the optimization parameters we’d like to use.</p><pre class="graf--pre" id="6b06">%env OPT_DIR=/opt/intel/deeplearning_deploymenttoolkit/deployment_tools/model_optimizer/model_optimizer_mxnet<br/>%env OPT_PRECISION=FP16<br/>%env OPT_FUSE=YES</pre><h4 class="graf-after--pre" id="9d6f">Installing the toolkit</h4><p id="5f85">Now we’re ready to download and install the toolkit. A few lines of script is all it takes.</p><pre class="graf--pre" id="370e">aws s3 cp $TOOLKIT_BUCKET$TOOLKIT_NAME .<br/>tar xfz $TOOLKIT_NAME<br/>cd $TOOLKIT_DIR<br/>chmod 755 install.sh<br/>sudo ./install.sh -s silent.cfg </pre><p class="graf-after--pre" id="4ba2">The next step is to install <strong class="markup--p-strong">Python dependencies</strong> required by the Model Converter.</p><p id="88dc">Conda makes it easy to create <strong class="markup--p-strong">isolated environments</strong>, so let’s build one for the Intel toolkit. This will save us from clobbering the environment we use for training.</p><p id="cf8b">Of course, you’ll only need to run these steps once per instance. The second (slightly cryptic) line is required to see the new kernel listed in the Jupyter menu.</p><pre class="graf--pre" id="bf10">conda create -n intel_toolkit -y</pre><pre class="graf--pre graf-after--pre" id="c836">python -m ipykernel install --user --name intel_toolkit --display-name "intel_toolkit"</pre><pre class="graf--pre graf-after--pre" id="ed64">source activate intel_toolkit<br/>cd $OPT_DIR<br/>pip install -r requirements.txt</pre><h4 class="graf-after--pre" id="2ca4">Downloading the model</h4><p id="a6b7">Nothing fancy here: simply copy the trained model from its S3 location.</p><pre class="graf--pre" id="0155">aws s3 cp $MODEL_BUCKET$MODEL_NAME"-symbol.json" .<br/>aws s3 cp $MODEL_BUCKET$MODEL_NAME"-0000.params" .</pre><h4 class="graf-after--pre" id="b9a5"><strong class="markup--h4-strong">Converting the model</strong></h4><p id="9307">We saw how to do this in the <a href="https://medium.com/@julsimon/exploring-ahem-aws-deeplens-fcad551886ef" target="_blank">previous post</a>. More information on parameters <a href="https://software.intel.com/en-us/inference-engine-devguide-converting-your-mxnet-model" target="_blank">here</a>.</p><pre class="graf--pre" id="3c02">python $OPT_DIR/mo_mxnet_converter.py \<br/> --models-dir . --output-dir . --model-name $MODEL_NAME \<br/>--precision $OPT_PRECISION --fuse $OPT_FUSE</pre><p class="graf-after--pre" id="117a">And we’re done! Now you can copy the converted model back to S3 and deploy it to Deep Lens.</p><h4 id="3453">Conclusion</h4><p id="fcce">This was surprisingly easy, don’t you think? Now you can optimize your models and deploy them to DeepLens. Please let me know about your projets, happy to share and retweet!</p><p id="42e5">As always, thank you for reading.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="0fdc">The soundtrack to this post was Judas Priest.1983…. Spikes, leather, spandex: legendary!</p><figure id="4adb"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/jRjrzpa38-0?feature=oembed" width="640"></iframe></figure></div></div></section>
</section>
</article></body></html>