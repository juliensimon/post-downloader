<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>An introduction to the MXNet API — part 4</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="bc2f">An introduction to the MXNet API — part 4</h3><p id="8749">In <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" target="_blank">part 3</a>, we built and trained our first neural network. We now know enough to take on more advanced examples.</p><p id="0d86">State of the art Deep Learning models are insanely complex. They have <strong class="markup--p-strong">hundreds of layers</strong> and take days — if not weeks — to train on vast amounts of data. Building and tuning these models requires a lot of expertise.</p><p id="7cce">Fortunately, using these models is much simpler and only requires <strong class="markup--p-strong">a few lines of code</strong>. In this article, we’re going to work with a pre-trained model for image classification called <strong class="markup--p-strong">Inception v3</strong>.</p><h4 id="61f5">Inception v3</h4><p id="fc21">Published in December 2015, <a href="https://arxiv.org/abs/1512.00567" target="_blank">Inception v3</a> is an evolution of the <a href="https://arxiv.org/abs/1409.4842" target="_blank">GoogleNet</a> model (which won the <a href="http://image-net.org/challenges/LSVRC/2014/" target="_blank">2014 ImageNet challenge</a>). We won’t go into the details of the research paper, but paraphrasing its conclusion, Inception v3 is <strong class="markup--p-strong">15–25% more accurate</strong> than the best models available at the time, while being <strong class="markup--p-strong">six times cheaper computationally</strong> and using at least <strong class="markup--p-strong">five times less parameters</strong> (i.e. less RAM is required to use the model).</p><p id="14df">Quite a beast, then. So how do we put it to work?</p><h4 id="8683">The MXNet model zoo</h4><p id="b13f">The <a href="http://mxnet.io/model_zoo/" target="_blank">model zoo</a> is a collection of <strong class="markup--p-strong">pre-trained models</strong> ready for use. You’ll find the <strong class="markup--p-strong">model definition</strong>, the <strong class="markup--p-strong">model parameters</strong> (i.e. the neuron weights) and instructions (maybe).</p><p id="10bd">Let’s download the definition and the parameters (you may have to change the filename). Feel free to open the first file: you’ll see the definition of all the layers. The second one is a binary file, leave it alone ;)</p><pre class="graf--pre" id="b850">$ wget <a class="markup--pre-anchor" href="http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json" rel="nofollow noopener noopener" target="_blank">http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json</a></pre><pre class="graf--pre graf-after--pre" id="4578">$ wget <a class="markup--pre-anchor" href="http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params" rel="nofollow noopener" target="_blank">http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params</a></pre><pre class="graf--pre graf-after--pre" id="3286">$ mv Inception-BN-0126.params Inception-BN-0000.params</pre><p class="graf-after--pre" id="51c6">Since this model has been trained on the <a href="http://www.image-net.org/" target="_blank">ImageNet</a> data set, we also need to download the corresponding list of image <strong class="markup--p-strong">categories</strong> (1000 of them).</p><pre class="graf--pre" id="c1c8">$ wget <a class="markup--pre-anchor" href="http://data.dmlc.ml/models/imagenet/synset.txt" rel="nofollow noopener" target="_blank">http://data.dmlc.ml/models/imagenet/synset.txt</a></pre><pre class="graf--pre graf-after--pre" id="17df">$ wc -l synset.txt<br/>    1000 synset.txt</pre><pre class="graf--pre graf-after--pre" id="ba49">$ head -5 synset.txt<br/>n01440764 tench, Tinca tinca<br/>n01443537 goldfish, Carassius auratus<br/>n01484850 great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias<br/>n01491361 tiger shark, Galeocerdo cuvieri<br/>n01494475 hammerhead, hammerhead shark</pre><p class="graf-after--pre" id="cf86">Ok, done. Now let’s get to work.</p><h4 id="ba0b">Loading the model</h4><p id="33c2">Here’s what we need to do:</p><ul class="postList"><li id="7c6f">load the model from its saved state: MXNet calls this a <strong class="markup--li-strong">checkpoint</strong>. In return, we get the input <em class="markup--li-em">Symbol</em> and the model parameters.</li></ul><pre class="graf--pre" id="4276">import mxnet as mx<br/><br/>sym, arg_params, aux_params = mx.model.load_checkpoint('Inception-BN', 0)</pre><ul class="postList"><li class="graf-after--pre" id="8a1c">create a new <em class="markup--li-em">Module</em> and assign it the input <em class="markup--li-em">Symbol</em>. We could also a <em class="markup--li-em">context</em> parameter indicating where we want to run the model: the default value is <em class="markup--li-em">cpu(0)</em>, but we’d use <em class="markup--li-em">gpu(0)</em> to run this on a GPU.</li></ul><pre class="graf--pre" id="9319">mod = mx.mod.Module(symbol=sym)</pre><ul class="postList"><li class="graf-after--pre" id="1ee7">bind the input <em class="markup--li-em">Symbol</em> to input data. We’ll call it ‘data’ because that’s its name in the <strong class="markup--li-strong">input layer</strong> of the network (look at the first few lines of the JSON file).</li><li id="767a">define the <strong class="markup--li-strong">shape</strong> of ‘data’ as 1 x 3 x 224 x 224. Don’t panic ;) ‘224 x 224’ is the image resolution, that’s how the model was trained. ‘3’ is the number of channels : red, green and blue (in this order). ‘1’ is the batch size: we’ll predict one image at a time.</li></ul><pre class="graf--pre" id="0911">mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])</pre><ul class="postList"><li class="graf-after--pre" id="0ddf">set the model parameters.</li></ul><pre class="graf--pre" id="35b2">mod.set_params(arg_params, aux_params)</pre><p class="graf-after--pre" id="7287">That’s all it takes. Four lines of code! Now it’s take to push some data in there and see what happens. Well… not quite yet.</p><h4 id="61d4">Preparing our data</h4><p id="82d0">Data preparation: making our life miserable since the Seventies… From relational databases to Machine Learning to Deep Learning, nothing has really changed in that respect. It’s boring but necessary. Let’s get it done.</p><p id="b0c2">Remember that the model expects a 4-dimension <em class="markup--p-em">NDArray</em> holding the red, green and blue channels of a single 224 x 224 image. We’re going to use the popular <a href="http://www.opencv.org" target="_blank">OpenCV</a> library to build this <em class="markup--p-em">NDArray</em> from our input image. If you don’t have OpenCV installed, running “<em class="markup--p-em">pip install opencv-python</em>” should be enough in most cases :)</p><p id="c7e0">Here are the steps:</p><ul class="postList"><li id="0c55"><strong class="markup--li-strong">read</strong> the image: this will return a <em class="markup--li-em">numpy</em> array shaped as (image height, image width, 3), with the three channels in <strong class="markup--li-strong">BGR</strong> order (blue, green and red).</li></ul><pre class="graf--pre" id="d3ac">img = cv2.imread(filename)</pre><ul class="postList"><li class="graf-after--pre" id="a7b7"><strong class="markup--li-strong">convert</strong> the image to <strong class="markup--li-strong">RGB</strong>.</li></ul><pre class="graf--pre" id="801c">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</pre><ul class="postList"><li class="graf-after--pre" id="eb69"><strong class="markup--li-strong">resize</strong> the image to <strong class="markup--li-strong">224 x 224</strong>.</li></ul><pre class="graf--pre" id="b1c4">img = cv2.resize(img, (224, 224,))</pre><ul class="postList"><li class="graf-after--pre" id="95a4"><strong class="markup--li-strong">reshape</strong> the array from (image height, image width, 3) to (3, image height, image width).</li></ul><pre class="graf--pre" id="abd9">img = np.swapaxes(img, 0, 2)<br/>img = np.swapaxes(img, 1, 2)</pre><ul class="postList"><li class="graf-after--pre" id="d6b8">add a <strong class="markup--li-strong">fourth dimension</strong> and build the <em class="markup--li-em">NDArray</em></li></ul><pre class="graf--pre" id="3f91">img = img[np.newaxis, :]<br/>array = mx.nd.array(img)</pre><pre class="graf--pre graf-after--pre" id="ddcc">&gt;&gt;&gt; print array.shape<br/>(1L, 3L, 224L, 224L)</pre><p class="graf-after--pre" id="4291">Dizzy? Let’s look at an example. Here’s our input picture.</p><figure id="845c"><img class="graf-image" src="image01.webp"/><figcaption>Input picture 448x336 (Source: metaltraveller.com)</figcaption></figure><p id="f1c2">Once processed, this picture has been resized and split into RGB channels stored in <em class="markup--p-em">array[0] </em>(<a href="https://gist.github.com/juliensimon/c62742b200396b4eadd8229a22c4cf0b" target="_blank">here</a> is the code used to generate the images below).</p><figure id="c90c"><img class="graf-image" src="image03.webp"/><figcaption>array[0][0] : 224x224 red channel</figcaption></figure><figure id="654d"><img class="graf-image" src="image02.webp"/><figcaption>array[0][1] : 224x224 green channel</figcaption></figure><figure id="ba70"><img class="graf-image" src="image04.webp"/><figcaption>array[0][2] : 224x224 blue channel</figcaption></figure><p id="47d8">If batch size was higher than 1, then we would have a second image in <em class="markup--p-em">array[1]</em>, a third in <em class="markup--p-em">array[2]</em> and so on.</p><p id="b7ed">Was this fun or what? Now let’s predict!</p><h4 id="c119">Predicting</h4><p id="1ac8">You may remember from <a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8" target="_blank">part 3</a> that a <em class="markup--p-em">Module</em> object must feed data to a model in <strong class="markup--p-strong">batches</strong>: the common way to do this is to use a <strong class="markup--p-strong">data iterator</strong> (specifically, we used an <em class="markup--p-em">NDArrayIter</em> object).</p><p id="948a">Here, we’d like to predict a <strong class="markup--p-strong">single</strong> image, so although we could use data iterator, it’d probably be overkill. Instead, we’re going to create a <strong class="markup--p-strong">named tuple</strong>, called <em class="markup--p-em">Batch</em>, which will act as a fake iterator by returning our input <em class="markup--p-em">NDArray </em>when its <em class="markup--p-em">data</em> attribute is referenced.</p><pre class="graf--pre" id="125c">from collections import namedtuple<br/>Batch = namedtuple('Batch', ['data'])</pre><p class="graf-after--pre" id="9493">Now we can pass this “batch” to the model and let it predict.</p><pre class="graf--pre" id="b399">mod.forward(Batch([array]))</pre><p class="graf-after--pre" id="c8c8">The model will output an <em class="markup--p-em">NDArray</em> holding the <strong class="markup--p-strong">1000 probabilities</strong>, corresponding to the 1000 categories. It has only one line since batch size is equal to 1.</p><pre class="graf--pre" id="0fe4">prob = mod.get_outputs()[0].asnumpy()</pre><pre class="graf--pre graf-after--pre" id="6402">&gt;&gt;&gt; prob.shape<br/>(1, 1000)</pre><p class="graf-after--pre" id="8631">Let’s turn this into an array with <em class="markup--p-em">squeeze</em>(). Then, using <em class="markup--p-em">argsort</em>(), we’re creating a second array holding the <strong class="markup--p-strong">index</strong> of these probabilities sorted in <strong class="markup--p-strong">descending order</strong>.</p><pre class="graf--pre" id="40c6">prob = np.squeeze(prob)</pre><pre class="graf--pre graf-after--pre" id="d118">&gt;&gt;&gt; prob.shape<br/>(1000,)<br/>&gt;&gt; prob<br/>[  4.14978594e-08   1.31608676e-05   2.51907986e-05   2.24045834e-05<br/>   2.30327873e-06   3.40798979e-05   7.41563645e-06   3.04062659e-08 <em class="markup--pre-em">etc.</em></pre><pre class="graf--pre graf-after--pre" id="1669">sortedprob = np.argsort(prob)[::-1]</pre><pre class="graf--pre graf-after--pre" id="6ee2">&gt;&gt; sortedprob.shape<br/>(1000,)</pre><p class="graf-after--pre" id="8b91">According to the model, the most likely category for this picture is <strong class="markup--p-strong">#546</strong> , with a probability of <strong class="markup--p-strong">58%</strong>.</p><pre class="graf--pre" id="82c1">&gt;&gt; sortedprob<br/>[546 819 862 818 542 402 650 420 983 632 733 644 513 875 776 917 795<br/><em class="markup--pre-em">etc.<br/></em>&gt;&gt; prob[546]<br/>0.58039135</pre><p class="graf-after--pre" id="0915">Let’s find the name of this category. Using the <em class="markup--p-em">synset.txt</em> file, we can build a <strong class="markup--p-strong">list of categories</strong> and find the one at index 546.</p><pre class="graf--pre" id="26fa">synsetfile = open('synset.txt', 'r')<br/>categorylist = []<br/>for line in synsetfile:<br/>  categorylist.append(line.rstrip())</pre><pre class="graf--pre graf-after--pre" id="708d">&gt;&gt;&gt; categorylist[546]<br/>'n03272010 electric guitar'</pre><p class="graf-after--pre" id="6531">What about the second highest category?</p><pre class="graf--pre" id="1610">&gt;&gt;&gt; prob[819]<br/>0.27168664<br/>&gt;&gt;&gt; categorylist[819]<br/>'n04296562 stage</pre><p class="graf-after--pre" id="32ce">That’s pretty good, don’t you think?</p><p id="391a">So there you go. Now you know how to use a <strong class="markup--p-strong">pre-trained, state of the art model</strong> for image classification. All it took was <strong class="markup--p-strong">4 lines of code</strong>… and the rest was just data preparation.</p><p id="ff06">You’ll find the full code below. Have fun and stay tuned :D</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="d037">Next:</p><ul class="postList"><li id="2b71"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db" target="_blank">Part 5</a>: More pre-trained models (VGG16 and ResNet-152)</li><li id="9a63"><a href="https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87" target="_blank">Part 6</a>: Real-time object detection on a Raspberry Pi (and it speaks, too!)</li></ul><figure id="c2f1"><script src="https://gist.github.com/juliensimon/4a5e999d9c851f0b036ab3870eccd59d.js"></script></figure></div></div></section>
</section>
</article></body></html>