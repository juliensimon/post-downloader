<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Training MXNet — part 4: distributed training</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="dcfe">Training MXNet — part 4: distributed training</h3><p id="1a5f">In <a href="https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0" target="_blank">part 3</a>, we worked with the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow noopener nofollow noopener noopener" target="_blank">CIFAR-10</a> data set and learned how to tweak optimisation parameters. We ended up training a 110-layer ResNext model using all 4 GPUs of a <a href="https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/" rel="nofollow noopener noopener" target="_blank">g2.8xlarge</a> instance… which took about 12 hours.</p><p id="dd69">In this article, I’ll show you how to use multiple instances to dramatically speed up training. Buckle up!</p><figure id="6c24"><img class="graf-image" src="image07.webp"/><figcaption>Training CIFAR-10 on 4 instances and 32 GPUs. Read on!</figcaption></figure><h4 id="372f">Creating the master node</h4><p id="522f">We’re going to work with <a href="https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/" target="_blank">p2.8xlarge</a> instances running the <strong class="markup--p-strong">Deep Learning AMI</strong>, <a href="https://aws.amazon.com/about-aws/whats-new/2017/04/deep-learning-ami-for-ubuntu-v-1-3-apr-2017-now-supports-caffe2/" target="_blank">Ubuntu edition</a>. However, you can easily replicate this with any kind of EC2 instance or even on a bunch of PCs running under your desk :)</p><p id="98da">Let’s get started. We’re going to configure the master node the way we like it and then we’ll <strong class="markup--p-strong">clone</strong> it to add more instances to our MXNet cluster. The first step is to go to the Marketplace section of the EC2 console and locate the Deep Learning AMI.</p><figure id="1a71"><img class="graf-image" src="image02.webp"/></figure><p id="11b8">Then, select the instance type you’d like to use. Please be mindful of instance costs: a p2.8xlarge is going to cost $7.20 per hour. Don’t worry, you can actually use <strong class="markup--p-strong">any instance type</strong>, as MXNet is able to use either the CPU(s) or the GPU(s) of the instance. Obviously, GPU instances will be much faster than t2.micros :)</p><p id="eb72">A few more clicks and you’re done. Just make sure the <strong class="markup--p-strong">SSH port</strong> is open and that you have created a new <strong class="markup--p-strong">key pair</strong> for the instance (let’s call it <em class="markup--p-em">ec2</em>). After a few minutes, you can <em class="markup--p-em">ssh</em> into the master node using the <em class="markup--p-em">ubuntu</em> user (not the <em class="markup--p-em">ec2-user</em>).</p><h4 id="dbff">Enabling distributed training in MXNET</h4><p id="4dde">By default, distributed training is not enabled in the source distribution, which means we probably have to rebuild MXNet from source. If your build already includes distributed training, you can skip this section.</p><p id="f41d">The Deep Learning AMI includes the MXNet sources: we just have to make them our own and refresh them to the latest stable version (<strong class="markup--p-strong">0.9.5</strong> at the time of writing).</p><figure id="a65b"><script src="https://gist.github.com/juliensimon/563af7489039509b78067e39989952dd.js"></script></figure><p id="3bc7">Then, we need to configure our <strong class="markup--p-strong">build options</strong>. The last one actually enables distributed training.</p><figure id="f935"><script src="https://gist.github.com/juliensimon/23d1464892c46c0a047641036225a27e.js"></script></figure><p id="f129">Now we can build and install the library. No need to add dependencies, as they’re already included in the AMI. I’m running a parallel make on 32 cores because that’s what a p2.8xlarge has.</p><figure id="c360"><script src="https://gist.github.com/juliensimon/d01dcdb9a6e708a7fc9dbf81634f0f6a.js"></script></figure><p id="fc44">Once the library is installed, it’s a good idea to run a quick Python check.</p><figure id="7d78"><img class="graf-image" src="image03.webp"/></figure><p id="c53b">Ok, this looks good. Let’s move on.</p><h4 id="26e8">Opening ports for distributed training</h4><p id="2c26">The master node and the worker nodes need to talk to one another to share the <strong class="markup--p-strong">data set</strong> as well as <strong class="markup--p-strong">training results</strong>. Thus, we need to alter the configuration of our security group to allow this.</p><p id="8e0a">The absolute simplest way to do this is to allow <strong class="markup--p-strong">all TCP</strong> communication between instances of the MXNet cluster, i.e. instances using the <strong class="markup--p-strong">same security group</strong>.</p><p id="e624">To do this, go to the EC2 console and edit the inbound rules of the security group of the master node. Add a rule allowing <strong class="markup--p-strong">all TCP traffic</strong> and use the actual name of the security group to <strong class="markup--p-strong">restrict</strong> source traffic.</p><figure id="a6d8"><img class="graf-image" src="image04.webp"/></figure><p id="f55c">Our instance is now ready. Let’s create the worker nodes.</p><h4 id="e817">Creating the worker nodes</h4><p id="bcd9">We’re going to create a <strong class="markup--p-strong">new AMI</strong> based on the master node. Then, we’ll use it to launch the workers. Locate your instance in the EC2 console and create an image.</p><figure id="004a"><img class="graf-image" src="image05.webp"/></figure><p id="65a2">After a few minutes, you’ll see the new AMI in the “Images” section of the EC2 console. You can now use it to launch your worker nodes.</p><p id="b440">Nothing complicated here: select the <strong class="markup--p-strong">instance type</strong>, the <strong class="markup--p-strong">number</strong> of instances you’d like to launch (3 in my case) and <strong class="markup--p-strong">the same security group</strong> as the master node.</p><p id="848b">A few more minutes and your instances are ready.</p><figure id="bc7f"><img class="graf-image" src="image08.webp"/></figure><p id="dc4d">Lovely. Write down the private IP adresses of each instance, we’re going to need them in a second.</p><h4 id="1568">Configuring the cluster</h4><p id="cdde">Let’s log in to the master node, move to the <em class="markup--p-em">tools</em> directory and look at the launcher.</p><figure id="df65"><img class="graf-image" src="image06.webp"/></figure><p id="1386">This is the tool we’ll use to start training on all nodes (master node included). It does two things:</p><ul class="postList"><li id="4415">using rsync, <strong class="markup--li-strong">copy the data set</strong> in <em class="markup--li-em">/tmp/mxnet</em> on each node. Alternatively, we could avoid this by sharing the data set across the nodes with home-made NFS or <a href="https://aws.amazon.com/efs/" target="_blank">Amazon EFS</a>.</li><li id="7318">using ssh, <strong class="markup--li-strong">run the python script</strong> that starts training. As you can see, other protocols are available, but we won’t look at them today.</li></ul><h4 id="e50b">Creating the hosts file</h4><p id="e440"><em class="markup--p-em">launch.py</em> needs the private IP address of all nodes (including the master node) to be declared in a file. It should look something like this.</p><figure id="6053"><img class="graf-image" src="image01.webp"/></figure><h4 id="25c8">Configuring SSH</h4><p id="9860">We need password-less <em class="markup--p-em">ssh</em> access between the master node and the worker nodes. If you already have this in place, you can skip this section.</p><p id="d3e2">We’ll keep things simple by creating a new key pair on our local computer and distributing it across the cluster.</p><blockquote class="graf--blockquote" id="1bc1"><strong class="markup--blockquote-strong">PLEASE</strong> do not reuse the <em class="markup--blockquote-em">ec2</em> key pair, it’s bad practice. Also, some of you may be tempted to bake the key pair in the AMI to avoid distributing it to all instances, but I would recommend against doing that since it means storing the private key on all nodes instead of just the master node. And <a class="markup--blockquote-anchor" href="https://heipei.github.io/2015/02/26/SSH-Agent-Forwarding-considered-harmful/" target="_blank">ssh agent forwarding isn’t great either</a>.</blockquote><figure class="graf-after--blockquote" id="d7b4"><script src="https://gist.github.com/juliensimon/eee83def5c7a78d95e704b646ab37734.js"></script></figure><p id="4383">Next, still from our local computer, we’re going to copy the <strong class="markup--p-strong">public key</strong> to all nodes (including the master node) and the <strong class="markup--p-strong">private key</strong> to the master node only.</p><figure id="63fc"><script src="https://gist.github.com/juliensimon/f91d5936537fe1f011aa0e8688ce3e16.js"></script></figure><p id="46a3">Finally, on the master node, we’ll start <em class="markup--p-em">ssh-agent</em> and add the <em class="markup--p-em">mxnet</em> identity.</p><figure id="b6f1"><script src="https://gist.github.com/juliensimon/8035ad6829a3a2767d9e7317c2efd682.js"></script></figure><p id="03ee">You should now be able to log in <strong class="markup--p-strong">from the master node to each worker node</strong> (including the master node itself). Please make sure that this is working properly before going on.</p><figure id="91ab"><script src="https://gist.github.com/juliensimon/2593cc3174b503d3359c3b25f723eaef.js"></script></figure><p id="8f38">If it does, you’re ready to train, buddy :)</p><h4 id="7a67">Launching distributed training</h4><p id="034f">Here’s the magic command: 4 nodes listed in the <em class="markup--p-em">hosts</em> file will receive a copy of the data set in <em class="markup--p-em">/tmp/mxnet </em>via <em class="markup--p-em">rsync</em>. Then, the master node will run the <em class="markup--p-em">train_cifar10.py</em> script on each node, training a 110-layer ResNext model on all 8 GPUs.</p><figure id="663c"><script src="https://gist.github.com/juliensimon/da33a2aa889782bf13954d7447164498.js"></script></figure><blockquote class="graf--blockquote" id="cdf1">If you’re running on CPU instances, just remove the gpus parameters.</blockquote><p class="graf-after--blockquote" id="b200">The PS_VERBOSE variable will output extra information. Very useful in case something goes wrong ;)</p><p id="ac07">You can check progress by logging in on the different nodes and running the ‘<em class="markup--p-em">nvidia-smi -l</em>’ command.</p><figure id="2291"><img class="graf-image" src="image07.webp"/></figure><p id="dd61">So how fast is this? As I mentioned before, it took about 12 hours to run 300 epochs on the 4 GPUs of a g2.8xlarge instance. The combined 32 GPUs of the 4 p2.8xlarge instances did it in <strong class="markup--p-strong">91 minutes!</strong></p><p id="a1f7">That’s an <strong class="markup--p-strong">8x speedup</strong>, which kind of makes sense since we have <strong class="markup--p-strong">8x more GPUs</strong>. I had read about it and now I see it with my own eyes: <strong class="markup--p-strong">linear scaling</strong> indeed! This makes me want to push it to 256 GPUs: it would only require 16 p2.16xlarge after all :D</p><p id="6c8d">Last but not least, my colleagues Naveen Swamy and Joseph Spisak wrote a very interesting <a href="https://aws.amazon.com/blogs/compute/distributed-deep-learning-made-easy/" target="_blank">blog post</a> on how to automate most of this stuff using AWS CloudFormation. This is definitely worth reading if you’re running everything in AWS.</p><p id="30ad">That’s it for today. Thank you very much for reading and for all the friendly support I’ve been receiving lately. It means a lot to me!</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="5a81">Next:</p><p id="dad0"><a href="https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460" target="_blank">Part 5 — Distributed training, EFS edition</a></p></div></div></section>
</section>
</article></body></html>
