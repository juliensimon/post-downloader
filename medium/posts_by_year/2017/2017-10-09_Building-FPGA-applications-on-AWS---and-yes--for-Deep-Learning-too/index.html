<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Building FPGA applications on AWS — and yes, for Deep Learning too</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="70bc">Building FPGA applications on AWS — and yes, for Deep Learning too</h3><p id="1365"><a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array" target="_blank">Field Programmable Gate Arrays</a> (FPGA) are not shiny new technology: indeed, the first commercial product dates back to 1985. So how could they be relevant to a bleeding edge topic like Deep Learning? Then again, neural networks themselves go back to the late Forties, so... There might be something afoot, then. Read on :)</p><blockquote class="graf--pullquote" id="0708">“Grab onto my arm now. Hold tight. We are going into a number of dark places, but I think I know the way. Just don’t let go of my arm” — Stephen King</blockquote><h4 class="graf-after--pullquote" id="98d9">The case for non-CPU architectures</h4><p id="520b">Until quite recently, the world of computing has been <strong class="markup--p-strong">unequivocally</strong> ruled by CPUs. However, for a while now, there have been <a href="http://www.economist.com/technology-quarterly/2016-03-12/after-moores-law" target="_blank">ever-growing doubts</a> on how sustainable <a href="https://en.wikipedia.org/wiki/Moore%27s_law" target="_blank">Moore’s Law</a> really is.</p><figure id="fe90"><img class="graf-image" src="image05.webp"/><figcaption>Source: Intel</figcaption></figure><p id="3753">To prevent chips from melting, clock speeds have been stagnating for years. In addition, even though lithography processes still manage to carve smaller and smaller features, we’re bound to hit <strong class="markup--p-strong">technology limits</strong> rather sooner than later: in the latest Intel Skylake architecture, a transistor is 100 atoms wide.</p><blockquote class="graf--blockquote" id="1e0f"><em class="markup--blockquote-em">The great man himself </em><a class="markup--blockquote-anchor" href="https://spectrum.ieee.org/computing/hardware/gordon-moore-the-man-whose-name-means-progress" target="_blank"><em class="markup--blockquote-em">publicly declared</em></a><em class="markup--blockquote-em"> in 2015</em>: « I guess I see Moore’s Law dying here in the next decade or so, but that’s not surprising ».</blockquote><p class="graf-after--blockquote" id="0fe0">Wait, there’s more.</p><h4 id="6e1e"><strong class="markup--h4-strong">With new workloads come new requirements</strong></h4><p id="431a">Another factor is helping foment the coming coup against King CPU: the emergence of <strong class="markup--p-strong">new workloads</strong>, such as genomics, financial computing or Deep Learning. As it happens, these involve staggering amounts of <strong class="markup--p-strong">mathematical computation</strong> which can greatly benefit from <strong class="markup--p-strong">massive parallelism</strong> (think tens of thousands of cores). Sure, it’s definitely not impossible to achieve this with CPU-based architectures — here’s a <a href="https://aws.amazon.com/blogs/aws/natural-language-processing-at-clemson-university-1-1-million-vcpus-ec2-spot-instances/" target="_blank">mind-boggling example</a> —but in recent years, a very serious contender has emerged: the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" target="_blank">Graphics Processing Unit</a> (GPU), spearheaded by Nvidia.</p><h4 id="71b1">The King is dead, long live the King (?)</h4><p id="1325">Equipped with <strong class="markup--p-strong">thousands of floating-point cores</strong>, a typical GPU is indeed a formidable crunching machine, able to deliver proper hardware parallelism at scale. Soon enough, researchers have understood how these chips could be applied to <strong class="markup--p-strong">Machine Learning</strong> and <strong class="markup--p-strong">Deep Learning</strong> at scale.</p><blockquote class="graf--blockquote" id="36f8">Patrice Y. Simard, Dave Steinkrau, Ian Buck, “<a class="markup--blockquote-anchor" href="https://www.computer.org/csdl/proceedings/icdar/2005/2420/00/24201115-abs.html" target="_blank">Using GPUs for Machine Learning Algorithms</a>”, 2005</blockquote><blockquote class="graf--blockquote graf-after--blockquote" id="baf3">Dan C. Cireşan, Ueli Meier, Jonathan Masci, Luca M. Gambardella, Jürgen Schmidhuber, “<a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1102.0183" target="_blank">High-Performance Neural Networks for Visual Object Classification</a>”, 2011</blockquote><p class="graf-after--blockquote" id="3281">And thus began the Age of the GPU, leading up to the design of computing monsters such as the <a href="https://devblogs.nvidia.com/parallelforall/inside-volta/" target="_blank">Nvidia V100</a>: 21.1 billion transistors, 815 square millimeters (1.36 square inch for my US friends), 5120 <strong class="markup--p-strong">CUDA cores</strong>, 640 <strong class="markup--p-strong">tensor cores</strong>. Surely, this should be enough for anyone… right?</p><h4 id="ba30">A chink in the GPU armor?</h4><p id="4f80">When it comes to brute force computing powers, GPUs are unmatched.</p><figure id="3043"><img class="graf-image" src="image03.webp"/><figcaption>I’m pretty sure this guy is actually the Nvidia CEO.</figcaption></figure><p id="ba4a">However, for some applications, they don’t deliver the most bang for your buck. Here are some reasons why you might not want to use a GPU:</p><ul class="postList"><li id="9e34"><strong class="markup--li-strong">Power consumption</strong> and maybe more importantly, <strong class="markup--li-strong">power efficienc</strong>y (aka TeraOPS per Watt). This does matter a lot in the embedded and IoT worlds.</li><li id="d28a">The need to process <strong class="markup--li-strong">custom data types</strong> (not everything is a float)</li><li id="1d85">Applications exhibiting <strong class="markup--li-strong">irregular parallelism</strong> (alternating phases of sequential and parallel processing) or <strong class="markup--li-strong">divergence</strong> (not all cores executing the same code at the same time).</li></ul><p id="21f4">What about Deep Learning specifically? Of course, we know that GPUs are great for training: their massive parallelism allows them to crunch large data sets in reasonable time. To optimize throughput and put all these cores to good use, we don’t forward single samples through the model: we use <strong class="markup--p-strong">batches</strong> of samples instead.</p><p id="0419">However, training is only half the story: what about <strong class="markup--p-strong">inference</strong>? Well, it depends. If your application can live with the latency required to collect enough samples to forward a full batch, then you should be fine. If not, then you’ll have to run inference on single samples and it’s likely that throughput will suffer.</p><p id="dda5">In order to get the best inference performance, the logical step would be to use a custom chip. For decades, the choice has been pretty simple: either build an Application Specific Integrated Circuit (<strong class="markup--p-strong">ASIC</strong>) or use an <strong class="markup--p-strong">FPGA</strong>.</p><h4 id="f5de">The ASIC way</h4><p id="f3e9">An ASIC is a <strong class="markup--p-strong">fully custom</strong> design, which is mass-produced and deployed in devices. Obviously, you get to tweak it and optimise it in the way that works best for your application: best performance, best power efficiency, etc. However, designing, producing and deploying an ASIC is a long, expensive and risky process. You’ll be lucky to complete it in less than 18 months.</p><p id="cb39">This is the route that Google took for their TPU chip. They did it in <strong class="markup--p-strong">15 months</strong>, which is impressive indeed. Just wonder how long it would take you.</p><blockquote class="graf--blockquote" id="681c">Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, “<a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1704.04760" target="_blank">In Datacenter Performance Analysis of a Tensor Processing Unit</a>”, 2017</blockquote><p class="graf-after--blockquote" id="a92b">Of course, ASICs are inflexible: if your application requirements change significantly, you have to start all over again.</p><h4 id="1383">The FPGA way</h4><p id="0898">As their name implies, FPGAs are (re)programmable logic circuits. A typical FPGA includes hundreds of thousands and sometimes millions of <strong class="markup--p-strong">logic cells</strong>, thousands of Digital Signal Processing (<strong class="markup--p-strong">DSP</strong>)“slices” as well as very fast <strong class="markup--p-strong">on-chip memory</strong>.</p><figure id="fd44"><img class="graf-image" src="image09.webp"/><figcaption>Source: embedded-vision.com</figcaption></figure><p id="0759">Not everyone enjoys digital circuits and Boolean algebra, so let’s keep this simple: FPGAs are <strong class="markup--p-strong">Lego digital architecture</strong>. By mixing and matching the right logic blocks, a system designer armed with the right tools can pretty much implement <strong class="markup--p-strong">anything</strong>… even an <a href="http://www.cs.columbia.edu/~sedwards/apple2fpga/" target="_blank">Apple ][</a> :)</p><figure id="99b5"><img class="graf-image" src="image02.webp"/><figcaption>Source: bober-optosensorik.de</figcaption></figure><h4 id="4a9f">Building FPGA applications</h4><p id="4175">Historically, building FPGA applications has required the purchase of costly <strong class="markup--p-strong">software and hardware tools</strong> in order to design, simulate, debug, synthesize and route custom logic designs. Let’s face it, they made it hard to scale engineering efforts.</p><p id="c501">Designing custom logic for FPGAs also required the mastery of esoteric languages like <strong class="markup--p-strong">VHDL</strong> or <strong class="markup--p-strong">Verilog</strong>… and your computer desktop would look something like this. Definitely not for everyone (including myself).</p><figure id="92b0"><img class="graf-image" src="image06.webp"/><figcaption>Source: AWS</figcaption></figure><p id="c1e6">Fortunately, developers now have the option to build FPGA applications in <strong class="markup--p-strong">C/C++</strong> thanks to the <a href="https://www.xilinx.com/products/design-tools/software-zone/sdaccel.html" target="_blank">SDAccel</a> environment and <strong class="markup--p-strong">OpenCL</strong>. The programming model won’t be unfamiliar to CUDA developers :)</p><figure id="eb14"><img class="graf-image" src="image08.webp"/><figcaption>Source: Xilinx</figcaption></figure><h4 id="539a">Deploying FPGA applications on AWS</h4><p id="25ca">About a year ago, AWS introduced <a href="https://aws.amazon.com/ec2/instance-types/f1/" target="_blank">Amazon EC2 F1 instances</a>.</p><figure id="c1dc"><img class="graf-image" src="image07.webp"/><figcaption>Source: AWS</figcaption></figure><p id="9f62">They rely on the <strong class="markup--p-strong">Xilinx Ultrascale+ VU9P chip</strong>. Here are some of the specs (<a href="https://www.xilinx.com/support/documentation/selection-guides/ultrascale-plus-fpga-product-selection-guide.pdf" target="_blank">PDF</a>): over 2.5 million System Logic Cells (<a href="https://www.xilinx.com/support/documentation/user_guides/ug574-ultrascale-clb.pdf" target="_blank">specs</a> — PDF) and 6,840 DSP slices (<a href="https://www.xilinx.com/support/documentation/user_guides/ug579-ultrascale-dsp.pdf" target="_blank">specs</a> — PDF). Yes, it’s a beast!</p><p id="0adc">In order to simplify FPGA development, AWS also provides an <a href="https://aws.amazon.com/marketplace/pp/B06VVYBLZZ" target="_blank">FPGA Developer AMI</a> coming with the full Xilinx SDx 2017.1 <strong class="markup--p-strong">tool suite</strong>… and a <strong class="markup--p-strong">free license</strong> :) The AMI also includes the AWS <strong class="markup--p-strong">SDK</strong> and <strong class="markup--p-strong">HDK</strong> to help you build and manage your FPGA images: both are Open Source and available on <a href="https://github.com/aws/aws-fpga" target="_blank">Github</a>.</p><p id="3e72">The overall process would look something like this:</p><ul class="postList"><li id="7557">Using the FPGA Developer AMI on a compute-optimized instance (such as a c4), design, simulate and build the <strong class="markup--li-strong">Amazon FPGA Image</strong> (AFI).</li><li id="471f">On an <a href="https://aws.amazon.com/ec2/instance-types/f1/" target="_blank">EC2 F1 instance</a>, use the AWS FPGA SDK to load the AFI and access it from a <strong class="markup--li-strong">host application</strong> running on the CPU.</li></ul><figure id="2165"><img class="graf-image" src="image01.webp"/><figcaption>Source: AWS</figcaption></figure><h4 id="ee5a">Building Neural Networks with FPGAs</h4><p id="8f5b">At the core of Neural Networks lies the “<strong class="markup--p-strong">Multiply and Accumulate</strong>” operation, where we multiply inputs by their respective weights and add all the results together. This can be easily implemented using a DSP slice. Yes, I know its a very simple example, but more complex operations like convolution or pooling could be implemented as well.</p><figure id="1684"><img class="graf-image" src="image04.webp"/><figcaption>Source: “FPGA Implementations of Neural Networks”, Springer, 2006</figcaption></figure><p id="15b1">Of course, modern FPGAs have tons of gates and they’re able to support very large models. However, in the interest of speed, latency and power consumption, it would make sense to try to <strong class="markup--p-strong">minimize the number of gates</strong>.</p><h4 id="c030">Optimizing Deep Learning models for FPGAs</h4><p id="8a60">There’s a lot of ongoing research to <strong class="markup--p-strong">simplify</strong> and <strong class="markup--p-strong">shrink</strong> Deep Learning models with <strong class="markup--p-strong">minimal</strong> loss of accuray. The three most popular techniques are:</p><p id="a5bd"><strong class="markup--p-strong">Quantization</strong>, i.e. using <strong class="markup--p-strong">integer weights</strong> (8, 4 or even 2-bit) instead of 32-bit floats. Less power is required: less gates are required to implement the model, and integer operations are cheaper than floating-point operations. Less memory is also required, as we save memory and shrink model size.</p><p id="6b97"><strong class="markup--p-strong">Pruning</strong>, i.e. <strong class="markup--p-strong">removing connections</strong> that play little or no role in predicting successfully. Computation speed goes up, latency goes down. Less memory is required, as we save memory and reduce model size.</p><p id="cf69"><strong class="markup--p-strong">Compression</strong>, i.e. <strong class="markup--p-strong">encoding weights</strong>, as they’re now integer-based and exhibit a smaller set of possible values. Less memory is required, as we save memory and reduce model size even further.</p><p id="5066">As a bonus, models may shrink so much that <strong class="markup--p-strong">on-chip SRAM</strong> could become a viable option. This would help in saving even more power (as SRAM is much <strong class="markup--p-strong">more efficient</strong> than DRAM) as well as speeding up computation (as on-chip RAM is always <strong class="markup--p-strong">faster</strong> to access than off-chip RAM).</p><p id="ed99">Using these techniques and more, researchers have obtained <strong class="markup--p-strong">spectacular</strong> results.</p><blockquote class="graf--blockquote" id="8423">Song Han et al, “<a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1510.00149" target="_blank">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a>”, 2016</blockquote><ul class="postList"><li class="graf-after--blockquote" id="cc29">Optimizing CNNs on CPU and GPU</li><li id="b5ba">AlexNet <strong class="markup--li-strong">35x</strong> smaller, VGG-16 <strong class="markup--li-strong">49x</strong> smaller</li><li id="27aa"><strong class="markup--li-strong">3x to 4x</strong> speedup, <strong class="markup--li-strong">3x to 7x</strong> more energy-efficient</li><li id="ede1">No loss of accuracy</li></ul><blockquote class="graf--blockquote" id="2306">Song Han et al, “<a class="markup--blockquote-anchor" href="https://arxiv.org/abs/1612.00694" target="_blank">ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA</a>”, 2017</blockquote><ul class="postList"><li class="graf-after--blockquote" id="67a0">Optimizing LSTM networks on Xilinx FPGA</li><li id="2871">FPGA vs CPU: <strong class="markup--li-strong">43x</strong> faster, <strong class="markup--li-strong">40x</strong> more energy-efficient</li><li id="fd0c">FPGA vs GPU: <strong class="markup--li-strong">3x</strong> faster, <strong class="markup--li-strong">11.5x</strong> more energy-efficient</li></ul><blockquote class="graf--blockquote" id="bb07">Nurvitadhi et al, “<a class="markup--blockquote-anchor" href="http://jaewoong.org/pubs/fpga17-next-generation-dnns.pdf" target="_blank">Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?</a>”, 2017</blockquote><ul class="postList"><li class="graf-after--blockquote" id="ddfb">Optimizing CNNs on Intel FPGA</li><li id="2a66">FPGA vs GPU: <strong class="markup--li-strong">60%</strong> faster, <strong class="markup--li-strong">2.3x</strong> more energy-efficient</li><li id="2d23">&lt;1% loss of accuracy</li></ul><h4 id="26d2">The next step: Deep Learning hardware</h4><p id="9bc1">Some of you may still remember the ill-fated <a href="https://en.wikipedia.org/wiki/Lisp_machine" target="_blank">LISP machines</a> and shiver at the thought of AI hardware. However, times have changed and researchers are moving fast here as well.</p><blockquote class="graf--blockquote" id="626a">Song Han, “<a class="markup--blockquote-anchor" href="http://isfpga.org/slides/D1_S1_Tutorial.pdf" target="_blank">Deep Learning Tutorial and Recent Trends</a>” (PDF), 2017</blockquote><p class="graf-after--blockquote" id="75b2">The topic picked up even more speed when Nvidia recently announced a new initiative, <a href="http://nvdla.org/" target="_blank">Nvidia Hardware for Deep Learning</a>. In a nutshell, this includes <a href="https://github.com/nvdla/" target="_blank">Open Source hardware blocks</a> implemented in Verilog that may be used to build Deep Learning accelerators for IoT applications:</p><ul class="postList"><li id="5e51">Convolution Core — optimized high-performance <strong class="markup--li-strong">convolution</strong> engine</li><li id="a33a">Single Data Processor — single-point lookup engine for <strong class="markup--li-strong">activation</strong> functions</li><li id="2905">Planar Data Processor — planar averaging engine for <strong class="markup--li-strong">pooling</strong></li><li id="e0c6">Channel Data Processor — multi-channel averaging engine for <strong class="markup--li-strong">normalization</strong> functions</li><li id="d6c2">Dedicated Memory and Data Reshape Engines — memory-to-memory <br/> transformation acceleration for <strong class="markup--li-strong">tensor reshape</strong> and <strong class="markup--li-strong">copy</strong> operations.</li></ul><blockquote class="graf--blockquote" id="8a68">Although clearly targeted at IoT devices, these building blocks can be simulated and deployed to F1 instances :)</blockquote><p class="graf-after--blockquote" id="35ea">This FPGA-based initiative is coming from the company that brought us GPUs in the first place, which should definitely raise a few eyebrows. In my humble opinion, <strong class="markup--p-strong">we should definitely pay attention</strong>: no one would know more than Nvidia about GPUs strengths and weaknesses and about speeding up Deep Learning computations. A exciting and clever move indeed.</p><blockquote class="graf--pullquote" id="872e">“What now? Let me tell you what now”</blockquote><blockquote class="graf--pullquote graf-after--pullquote" id="0743">— Marcellus (Pulp Fiction)</blockquote><p class="graf-after--pullquote" id="944f">I don’t have a crystal ball, but here are a few closing predictions based on extensive analysis of my gut feelings :-P</p><ul class="postList"><li id="21cb">Deep Learning is shaping up to be a <strong class="markup--li-strong">major workload</strong> for public clouds and IoT. No single hardware architecture can win both battles.</li><li id="5291">Much more infrastructure will be used for <strong class="markup--li-strong">inference</strong> than for training (I’d expect multiple orders of magnitude). Again, no single hardware architecture can win both battles.</li><li id="8957"><strong class="markup--li-strong">Cloud-based GPUs</strong> will dominate <strong class="markup--li-strong">training</strong> for the foreseeable future.</li><li id="15aa"><strong class="markup--li-strong">Cloud-based inference</strong> will be the mother of all battles. ASICs look good, but I just don’t see Nvidia letting go. Grab some popcorn and wait for the showdown.</li><li id="bf82"><strong class="markup--li-strong">CPU inference</strong> will still be a thing for smaller IoT devices, which is why software acceleration solutions like <a href="https://software.intel.com/en-us/mkl" target="_blank">Intel MKL</a> or <a href="https://github.com/Maratyszcza/NNPACK" target="_blank">NNPACK</a> are important.</li><li id="9b4d">For larger IoT devices, we may witness an <strong class="markup--li-strong">inference-driven FPGA renaissance</strong>. Current GPUs are too power-hungry and ASICs too inflexible.</li></ul><p id="1155">Well, we made it. No code this time, but I hope you still enjoyed this :)</p><p id="9bf6">Thanks for reading.</p></div></div></section>
</section>
</article></body></html>