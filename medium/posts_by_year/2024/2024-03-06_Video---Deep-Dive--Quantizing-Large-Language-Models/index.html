<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Videoâ€Šâ€”â€ŠDeep Dive: Quantizing Large Language Models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="b01d">Videoâ€Šâ€”â€ŠDeep Dive: Quantizing Large LanguageÂ Models</h3><p id="e2ba">Quantization is an excellent technique to compress Large Language Models (LLM) and accelerate their inference.</p><p id="6068">In this 2-part video, we discuss model quantization, first introducing what it is, and how to get an intuition of rescaling and the problems it creates. Then we introduce the different types of quantization: dynamic post-training quantization, static post-training quantization, and quantization-aware training. Finally, we look at and compare quantization techniques: PyTorch, ZeroQuant, bitsandbytes, SmoothQuant, GPTQ, AWQ, HQQ, and the Hugging Face Optimum Intel library ðŸ˜Ž</p><figure id="4738"><img class="graf-image" src="image01.webp"/></figure><p id="92da">Part 1:</p><figure id="cb3b"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/kw7S-3s50uk?feature=oembed" width="700"></iframe></figure><p id="e35f">Part 2:</p><figure id="1cf4"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/fXBBwCIA0Ds?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
