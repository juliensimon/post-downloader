<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Arcee.ai Llama-3.1-SuperNova-Lite is officially the ðŸ¥‡ 8-billion parameter model</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="264a">Arcee.ai Llama-3.1-SuperNova-Lite is officially the ðŸ¥‡ 8-billion parameter model</h3><p id="2640">The results came in 15 minutes ago. Arcee.ai Llama-3.1-SuperNova-Lite is officially the ðŸ¥‡ 8-billion parameter model on the <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" rel="noopener noreferrer nofollow noopener" target="_blank">Hugging Face LLM Leaderboard</a>.</p><figure id="b03e"><img class="graf-image" src="image01.webp"/></figure><p id="e588">âž¡ Model page: <a href="https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite" rel="noopener noreferrer nofollow noopener" target="_blank">https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite</a></p><p id="8509">âž¡ Notebook to deploy on SageMaker (GPU): <a href="https://github.com/arcee-ai/aws-samples/blob/main/model_notebooks/sample-notebook-llama-supernova-lite-on-sagemaker.ipynb" rel="noopener noreferrer nofollow noopener" target="_blank">https://github.com/arcee-ai/aws-samples/blob/main/model_notebooks/sample-notebook-llama-supernova-lite-on-sagemaker.ipynb</a></p><p id="6075">âž¡ Notebook to deploy on SageMaker (Inferentia2): <a href="https://github.com/arcee-ai/aws-samples/blob/main/model_notebooks/sample-notebook-llama-supernova-lite-on-sagemaker-inf2.ipynb" rel="noopener noreferrer nofollow noopener" target="_blank">https://github.com/arcee-ai/aws-samples/blob/main/model_notebooks/sample-notebook-llama-supernova-lite-on-sagemaker-inf2.ipynb</a></p><p id="76dd">This model is a scaled-down version of our SuperNova Llama-3.1â€“70B, which we believe is the best 70B available today.</p><p id="9040">âž¡ SuperNova blog post: <a href="https://blog.arcee.ai/meet-arcee-supernova-our-flagship-70b-model-alternative-to-openai/" rel="noopener noreferrer nofollow noopener" target="_blank">https://blog.arcee.ai/meet-arcee-supernova-our-flagship-70b-model-alternative-to-openai/</a></p><p id="dd7c">âž¡ Deploy SuperNova from the AWS Marketplace: <a href="https://aws.amazon.com/marketplace/pp/prodview-sb2ndlhwmzbhi" rel="noopener noreferrer nofollow noopener" target="_blank">https://aws.amazon.com/marketplace/pp/prodview-sb2ndlhwmzbhi</a></p><p id="654b"><strong class="markup--p-strong">#ai</strong> <strong class="markup--p-strong">#slm</strong> <strong class="markup--p-strong">#byebyeopenai</strong></p></div></div></section>
</section>
</article></body></html>