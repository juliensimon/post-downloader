<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Homunculus 12B and GLM-4–32B-Base-32K: 2 new Arcee AI research-oriented models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="b848">Homunculus 12B and GLM-4–32B-Base-32K: 2 new Arcee AI research-oriented models</h3><figure id="6672"><img class="graf-image" src="image01.webp"/></figure><p id="2937">In this new video, I introduce two new research-oriented models that Arcee AI recently released on Hugging Face.</p><p id="7227">Homunculus is a 12 billion-parameter instruction model distilled from Qwen3–235B onto the Mistral AI Nemo backbone. It was purpose-built to preserve Qwen’s two-mode interaction style — /think (deliberate chain-of-thought) and /nothink (concise answers) — while running on a single consumer GPU, and even on CPU as demonstrated in the video.</p><p id="101f">GLM-4–32B-Base-32K is an enhanced version of Tsinghua University’s THUDM’s GLM-4–32B-Base-0414, specifically engineered to offer robust performance over an extended context window. While the original model’s capabilities degraded after 8,192 tokens, this version maintains strong performance up to a 32,000-token context, making it ideal for tasks requiring long-context understanding and processing.</p><figure id="7ae8"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/mhrcPviW-MU?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>