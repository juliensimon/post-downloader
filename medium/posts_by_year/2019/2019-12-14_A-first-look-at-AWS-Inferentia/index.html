<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>A first look at AWS Inferentia</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="35c5">A first look at AWS Inferentia</h3><p id="e762">Launched at AWS re:Invent 2019, <a href="https://aws.amazon.com/machine-learning/inferentia/" target="_blank">AWS Inferentia</a> is a high performance machine learning inference chip, custom designed by AWS: its purpose is to deliver cost effective, low latency predictions at scale. Inferentia is present in <a href="https://aws.amazon.com/ec2/instance-types/inf1/" target="_blank">Amazon EC2 inf1</a> instances, a new family of instances also <a href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/" target="_blank">launched</a> at re:Invent.</p><p id="80b4">In this post, I’d like to show you how to get started with Inferentia and <a href="https://www.tensorflow.org" target="_blank">TensorFlow</a>. Please note that <a href="https://mxnet.apache.org" target="_blank">Apache MXNet</a>, <a href="https://pytorch.org" target="_blank">PyTorch</a> and <a href="https://onnx.ai" target="_blank">ONNX</a> are also supported.</p><h4 id="c835">A primer on Inferentia</h4><p id="959b">The <a href="https://www.youtube.com/watch?v=17r1EapAxpk" target="_blank">CMP324</a> breakout session is a great introduction to Inferentia, and the Alexa use case is a rare look under the hood. It’s well worth your time.</p><figure id="e7e5"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/17r1EapAxpk?feature=oembed" width="700"></iframe></figure><p id="3b69">In a nutshell, each Inferentia chip hosts 4 NeuronCores. Each one of these implements a “high performance <a href="https://en.wikipedia.org/wiki/Systolic_array" target="_blank">systolic array</a> matrix multipy engine” (nicely put, Gadi), and is also equipped with a large on-chip cache.</p><figure id="459a"><img class="graf-image" src="image07.webp"/></figure><p id="ed55">Chips are interconnected, which makes it possible to:</p><ul class="postList"><li id="c682">Partition a model across multiple cores (and Inferentia chips, if several are available), storing it 100% in on-cache memory.</li><li id="97c4">Stream data at full speed through the pipeline of cores, without having to deal with latency caused by external memory access.</li></ul><figure id="6381"><img class="graf-image" src="image05.webp"/></figure><p id="719c">Alternatively, you can run inference with different models on the same Inferentia chip. This is achieved by partitioning NeuronCores into NeuronCore Groups, and by loading different models on different groups.</p><h4 id="dd95">The Neuron SDK</h4><p id="03f4">In order to run on Inferentia, models first need to be compiled to a hardware-optimized representation. Then, they may be loaded, executed and profiled using a specific runtime. These operations can be performed through command-line tools available in the <a href="https://github.com/aws/aws-neuron-sdk" target="_blank">AWS Neuron SDK</a>, or through framework APIs.</p><div class="graf--mixtapeEmbed" id="8013"><a class="markup--mixtapeEmbed-anchor" href="https://github.com/aws/aws-neuron-sdk" title="https://github.com/aws/aws-neuron-sdk"><strong class="markup--mixtapeEmbed-strong">aws/aws-neuron-sdk</strong><br/><em class="markup--mixtapeEmbed-em">AWS Neuron Overview Getting started AWS Neuron is a software development kit (SDK) enabling high-performance deep…</em>github.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1c2339412aac7cbb1d63f50156d6a0e8" data-thumbnail-img-id="0*ovQiaC2pBNJgP-t_" href="https://github.com/aws/aws-neuron-sdk" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ovQiaC2pBNJgP-t_);"></a></div><p class="graf-after--mixtapeEmbed" id="4b4b">Let’s get started!</p><h4 id="8182">Launching an EC2 instance for model compilation</h4><p id="b9cc">This first step doesn’t require an <em class="markup--p-em">inf1</em> instance. In fact, you should use a <strong class="markup--p-strong">compute-optimized instance</strong> for fast and cost effective compilation. In order to avoid any software configuration, you should also use the <a href="https://aws.amazon.com/machine-learning/amis/" target="_blank"><strong class="markup--p-strong">Deep Learning AMI</strong></a>, which comes preinstalled with the Neuron SDK and with up to date frameworks.</p><p id="e627">At the time of writing, the most recent Deep Learning AMI for Amazon Linux 2 is version 26.0, and it’s identifier is <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=ami-08e68326c36bf3710" target="_blank"><em class="markup--p-em">ami-08e68326c36bf3710</em></a>.</p><figure id="6324"><img class="graf-image" src="image04.webp"/></figure><p id="9734">Using this AMI, I fire up a <em class="markup--p-em">c5d.4xlarge</em> instance. No special settings are required: just make sure you allow SSH access in the Security Group.</p><figure id="d9eb"><img class="graf-image" src="image06.webp"/><figcaption>family, instance name, vCPUs, RAM, storage</figcaption></figure><p id="8c60">Once the instance is up, I ssh to it, and I’m greeted by the familiar Deep Learning AMI banner, tellling me that <a href="https://docs.conda.io/en/latest/" target="_blank">Conda</a> environments are available for TensorFlow and Apache MXNet.</p><pre class="graf--pre" id="0889">====================================================================<br/> __|  __|_  )<br/> _|  (     / Deep Learning AMI (Amazon Linux 2) Version 26.0<br/>___|\___|___|<br/>====================================================================</pre><pre class="graf--pre graf-after--pre" id="0c85">Please use one of the following commands to start the required environment with the framework of your choice:<br/>for MXNet(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) <br/>source activate mxnet_p36<br/>for MXNet(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) <br/>source activate mxnet_p27<br/><strong class="markup--pre-strong">for MXNet(+AWS Neuron) with Python3 <br/>source activate aws_neuron_mxnet_p36</strong><br/>for TensorFlow(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) source activate tensorflow_p36<br/>for TensorFlow(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) source activate tensorflow_p27<br/><strong class="markup--pre-strong">for TensorFlow(+AWS Neuron) with Python3<br/>source activate aws_neuron_tensorflow_p36</strong><br/>for TensorFlow 2(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) ssource activate tensorflow2_p36<br/>for TensorFlow 2(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) ssource activate tensorflow2_p27<br/>for PyTorch with Python3 (CUDA 10.1 and Intel MKL) <br/>source activate pytorch_p36<br/>for PyTorch with Python2 (CUDA 10.1 and Intel MKL)<br/>source activate pytorch_p27<br/>for Chainer with Python2 (CUDA 10.0 and Intel iDeep)<br/>source activate chainer_p27<br/>for Chainer with Python3 (CUDA 10.0 and Intel iDeep)<br/>source activate chainer_p36<br/>for base Python2 (CUDA 10.0)<br/>source activate python2<br/>for base Python3 (CUDA 10.0) <br/>source activate python3</pre><pre class="graf--pre graf-after--pre" id="17d0">Official Conda User Guide: <a class="markup--pre-anchor" href="https://docs.conda.io/projects/conda/en/latest/user-guide/" rel="nofollow noopener" target="_blank">https://docs.conda.io/projects/conda/en/latest/user-guide/</a><br/>AWS Deep Learning AMI Homepage: <a class="markup--pre-anchor" href="https://aws.amazon.com/machine-learning/amis/" rel="nofollow noopener" target="_blank">https://aws.amazon.com/machine-learning/amis/</a><br/>Developer Guide and Release Notes: <a class="markup--pre-anchor" href="https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html" rel="nofollow noopener" target="_blank">https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html</a><br/>Support: <a class="markup--pre-anchor" href="https://forums.aws.amazon.com/forum.jspa?forumID=263" rel="nofollow noopener" target="_blank">https://forums.aws.amazon.com/forum.jspa?forumID=263</a><br/>For a fully managed experience, check out Amazon SageMaker at <a class="markup--pre-anchor" href="https://aws.amazon.com/sagemaker" rel="nofollow noopener" target="_blank">https://aws.amazon.com/sagemaker</a><br/><strong class="markup--pre-strong">When using INF1 type instances, please update regularly using the instructions at: </strong><a class="markup--pre-anchor" href="https://github.com/aws/aws-neuron-sdk/tree/master/release-notes" rel="nofollow noopener" target="_blank"><strong class="markup--pre-strong">https://github.com/aws/aws-neuron-sdk/tree/master/release-notes</strong></a><br/>====================================================================</pre><p class="graf-after--pre" id="cffb">I activate the appropriate environment, which provides all required dependencies.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="f482">For the rest of this post, any shell command prefixed by <em class="markup--blockquote-em">(aws_neuron_tensorflow_p36)</em> should be run inside that Conda environment.</blockquote><pre class="graf--pre graf-after--blockquote" id="c28e">$ source activate aws_neuron_tensorflow_p36<br/>(aws_neuron_tensorflow_p36) $</pre><p class="graf-after--pre" id="8d77">Next, I upgrade the <em class="markup--p-em">tensorflow-neuron</em> package.</p><pre class="graf--pre" id="33af">(aws_neuron_tensorflow_p36) $ conda install numpy=1.17.2 --yes<br/>(aws_neuron_tensorflow_p36) $ conda update tensorflow-neuron</pre><p class="graf-after--pre" id="3011">We’re now ready to fetch a model and compile it.</p><h4 id="f30f">Compiling a model</h4><p id="30d5">The code below grabs a <a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet50</a> image classification model pretrained on the <a href="http://image-net.org" target="_blank">ImageNet</a> dataset, and stores it in the <em class="markup--p-em">resnet50</em> directory.</p><p id="c41b">Then, it compiles it for Inferentia. I highlighted the single line of code required: everything else is vanilla TensorFlow. Then, the compiled model is saved in the <em class="markup--p-em">ws_resnet50</em> directory, and in a ZIP file for easy copy to an <em class="markup--p-em">inf1</em> instance.</p><pre class="graf--pre" id="785e">import os<br/>import time<br/>import shutil<br/>import tensorflow as tf<br/><strong class="markup--pre-strong">import tensorflow.neuron as tfn</strong><br/>import tensorflow.compat.v1.keras as keras<br/>from tensorflow.keras.applications.resnet50 import ResNet50<br/>from tensorflow.keras.applications.resnet50 import preprocess_input<br/><br/># Create a workspace<br/>WORKSPACE = './ws_resnet50'<br/>os.makedirs(WORKSPACE, exist_ok=True)<br/><br/># Prepare export directory (old one removed)<br/>model_dir = os.path.join(WORKSPACE, 'resnet50')<br/>compiled_model_dir = os.path.join(WORKSPACE, 'resnet50_neuron')<br/>shutil.rmtree(model_dir, ignore_errors=True)<br/>shutil.rmtree(compiled_model_dir, ignore_errors=True)<br/><br/># Instantiate Keras ResNet50 model<br/>keras.backend.set_learning_phase(0)<br/>keras.backend.set_image_data_format('channels_last')<br/><br/>model = ResNet50(weights='imagenet')<br/><br/># Export SavedModel<br/>tf.saved_model.simple_save(<br/>    session            = keras.backend.get_session(),<br/>    export_dir         = model_dir,<br/>    inputs             = {'input': model.inputs[0]},<br/>    outputs            = {'output': model.outputs[0]})<br/><br/><strong class="markup--pre-strong"># Compile using Neuron<br/>tfn.saved_model.compile(model_dir, compiled_model_dir)    </strong><br/><br/># Prepare SavedModel for uploading to Inf1 instance<br/>shutil.make_archive('./resnet50_neuron', 'zip', WORKSPACE, 'resnet50_neuron')</pre><p class="graf-after--pre" id="a3d7">That one <a href="https://github.com/aws/aws-neuron-sdk/blob/master/docs/tensorflow-neuron/api-compilation-python-api.md" target="_blank">API</a> is all it takes! Impressive.</p><blockquote class="graf--blockquote" id="1379">Power users will enjoy reading about the CLI compiler, <a class="markup--blockquote-anchor" href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-cc" target="_blank">neuron-cc</a>.</blockquote><p class="graf-after--blockquote" id="bc75">Running this code produces the expected output.</p><pre class="graf--pre" id="f0cb">(aws_neuron_tensorflow_p36) $ python compile_resnet.py</pre><pre class="graf--pre graf-after--pre" id="4999">&lt;output removed&gt;<br/>Downloading data from <a class="markup--pre-anchor" href="https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5" rel="nofollow noopener" target="_blank">https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5</a><br/>102973440/102967424 [==============================] - 2s 0us/step<br/>&lt;output removed&gt;<br/>INFO:tensorflow:fusing subgraph neuron_op_d6f098c01c780733 with neuron-cc<br/>INFO:tensorflow:Number of operations in TensorFlow session: 4638<br/>INFO:tensorflow:Number of operations after tf.neuron optimizations: 556<br/>INFO:tensorflow:Number of operations placed on Neuron runtime: 554<br/>INFO:tensorflow:Successfully converted ./ws_resnet50/resnet50 to ./ws_resnet50/resnet50_neuron</pre><p class="graf-after--pre" id="054b">Then, I simply copy the ZIP file to an Amazon S3 bucket, probably the easiest way to share it with <em class="markup--p-em">inf1</em> instances used for inference.</p><pre class="graf--pre" id="aba2">$ ls *.zip<br/>resnet50_neuron.zip<br/>$ aws s3 mb s3://jsimon-inf1-useast1 <br/>$ aws s3 cp resnet50_neuron.zip s3://jsimon-inf1-useast1<br/>upload: ./resnet50_neuron.zip to s3://jsimon-inf1-useast1/resnet50_neuron.zip</pre><p class="graf-after--pre" id="f164">Alright, let’s fire up one of these babies.</p><h4 id="1187">Predicting on Inferentia with TensorFlow</h4><p id="d44b">Using the same AMI as above, I launch an <em class="markup--p-em">inf1.xlarge</em> instance.</p><figure id="cb7b"><img class="graf-image" src="image03.webp"/><figcaption>family, instance name, vCPUs, RAM, storage</figcaption></figure><p id="19de">Once this instance is up, I ssh to it, and I update the Neuron CLI tools.</p><pre class="graf--pre" id="94d3">$ sudo yum install aws-neuron-tools aws-neuron-runtime-base aws-neuron-runtime -y</pre><p class="graf-after--pre" id="ba56">For example, I can view hardware properties using <a href="https://github.com/aws/aws-neuron-sdk/tree/master/docs/neuron-tools" target="_blank"><em class="markup--p-em">neuron-ls</em></a>.</p><figure id="70bc"><img class="graf-image" src="image01.webp"/></figure><p id="262c">4 NeuronCores, as expected. The ‘east’ and ‘west’ columns show connections to other Inferentia chips: as this instance only has one, they’re empty.</p><p id="45ba">Next, I retrieve the compiled model from my S3 bucket, and extract it. I also download a test image.</p><pre class="graf--pre" id="f987">$ aws s3 cp s3://jsimon-inf1-useast1/resnet50_neuron.zip .<br/>download: s3://jsimon-inf1-useast1/resnet50_neuron.zip to resnet50_neuron.zip<br/>$ unzip resnet50_neuron.zip<br/>Archive: resnet50_neuron.zip<br/> creating: resnet50_neuron/<br/> creating: resnet50_neuron/variables/<br/> inflating: resnet50_neuron/saved_model.pb<br/>$ curl -O <a class="markup--pre-anchor" href="https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg" target="_blank">https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg</a></pre><p class="graf-after--pre" id="ba0d">Using the code below, I load and transform the test image. I then load the compiled model, and use it to classify the image.</p><pre class="graf--pre" id="aaf7">import os<br/>import time<br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing import image<br/>from tensorflow.keras.applications import resnet50<br/><br/>tf.keras.backend.set_image_data_format('channels_last')<br/><br/># Create input from image<br/>img_sgl = image.load_img('kitten_small.jpg', target_size=(224, 224))<br/>img_arr = image.img_to_array(img_sgl)<br/>img_arr2 = np.expand_dims(img_arr, axis=0)<br/>img_arr3 = resnet50.preprocess_input(img_arr2)<br/><br/># Load model<br/>COMPILED_MODEL_DIR = './resnet50_neuron/'<br/>predictor_inferentia = tf.contrib.predictor.from_saved_model(COMPILED_MODEL_DIR)<br/><br/># Run inference<br/>model_feed_dict={'input': img_arr3}<br/>infa_rslts = predictor_inferentia(model_feed_dict);<br/><br/># Display results<br/>print(resnet50.decode_predictions(infa_rslts["output"], top=5)[0])</pre><p class="graf-after--pre" id="17e8">Can you guess how many lines of Inferentia specific code are present here? The answer is <strong class="markup--p-strong">zero</strong>. We seamlessly use the <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/predictor" target="_blank"><em class="markup--p-em">tf.contrib.predictor</em></a> API. Woohoo!</p><p id="ebad">Running this code produces the expected output, and we see the top 5 classes for the image.</p><pre class="graf--pre" id="5a24">(aws_neuron_tensorflow_p36) $ python infer_resnet50.py</pre><pre class="graf--pre graf-after--pre" id="d7ef">&lt;output removed&gt;<br/>[('n02123045', 'tabby', 0.6918919), ('n02127052', 'lynx', 0.12770271), ('n02123159', 'tiger_cat', 0.08277027), ('n02124075', 'Egyptian_cat', 0.06418919), ('n02128757', 'snow_leopard', 0.009290541)]</pre><p class="graf-after--pre" id="1454">Now let’s see how we can load a compiled model using <a href="https://www.tensorflow.org/tfx/guide/serving" target="_blank">TensorFlow Serving,</a> which is a very good option for production deployments.</p><h4 id="6ac4">Predicting on Inferentia with TensorFlow Serving</h4><p id="4a5f">First, we need to package the model properly, and move it to a directory reflecting its version. We have only one here, so let’s move the saved model to a directory named ‘1’.</p><pre class="graf--pre" id="507e">$ pwd<br/>/home/ec2-user/resnet50_neuron<br/>$ mkdir 1<br/>$ mv * 1</pre><p class="graf-after--pre" id="e024">First-time users of TensorFlow Serving are often confused by the file layout, so here’s what it should look like. This directory is the one you should pass to TensorFlow Serving using the <em class="markup--p-em">model_base_path</em> argument.</p><figure id="8b31"><img class="graf-image" src="image02.webp"/></figure><p id="e6dd">Now, we can launch TensorFlow Serving, and load the compiled model. Once again, this is vanilla TensorFlow.</p><pre class="graf--pre" id="384f">(aws_neuron_tensorflow_p36) $ tensorflow_model_server_neuron <br/>--model_name=resnet50 <br/>--model_base_path=/home/ec2-user/resnet50_neuron <br/>--port=8500</pre><pre class="graf--pre graf-after--pre" id="1bdd">2019–12–13 16:16:27.704882: I tensorflow_serving/core/loader_harness.cc:87] <strong class="markup--pre-strong">Successfully loaded servable version {name: resnet50 version: 1}</strong><br/>2019–12–13 16:16:27.706241: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500</pre><p class="graf-after--pre" id="949c">Once TensorFlow Serving is up and running, we can use the script below to load a test image, and send it for prediction. At the risk of repeating myself… this is vanilla TensorFlow :)</p><pre class="graf--pre" id="72b2">import numpy as np<br/>import grpc<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing import image<br/>from tensorflow.keras.applications.resnet50 import preprocess_input<br/>from tensorflow.keras.applications.resnet50 import decode_predictions<br/>from tensorflow_serving.apis import predict_pb2<br/>from tensorflow_serving.apis import prediction_service_pb2_grpc<br/><br/>if __name__ == '__main__':<br/>    chan = grpc.insecure_channel('localhost:8500')<br/>    stub = prediction_service_pb2_grpc.PredictionServiceStub(chan)</pre><pre class="graf--pre graf-after--pre" id="d7ad">    img_file="kitten_small.jpg"      <br/>    img = image.load_img(img_file, target_size=(224, 224))<br/>    img_array = preprocess_input(image.img_to_array(img)[None, ...])</pre><pre class="graf--pre graf-after--pre" id="7973">    request = predict_pb2.PredictRequest()<br/>    request.model_spec.name = 'resnet50'<br/>    request.inputs['input'].CopyFrom(<br/>        tf.contrib.util.make_tensor_proto(<br/>            img_array, shape=img_array.shape)<br/>    )</pre><pre class="graf--pre graf-after--pre" id="0d07">    result = stub.Predict(request)<br/>    prediction = tf.make_ndarray(result.outputs['output'])<br/>    print(decode_predictions(prediction))</pre><p class="graf-after--pre" id="4a1f">Running this code produces the expected output, and we see the top 5 classes for the image.</p><pre class="graf--pre" id="42e6">(aws_neuron_tensorflow_p36) $ python tfserving_resnet50.py</pre><pre class="graf--pre graf-after--pre" id="f27a">&lt;output removed&gt;<br/>[[(‘n02123045’, ‘tabby’, 0.6918919), (‘n02127052’, ‘lynx’, 0.12770271), (‘n02123159’, ‘tiger_cat’, 0.08277027), (‘n02124075’, ‘Egyptian_cat’, 0.06418919), (‘n02128757’, ‘snow_leopard’, 0.009290541)]]</pre><p class="graf-after--pre" id="9cd2">I recorded this demo, and it’s available on YouTube.</p><figure id="b370"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/ifOR6CEINLo?feature=oembed" width="640"></iframe></figure><h4 id="ecb6">Diving deeper</h4><p id="9a67">That’s it for today. I hope I gave you a clear introduction to AWS Inferentia, and how easy it is to use it! All it took is <strong class="markup--p-strong">one line of code</strong> to compile our model.</p><p id="4182">If you’d like to dive deeper, I highly recommend the excellent <a href="https://github.com/awshlabs/reinvent19Inf1Lab" target="_blank">workshop</a> delivered at re:Invent by my colleague Wenming Ye. One of the labs shows you how to compile a 32-bit floating point (FP32) ResNet50 model to 16-bit floating point (FP16). By reducing arithmetic complexity, this technique is known to improve performance while preserving accuracy. Indeed, on an <em class="markup--p-em">inf1.2xlarge</em> instance, the FP16 model delivers an impressive <strong class="markup--p-strong">1,500 image classifications per second</strong>!</p><p id="8a3c">As always, thank you for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="0042">Inferentia NeuronCores are totally hardcore. They destroy everything \m/</p><figure id="fc46"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/DBwgX8yBqsw?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
