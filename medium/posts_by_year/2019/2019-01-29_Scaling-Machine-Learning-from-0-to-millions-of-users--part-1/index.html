<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Scaling Machine Learning from 0 to millions of users, part 1</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="0a14">Scaling Machine Learning from 0 to millions of users — part 1</h3><h4 class="graf--subtitle" id="29c3">Breaking out of the laptop</h4><p id="8eca">I suppose most Machine Learning (ML) models are conceived on a whiteboard or a napkin, and born on a laptop. As the fledgling creatures start babbling their first predictions, we’re filled with pride and high hopes for their future abilities. Alas, we know deep down in our heart that that not all of them will be successful, far from it.</p><p id="1a62">A small number fail us quickly as we build them. Others look promising, and demonstrate some level of predictive power. We are then faced with the grim challenge of deploying them in a production environment, where they’ll either prove their legendary valour or die an inglorious death.</p><figure id="213f"><img class="graf-image" src="image01.webp"/><figcaption>One day, your models will rule the world… if you read all these posts and pay attention ;)</figcaption></figure><p id="5949">In this series of opinionated posts, we’ll discuss <strong class="markup--p-strong">how to train ML models and deploy them to production, from humble beginnings to world domination</strong>. Along the way, we’ll try to take justified and reasonable steps, fighting the evil forces of over-engineering, Hype Driven Development and “why don’t you just use XYZ?”.</p><blockquote class="graf--pullquote" id="c4f5">Enjoy the safe comfort of your Data Science sandbox while you can, and prepare yourself for the cold, harsh world of production.</blockquote><h3 class="graf-after--pullquote" id="7260"><strong class="markup--h3-strong">Day 0</strong></h3><p id="afd8">So you want to build a ML model. Hmmm. Let’s pause for a minute and consider this:</p><ul class="postList"><li id="c0f3">Could your business problem be addressed by a <strong class="markup--li-strong">high-level AWS service</strong>, such as <a href="http://aws.amazon.com/rekognition" target="_blank">Amazon Rekognition</a>, <a href="http://aws.amazon.com/rekognition" target="_blank">Amazon Polly</a>, etc.?</li><li id="cabe">Or by the <a href="https://medium.com/@julsimon/applying-machine-learning-to-aws-services-9768f926f11f" target="_blank">growing list of applied ML features</a> embedded in other AWS services?</li></ul><p id="aeb3">Don’t wave this off: <strong class="markup--p-strong">no Machine Learning is easier to manage than no Machine Learning</strong>. Figuring a way to use high-level services could save you weeks of work, maybe months.</p><h4 id="a08e">If the answer is “yes”</h4><p id="914a">Please ask yourself:</p><ul class="postList"><li id="7e89">Why would you go through all the trouble of building a redundant custom solution?</li><li id="a8c6">Are you really “<em class="markup--li-em">missing features</em>”? What’s the <strong class="markup--li-strong">real</strong> business impact?</li><li id="3394">Do you really need “<em class="markup--li-em">more accuracy</em>” How do you know <strong class="markup--li-strong">you</strong> could reach it?</li></ul><p id="8a4f">If you’re unsure, why not run a <strong class="markup--p-strong">quick PoC </strong>with your own data? These services are fully-managed (no… more… servers) and very easy to integrate in any application. It shouldn’t take a lot of time to figure them out, and you would then have solid data to make an <strong class="markup--p-strong">educated decision</strong> on whether you really need to train your own model or not.</p><blockquote class="graf--blockquote" id="a400">If these services work well enough for you, congratulations, you’re mostly done! If you decide to build, I’d love to hear your feedback. Please get in touch.</blockquote><h4 class="graf-after--blockquote" id="4cf6"><strong class="markup--h4-strong">If this answer is “no”</strong></h4><p id="69c4">Please ask yourself the question again! Most of us have an amazing capability to twist reality and deceive ourselves :) If the honest answer is really “no”, then I’d still recommend thinking about <strong class="markup--p-strong">subprocesses</strong> where you could use the high-level services, e.g. :</p><ul class="postList"><li id="7176">using <a href="http://aws.amazon.com/translate" target="_blank">Amazon Translate</a> for supported language pairs and using your own solution for the rest.</li><li id="2f73">using <a href="http://aws.amazon.com/rekognition" target="_blank">Amazon Rekognition</a> to detect faces before feeding them to your model,</li><li id="147b">using <a href="http://aws.amazon.com/textract" target="_blank">Amazon Textract</a> to extract text before feeding it to your NLP model.</li></ul><p id="3885">This isn’t about pitching AWS services (do I look like a salesperson?). I’m simply <strong class="markup--p-strong">trying to save you from reinventing the wheel </strong>(or parts of the wheel): you should really be <strong class="markup--p-strong">focusing on the business problem</strong> at hand, instead of building a house of cards that you read about in a blog post or saw at a conference. Yes, it may look great on your resume, and the wheel is initially a fun merry-go-round… and then, it turns into <strong class="markup--p-strong">the Wheel of Pain, you’re chained to it and someone else is holding the whip</strong>.</p><figure id="4e63"><img class="graf-image" src="image03.webp"/><figcaption>Why did I blindly trust that meetup talk? Crom! Help me escape and bash that guy’s skull with his laptop.</figcaption></figure><p id="9e54">Anyway, enough negativity :) You do need a model, let’s move on.</p><h3 id="0adf">Day 1: one user (you)</h3><p id="06ad">We’ll start our journey at the stage where you’ve trained a model on your local machine (or a local dev server), using a popular open source library like <a href="https://scikit-learn.org" target="_blank">scikit-learn</a>, <a href="https://www.tensorflow.org" target="_blank">TensorFlow</a> or <a href="https://mxnet.incubator.apache.org" target="_blank">Apache MXNet</a>. Maybe you’ve even implemented your own algorithm (Data scientists, you devils).</p><p id="2766">You’ve measured the model’s accuracy using your test set, and things look good. Now you’d like to deploy the model to production in order to check its actual behaviour, run A/B tests, etc. Where to start?</p><h4 id="e3c4">Batch prediction or real-time prediction?</h4><p id="bee5">First, you should figure out whether your application requires <strong class="markup--p-strong">batch prediction</strong> (i.e. collect a large number data points, process them periodically and store results somewhere), or <strong class="markup--p-strong">real-time prediction</strong> (i.e. send a data point to a web service and receive an immediate prediction ). The reason why I bring this point early on is because it has a large impact on deployment complexity.</p><p id="bd97">At first sight, real-time prediction sounds more appealing (because… real-time, yeah!), but it also comes with stronger requirements, inherent in web services: high availability, the ability to handle traffic bursts, etc. Batch is more relaxed, as it only needs to run every now and then: as long as you don’t lose data, no one will see if it’s broken in between ;)</p><p id="9357">Scaling is not a concern right now: all you care about is deploying your model, kicking the tires, running some performance tests, etc. From my experience, <strong class="markup--p-strong">you’ve probably taken the shortest route and deployed everything to a single Amazon EC2 instance</strong>. Everybody knows a bit of Linux CLI, and you read somewhere that using “IaaS will protect you from evil vendor lock-in”. Ha! EC2 it is, then!</p><blockquote class="graf--blockquote" id="49ea">I hear screams of horror and disbelief across the AWS time-space continuum, and maybe some snarky comments along the lines of “oh this is totally stupid, no one actually does that!”. Well, I’ll put money on the fact that this is by far how most people get started. Congrats if you didn’t, but please let me show these good people which way is out before they really hurt themselves ;)</blockquote><p class="graf-after--blockquote" id="f8c6">And so, staring into my magic mirror, I see…</p><h4 id="75f3">Batch prediction</h4><p id="bff1">You’ve copied your model, your batch script and your application to an EC2 instance. Your batch script runs periodically as a cron job, and saves predicted data to local storage. Your application loads both the model and initial predicted data at startup, and uses it to do whatever it has to do. It also periodically checks for updated predictions, and loads them whenever they’re available.</p><h4 id="b9cb">Real-time prediction</h4><p id="3b3d">You’ve embedded the model in your application, loading it at startup and serving predictions using all kinds of data (user input, files, APIs, etc.).</p><p id="6938">One way or the other, you’re now running predictions in the cloud, and life is good. You celebrate with a pint of stout… or maybe gluten-free, fair-trade, organic soy milk latte, because it’s 2019 after all.</p><h3 id="a128">Week 1: one sorry user (you)</h3><p id="eac3">The model predicts nicely, and you’d like to invest more time in collecting more data and adding features. Unfortunately, it didn’t take long for things to go wrong and you’re now <strong class="markup--p-strong">bogged down in all kinds of issues</strong> (non exhaustive list below):</p><ul class="postList"><li id="8262">Training on your laptop and deploying manually to the cloud is painful and error-prone.</li><li id="9249">You accidentally terminated your EC2 instance and had to reinstall everything from scratch.</li><li id="a64c">You ‘<em class="markup--li-em">pip install</em>’-ed a Python library and now your EC2 instance is all messed up.</li><li id="83ef">You had to manually install two other instances for your colleagues, and now you can’t really be sure that you’re all using identical environments.</li><li id="59c4">Your first load test failed, but you’re not sure what to blame: application? model? the ancients wizards of Acheron?</li><li id="a93b">You’d like to implement the same algorithm in TensorFlow, and maybe Apache MXNet too: more environments, more deployments. No time for that.</li><li id="c3a8">And of course, everyone’s favorite: Sales have heard that “your product now has AI capabilities”. You’re terrified that they could sell it to a customer and ask you to go live at scale next week.</li></ul><p id="ebb5">The list goes on. It would be funny if it wasn’t real (feel free to add your own examples in the comments). All of the sudden, this ML adventure doesn’t sound as exciting, does it? <strong class="markup--p-strong">You’re spending most of your time on firefighting, not on building the best possible model</strong>. This can’t go on!</p><figure id="9908"><img class="graf-image" src="image02.webp"/><figcaption>I’ve revoked your IAM credentials on ‘<em class="markup--figure-em">TerminateInstances</em>’. Yes, even in the dev account. Any questions?</figcaption></figure><h3 id="2343">Week 2: fighting back</h3><p id="fede">Someone on the team watched this really cool AWS video, featuring a new ML service called <a href="http://aws.amazon.com/sagemaker" target="_blank">Amazon SageMaker</a>. You make a mental note of it, but right now, there’s no time to rebuild everything: Sales is breathing down your neck, you have a customer demo in a few days, and you need to harden the existing solution.</p><p id="03fb">Chances are, you don’t have a mountain of data yet: training can wait. You need to focus on making prediction reliable. Here are some solid techniques measures that won’t take more than a few days to implement.</p><h4 id="c5d0">Use the Deep Learning AMI</h4><p id="d4f3">Maintained by AWS, this <a href="https://aws.amazon.com/machine-learning/amis/" target="_blank">Amazon Machine Image</a> comes <strong class="markup--p-strong">pre-installed</strong> with a lot of tools and libraries that you’ll probably need: open source, NVIDIA drivers, etc. Not having to manage them will save you a lot of time, and will also guarantee that your multiple instances run with the same setup.</p><p id="33ce">The AMI also comes with the <a href="https://conda.io/en/latest/" target="_blank">Conda</a> <strong class="markup--p-strong">dependency and environment manager</strong>, which lets you quickly and easily create many isolated environments: that’s a great way to test your code with different Python versions or different libraries, without unexpectedly clobbering everything.</p><p id="114a">Last but not least, this AMI is <strong class="markup--p-strong">free of charge</strong>, and just like any other AMI, you can customize if you *really* have to.</p><h4 id="38de">Break the monolith</h4><p id="8b5e">Your application code and your prediction code have <strong class="markup--p-strong">different requirements</strong>. Unless you have a compelling reason to do so (ultra low latency might be one), they shouldn’t live under the same roof. Let’s look at some reasons why:</p><ul class="postList"><li id="7372"><strong class="markup--li-strong">Deployment</strong>: do you want to restart or update your app every time you update the model? Or ping your app to reload it or whatever? No no no no. Keep it simple: when it comes to decoupling, <strong class="markup--li-strong">nothing beats building separate services</strong>.</li><li id="7674"><strong class="markup--li-strong">Performance</strong>: what if your application code runs best on memory-intensive instances and your ML model requires a GPU? How will you handle that trade-off? Why would you favour one or the other? Separating them lets you <strong class="markup--li-strong">pick the best instance type for each use case</strong>.</li><li id="3df6"><strong class="markup--li-strong">Scalability</strong>: what if your application code and your model have different scalability profiles? It would be a shame to scale out on GPU instances because a small piece of your application code is running hot… Again, it’s better to keep things separated, this will help take the most <strong class="markup--li-strong">appropriate scaling decisions</strong> as well as reduce cost.</li></ul><p id="f5da">Now, what about pre-processing / post-processing code, i.e. actions that you need to take on the data just before and just after predicting. Where should it go? It’s hard to come up with a definitive answer: I’d say that <strong class="markup--p-strong">model-independent actions</strong> (formatting, logging, etc.) should stay in the application, whereas <strong class="markup--p-strong">model-dependent actions</strong> (feature engineering) should stay close to the model to avoid deployment inconsistencies.</p><h4 id="238c">Build a prediction service</h4><p id="d274">Separating the prediction code from the application code doesn’t have to be painful, and you can reuse <strong class="markup--p-strong">solid, scalable tools</strong> to build a prediction service. Let’s look at some options:</p><ul class="postList"><li id="ba89"><strong class="markup--li-strong">Scikit-learn</strong>: when it comes to building web services in Python, I’m a big fan of <a href="http://flask.pocoo.org" target="_blank">Flask</a>. It’s neat, simple and it scales well. No need to look further IMHO. You code would look something like that.</li></ul><pre class="graf--pre" id="9f64"># My awesome API<br/>from flask import Flask<br/>import pickle</pre><pre class="graf--pre graf-after--pre" id="c1a2">app = Flask(__name__)<br/>model = pickle.load(open("my_awesome_model.sav", 'rb'))<br/>...<br/><code class="markup--code markup--pre-code">@app.route('/predict', methods=['POST'])<br/></code>def predict():<br/>    # Grab data from the HTTP request<br/>    ...<br/>    model.predict(...)<br/>    ...</pre><ul class="postList"><li class="graf-after--pre" id="1225"><strong class="markup--li-strong">TensorFlow</strong>: no coding required! You can use <a href="https://www.tensorflow.org/serving/" target="_blank"><strong class="markup--li-strong">TensorFlow Serving</strong></a> to serve predictions at scale. Once you’ve trained your model and saved it to the proper format, all it takes to serve predictions is:</li></ul><pre class="graf--pre" id="34b5"><code class="markup--code markup--pre-code">docker run -p 8500:8500 \<br/>--mount type=bind,source=/tmp/myModel,target=/models/myModel \<br/>-e MODEL_NAME=myModel -t tensorflow/serving &amp;</code></pre><ul class="postList"><li class="graf-after--pre" id="2b60"><strong class="markup--li-strong">Apache MXNet</strong>: in a similar way, Apache MXNet provides a <a href="https://github.com/awslabs/mxnet-model-server" target="_blank"><strong class="markup--li-strong">model server</strong>,</a> able to serve MXNet and <a href="https://onnx.ai/" target="_blank"><strong class="markup--li-strong">ONNX</strong></a> models (the latter is a common format supported by PyTorch, Caffe2 and more). It can either run as a stand-alone application, or <a href="https://github.com/awslabs/mxnet-model-server/blob/master/docker/README.md" target="_blank">inside a Docker container</a>.</li></ul><p id="5ab6">Both model servers are pre-installed on the <strong class="markup--p-strong">Deep Learning AMI: </strong>that’s another reason to use it. To keep things simple, you could leave your pre/post-processing in the application and invoke the model deployed by the model server. A word of warning, however: these models servers implement neither authentication nor throttling, so please make sure not to expose them directly to Internet traffic.</p><ul class="postList"><li id="0e66"><strong class="markup--li-strong">Anything else</strong>: if you’re using another environment (say, custom code) or non-web architectures (say, message passing), the same pattern should apply: build a separate service that can be <strong class="markup--li-strong">deployed and scaled independently</strong>.</li></ul><h4 id="0d4e">(Optional) Containerize your application</h4><p id="77ca">Since you’ve decided to split your code, I would definitely recommend that you use the opportunity to package the different pieces in Docker containers: one for <strong class="markup--p-strong">training</strong>, one for <strong class="markup--p-strong">prediction</strong>, one (or more) for the <strong class="markup--p-strong">application</strong>. It’s not strictly necessary at this stage, but if you can spare the time, I believe the premature investment is worth it.</p><blockquote class="graf--blockquote graf--hasDropCapModel" id="4746">If you’ve been living under a rock or never really paid attention to containers, now’s probably the time to catch up:) I highly recommend running the <a class="markup--blockquote-anchor" href="https://docs.docker.com/get-started/" target="_blank">Docker tutorial</a>, which will teach you everything you need to know for our purpose.</blockquote><p class="graf-after--blockquote" id="fa9c">Containers make it easy to <strong class="markup--p-strong">move code across different environments</strong> (dev, test, prod, etc.) and instances. They solve all kinds of dependency issues, which tend to pop up even if you’re only managing a small number of instances. Later on, containers will also be a pre-requisite for larger-scale solutions such as Docker clusters or Amazon SageMaker.</p><h3 id="2d30">End of week 2</h3><p id="a52d">After a rough start, things are looking up!</p><ul class="postList"><li id="7da4">The Deep Learning AMI provides a stable, well-maintained foundation to build on.</li><li id="47a3">Containers help you move and deploy your application with much less infrastructure drama than before.</li><li id="98b1">Prediction now lives outside of your application, making testing, deployment and scaling simpler.</li><li id="4c6a">If you can use them, model servers save you most of the trouble of writing a prediction service.</li></ul><p id="de8a">Still, don’t get too excited. Yes, we’re back on track and ready to for bigger things, but there’s still a ton of work to do. What about <strong class="markup--p-strong">scaling prediction to multiple instances,</strong> <strong class="markup--p-strong">high availability</strong>, <strong class="markup--p-strong">managing cost</strong>, etc. And what should we do when mountains of training data start piling up? <strong class="markup--p-strong">Face it, we’ve barely scratched the surface</strong>.</p><p id="e815">“<em class="markup--p-em">Old fool! Load balancers! Auto Scaling! Automation!</em>”, I hear you cry. Oh, you mean you’re in a hurry to manage infrastructure again? I thought you guys wanted to to Machine Learning ;)</p><p id="c500">On this bombshell, it’s time to call it a day. <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-2-80b0d1d7fc61" target="_blank">In the next post</a>, we’ll start comparing and challenging options for larger-scale ML training: <strong class="markup--p-strong">EC2</strong> vs. <strong class="markup--p-strong">ECS/EKS</strong> vs <strong class="markup--p-strong">SageMaker</strong>. An epic battle, no doubt.</p><p id="614c">Thanks for reading. Agree? Disagree? Great! Happy to discuss here or on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a>.</p></div></div></section><section class="section"><div><hr/></div><div><div><p id="edb2"><em class="markup--p-em">Best movie soundtrack ever. Yes, The Fellowship of the Ring only comes second :)</em></p><figure id="0a75"><iframe frameborder="0" height="480" scrolling="no" src="https://www.youtube.com/embed/j7VKwS5K-Hc?feature=oembed" width="640"></iframe></figure></div></div></section>
</section>
</article></body></html>