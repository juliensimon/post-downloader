<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Scaling Machine Learning from 0 to millions of users — part 2</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="330c">Scaling Machine Learning from 0 to millions of users — part 2</h3><h4 class="graf--subtitle" id="ada6">Training: EC2, EMR, ECS, EKS or SageMaker?</h4><p id="82dd">In <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" target="_blank">part 1</a>, we broke out of the laptop, and decided to deploy our prediction service on a virtual machine. By doing so, we discussed a few simple techniques that helped with initial scalability… and hopefully with reducing manual ops. Since then, despite a few production hiccups due the lack of high availability, life has been pretty good.</p><p id="bb3b">However, traffic soon starts to increase, data piles up, more models need to be trained, etc. Technical and business stakes are getting higher, and let’s face it, the current architecture will go underwater soon. Time’s up: in this post, we’ll focus on <strong class="markup--p-strong">scaling training to a large number of machines</strong>.</p><figure id="b039"><img class="graf-image" src="image04.webp"/><figcaption>Two hundred… and fifty… six… GPUs!!! Ja!</figcaption></figure><blockquote class="graf--blockquote graf--hasDropCapModel" id="decd">This is an opinionated series, remember? It’s also based on what I hear when talking to real-life customers, not ideal ones. Reality is often ugly, and only tech hipsters (and top management) think that it looks exactly like that fancy article your read on &lt;insert_name_here&gt; ;)</blockquote><blockquote class="graf--blockquote graf--hasDropCapModel graf-after--blockquote" id="4f15">Special thanks to rockin’ Evangelists Abby Fuller, Jerry Hargrove, Adrian Hornsby, Brent Langston and Ian Massingham for their tips and ideas.</blockquote><h3 class="graf-after--blockquote" id="3dcc"><strong class="markup--h3-strong">Scaling up will fix it… right?</strong></h3><p id="4f1f">Yes and no. Yes, it can be a <strong class="markup--p-strong">short-term solution</strong> to use a large server for training and prediction. Amazon EC2 has a ton of <a href="https://aws.amazon.com/ec2/instance-types/" target="_blank">instance types</a> to pick from, and all it takes is <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/stop-instances.html" target="_blank">stopping</a> your instance, <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/modify-instance-attribute.html" target="_blank">changing the instance type</a>, and <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/start-instances.html" target="_blank">starting</a> it again.</p><blockquote class="graf--pullquote" id="b704">You should only be using APIs from now on. When it comes to scaling tech &amp; ops, manual work in any GUI is the Antichrist.</blockquote><p class="graf-after--pullquote" id="ba67">No, because it’s only a <strong class="markup--p-strong">temporary solution</strong>:</p><ul class="postList"><li id="3b94">It’s fine to use bigger instances, until <strong class="markup--li-strong">there is no bigger instance</strong>. Then what?</li><li id="bd7f">It’s also quite possible that <strong class="markup--li-strong">your workload won’t scale nicely</strong>, and won’t make full use of the additional hardware (RAM, CPU cores, I/O, etc.). A marginal performance gain isn’t worth the extra spend.</li><li id="fcff">Most of all, <strong class="markup--li-strong">scaling up will simply delay the inevitable</strong>. Keep doing it, and the only thing you’ll end up with is a bigger problem to solve.</li></ul><p id="9023">In the spirit of avoiding over-engineering, it’s OK to scale up a couple of times, but if monitoring keeps pointing at the Impassable Wall of Scalability Doom, I’d advise you to <strong class="markup--p-strong">act a little too early rather than a little too late</strong>: things scale linearly until they don’t, and you don’t want to find out what happens when the exponential starts rising!</p><h3 id="bfac">Scaling out</h3><p id="bf7d">When it comes to training Machine Learning (ML) models, the top requirements are actually pretty simple:</p><ul class="postList"><li id="517c"><strong class="markup--li-strong">Reliable, scalable storage </strong>for your data sets.</li><li id="c33a"><strong class="markup--li-strong">Elastic compute clusters</strong>, that can be started on-demand in lots of different configurations (hardware, frameworks, etc.).</li><li id="9332"><strong class="markup--li-strong">As little ops as possible: </strong>ML is what you should focus on, because ML is what turns your raw data into revenue / profit / improver customer experience.</li></ul><blockquote class="graf--blockquote graf--hasDropCapModel" id="20c6">“That’s not the full list! We want total control, no lock-in, low bills, top performance… and everything else too, whatever it is”. Yes, yes, we’ll get there :)</blockquote><h3 class="graf-after--blockquote" id="0068">Storage</h3><p id="2ed6">Let’s get that one out of the way: <strong class="markup--p-strong">your data goes to Amazon S3</strong>. Any other choice would need a bullet-proof justification (try me!). Throughput? Well, you now get up to <strong class="markup--p-strong">25 Gigabit per second between EC2 and S3</strong>. That should enough for now. Scalability? High availability? Security? No ops? Cost? Check.</p><div class="graf--mixtapeEmbed" id="e0b5"><a class="markup--mixtapeEmbed-anchor" href="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/" title="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/"><strong class="markup--mixtapeEmbed-strong">The Floodgates Are Open - Increased Network Bandwidth for EC2 Instances | Amazon Web Services</strong><br/><em class="markup--mixtapeEmbed-em">I hope that you have configured your AMIs and your current-generation EC2 instances to use the Elastic Network Adapter…</em>aws.amazon.com</a><a class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="93be5551fa6453f2c6979b4857a7bb20" href="https://aws.amazon.com/blogs/aws/the-floodgates-are-open-increased-network-bandwidth-for-ec2-instances/"></a></div><blockquote class="graf--blockquote graf--hasDropCapModel graf-after--mixtapeEmbed" id="f181">At this point, anyone in your team coming up with a “high performance storage cluster based on this super cool open source project” should be hit on the head with a heavy object until they stop moving. No mercy for wasting time, putting projects at risk, and over-engineering.</blockquote><figure class="graf-after--blockquote" id="c88c"><img class="graf-image" src="image05.webp"/><figcaption>Because “the doc wasn’t clear” and “bugs happen to everyone”.</figcaption></figure><p id="c5c7">OK, now what about compute options? <strong class="markup--p-strong">Amazon EC2? Amazon EMR? Container services? Amazon SageMaker?</strong></p><blockquote class="graf--pullquote" id="c09a">Begun the Scaling War has.</blockquote><h3 class="graf-after--pullquote" id="4732">Amazon EC2</h3><p id="7192">Our journey started on EC2, so it can be quite tempting to continue there. Is that a good enough reason?</p><p id="dc08">As discussed in <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" target="_blank">part 1</a>, the <a href="https://aws.amazon.com/machine-learning/amis/" target="_blank"><strong class="markup--p-strong">Deep Learning AMI</strong></a> makes your life much simpler. It’s packed with open source tools and libraries optimized by AWS (<a href="https://aws.amazon.com/about-aws/whats-new/2018/10/chainer4-4_theano_1-0-2_launch_deep_learning_ami/?nc1=h_ls" target="_blank">11x speedup</a> on TensorFlow 1.11, anyone? Or maybe <a href="https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/" target="_blank">linear scaling up to 256 GPUs</a>?). <a href="https://github.com/uber/horovod" target="_blank"><strong class="markup--p-strong">Horovod</strong></a>, a popular library for distributed training, is also <a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-tensorflow.html" target="_blank">included</a>. All of this will make it much easier to setup efficient, distributed training clusters.</p><p id="fb38">Some constraints may prevent you from using that <strong class="markup--p-strong">AWS-maintained AMI</strong>: need for a specific OS vendor, licensing restrictions, having to run identical builds across different providers, etc. Unfortunately, you’ll have to set everything up yourself. Don’t underestimate that task: you’ll have to do it again and again as new versions are released. Even with automation, that’ll never be a lights-out operation.</p><p id="d7a1">Whichever AMI you use, setting up a training cluster means:</p><ol class="postList"><li id="832c">Firing up a bunch of instances,</li><li id="3b9f">Picking one as the leader, and setting up distributed training. That usually involves listing hostnames / IP addresses of other machines in the cluster.</li><li id="b07e">Start training,</li><li id="0189">Once training is complete, grab the trained model and save it in Amazon S3.</li><li id="cfba">Shut the training cluster down.</li></ol><p id="26a6">Quite a bit of work, then, which is probably why most of you go through steps 1 and 2 once, <strong class="markup--p-strong">run the cluster 24/7</strong>, and never make it to step 5… And this, ladies and gentlemen, is my main concern for using EC2 here. <strong class="markup--p-strong">Unless you automate all of this</strong> (with <a href="http://aws.amazon.com/cloudformation" target="_blank">AWS CloudFormation</a>, <a href="https://www.terraform.io" target="_blank">Terraform</a>, CLI scripts, etc.), <strong class="markup--p-strong">you will waste a ton of money</strong>. Someone up above will quickly put a cap on your budget, meaning that you’ll probably end up with a fixed-size cluster that needs to be time-shared by multiple developers / teams… and of course, someone will develop a nice intranet page to book time slots on the cluster. Congratulations, you’re reinvented the mainframe!</p><figure id="e256"><img class="graf-image" src="image03.webp"/><figcaption>Resnet-50 training in 3… 2… 1…</figcaption></figure><blockquote class="graf--blockquote graf--hasDropCapModel" id="4ebf">Don’t laugh. I’ve met a very large — and otherwise brilliant — AI company managing hundreds of physical GPU servers just like this. Unfortunately, I’m sure some customers do the same on EC2… Get in touch if you’re stuck there, we can help!</blockquote><p class="graf-after--blockquote" id="a20c">Of course, maybe your DevOps team was kind enough to provide <strong class="markup--p-strong">an all-singing, all-dancing cluster provisioning script</strong> that each developer can run to get their own training cluster (buy them lots of beer: that rarely ever happens). Would that be a good enough reason to stick to EC2 for training?</p><p id="a9fb">Maybe. Techniques for cost optimization on EC2 are well-known: <a href="https://aws.amazon.com/ec2/pricing/reserved-instances/" target="_blank">reserved instances</a>, <a href="https://aws.amazon.com/ec2/spot/" target="_blank">spot instances</a>, etc. Doing that right may offset the extra DevOps costs. Only you can find out, and you definitely should.</p><blockquote class="graf--pullquote graf--hasDropCapModel" id="d77e"><strong class="markup--pullquote-strong">If you don’t have time or skills to get automation and cost optimization right, I’d think twice about running training jobs at scale on EC2.</strong></blockquote><h3 class="graf-after--pullquote" id="ba95"><strong class="markup--h3-strong">Amazon EMR</strong></h3><p id="224b"><a href="http://aws.amazon.com/emr" target="_blank">Amazon EMR</a> in a ML discussion? Well, yes: <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-tensorflow.html" target="_blank">TensorFlow</a> and <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-mxnet.html" target="_blank">Apache MXNet</a> are part of the EMR distribution, and of course, <a href="https://spark.apache.org/mllib/" target="_blank">Spark MLlib</a> is also included. EMR supports <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-supported-instance-types.html" target="_blank">compute-optimized instances</a> (c5 and p3), so it looks like we have everything we need.</p><p id="ca99">Here are <strong class="markup--p-strong">a few good reasons to run training jobs on EMR</strong>:</p><ul class="postList"><li id="acba">You already use EMR for other tasks, with solid automation (on-demand clusters, <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-work-with-steps.html" target="_blank">steps</a>, etc.) and cost optimization (<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html" target="_blank">spot instances</a>!).</li><li id="7adf">Your data requires a lot of ETL, and Hive / Spark would work great for that. Why not run everything in one place?</li><li id="d0e0">Spark MLlib has the algos you need.</li><li id="acb4">You read somewhere that there a <a href="https://github.com/aws/sagemaker-spark" target="_blank">SageMaker SDK for Spark</a>, so that could be another option for the future.</li></ul><p id="841d">And <strong class="markup--p-strong">equally good reasons NOT to do it</strong>:</p><ul class="postList"><li id="3b6a">You don’t have time or skills to automate and optimize costs. GPU-based EMR clusters running 24/7 at on-demand price with zero load <em class="markup--li-em">&lt;rolls a D100 for sanity check… &gt;</em></li><li id="c5c3">You’re using neither TensorFlow, Apache MXNet nor Spark MLlib. Yes, you could install additional packages to your clusters, but that’s extra work.</li><li id="4a5f">Your ETL and ML jobs have <strong class="markup--li-strong">conflicting instance requirements</strong>. Let’s say that they would respectively run best on 8 <em class="markup--li-em">r5.4xlarge</em> and 2 <em class="markup--li-em">p3.8xlarge</em> for training. How do you compromise? That’s a hard call, and you may end up picking an instance type that’s suboptimal for both ETL and training… or creating a dedicated GPU cluster (another one to manage and worry about).</li></ul><p id="253f">I have <strong class="markup--p-strong">mixed feelings</strong> about this: I’d be fine with piling some amount of ML on top on an existing cluster, but <strong class="markup--p-strong">unless it was 100% based on Spark MLlib, scaling it simply wouldn’t feel right</strong>.</p><h3 id="62f4">Container services</h3><p id="f0ec">In <a href="https://medium.com/@julsimon/scaling-machine-learning-from-0-to-millions-of-users-part-1-a2d36a5e849" target="_blank">part 1</a>, I suggested early one that you containerize your code in order to solve deployment issues. Obviously, this would also pay dividends when deploying to Docker clusters, whether on <a href="http://aws.amazon.com/ecs" target="_blank"><strong class="markup--p-strong">Amazon ECS</strong></a> or <a href="http://aws.amazon.com/eks" target="_blank"><strong class="markup--p-strong">Amazon EKS</strong></a>.</p><p id="33a0">You can <strong class="markup--p-strong">run any workload</strong> in a Docker container, and you can <strong class="markup--p-strong">move it around </strong>without any restriction, from your laptop to your production environment (or so I’m told). When running on AWS, costs can be squeezed with auto scaling, reserved instances, spot instances, etc. Woohoo.</p><p id="eaaa">From a training perspective, containers give you <strong class="markup--p-strong">full flexibility</strong> to use any open source library, or even your own custom code. All popular ML/DL libraries provide <strong class="markup--p-strong">base images</strong>, which you can either run directly or customize, and these will save you a lot of time.</p><p id="5ecd">Thanks to auto scaling now supporting <a href="https://aws.amazon.com/blogs/aws/new-ec2-auto-scaling-groups-with-multiple-instance-types-purchase-options/" target="_blank">mixed instances types</a>, <strong class="markup--p-strong">different instance types can coexist within the same cluster. </strong>Thus, you can easily add compute-optimized instances to any cluster and schedule your ML/DL trainings there. Of course, you can also create a dedicated cluster for training if you think that makes more sense.</p><p id="3e2b">Last but not least, <strong class="markup--p-strong">GPU instances</strong> are supported on both services, with <strong class="markup--p-strong">GPU-optimized AMIs</strong> to boot (nvidia-docker, NVIDIA drivers, etc).</p><h4 id="e1b4">Training on Amazon ECS</h4><p id="f0a4">To minimize training time and cost, you need to make sure that training jobs run on the <strong class="markup--p-strong">most appropriate instance type</strong> (say <em class="markup--p-em">c5</em> or <em class="markup--p-em">p3</em>). Amazon ECS lets you add <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html" target="_blank">placement constraints</a> in <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html" target="_blank">task definitions</a>. Here’s how we would ask ECS to schedule this task only on <em class="markup--p-em">p3</em> instances.</p><pre class="graf--pre" id="7126"><code class="markup--code markup--pre-code">"placementConstraints": [<br/>    {<br/>        "expression": "attribute:ecs.instance-type =~ p3.*",<br/>        "type": "memberOf"<br/>    }<br/>]</code></pre><p class="graf-after--pre" id="bbbb">We can also one step further, thanks to a new feature that lets <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-gpu.html" target="_blank"><strong class="markup--p-strong">pin a specific number of GPUs</strong></a> to a given task, e.g:</p><pre class="graf--pre" id="52fa">{ "containerDefinitions": [ <br/>    { "resourceRequirements" : [ <br/>       { "type" : "GPU", "value" : <code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0"><em class="markup--pre-em">"</em>2</code>" }<br/>      ],<br/>    },<br/>   ... <br/>}</pre><p class="graf-after--pre" id="8e20">Here’s a nice blog post with additional details.</p><div class="graf--mixtapeEmbed" id="5af8"><a class="markup--mixtapeEmbed-anchor" href="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/" title="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/"><strong class="markup--mixtapeEmbed-strong">Scheduling GPUs for deep learning tasks on Amazon ECS | Amazon Web Services</strong><br/><em class="markup--mixtapeEmbed-em">This post is contributed by Brent Langston - Sr. Developer Advocate, Amazon Container Services Last week, AWS announced…</em>aws.amazon.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6fc0f603fdb3115007783858c6dc9a0b" data-thumbnail-img-id="0*nOtoUfH3SK6htXVs" href="https://aws.amazon.com/blogs/compute/scheduling-gpus-for-deep-learning-tasks-on-amazon-ecs/" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*nOtoUfH3SK6htXVs);"></a></div><h4 class="graf-after--mixtapeEmbed" id="604c">Training on Amazon EKS</h4><p id="6688">You can pretty much do the same thing on EKS. This nice blog post will walk you through the whole process of adding <em class="markup--p-em">p3</em> worker nodes to an existing cluster, defining a GPU-powered pod, and launching it on the cluster.</p><div class="graf--mixtapeEmbed" id="5422"><a class="markup--mixtapeEmbed-anchor" href="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/" title="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/"><strong class="markup--mixtapeEmbed-strong">Running GPU-Accelerated Kubernetes Workloads on P3 and P2 EC2 Instances with Amazon EKS | Amazon…</strong><br/><em class="markup--mixtapeEmbed-em">This post contributed by Scott Malkie, AWS Solutions Architect Amazon EC2 P3 and P2 instances, featuring NVIDIA GPUs…</em>aws.amazon.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="dce9b4039f609b693e2419d1419ff5ae" data-thumbnail-img-id="0*gAJ-l2__7uvA8Bsc" href="https://aws.amazon.com/blogs/compute/running-gpu-accelerated-kubernetes-workloads-on-p3-and-p2-ec2-instances-with-amazon-eks/" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*gAJ-l2__7uvA8Bsc);"></a></div><h4 class="graf-after--mixtapeEmbed" id="0513">Container services for ML/DL training, yes or no?</h4><blockquote class="graf--blockquote graf--hasDropCapModel" id="bef0">If you belong to the Wild Hyperborean Horde who’d rather eat frozen Yack poo than NOT use home-made containers for absolutely everything, you’ve already answered the question, haven’t you? ;)</blockquote><figure class="graf-after--blockquote" id="234e"><img class="graf-image" src="image01.webp"/><figcaption>Yes, for a very short while. And then you got your guts ripped out. Hmm.</figcaption></figure><p id="9b91">If the need ever arose, <strong class="markup--p-strong">I wouldn’t worry about scaling container services to a large number of nodes</strong> (ECS was designed to <a href="https://www.allthingsdistributed.com/2015/07/under-the-hood-of-the-amazon-ec2-container-service.html" target="_blank">scale linearly to 1,000+ nodes</a>). However, once again, you should very much worry about your ability to <strong class="markup--p-strong">scale ops</strong> (containers, clusters, etc.) and <strong class="markup--p-strong">manage cost</strong>.</p><p id="5410">If you work in a Docker shop where another team is managing clusters, providing you with <strong class="markup--p-strong">agile, automated and cost-effective ways to provision</strong> them, then sure. It could be as easy as committing a TensorFlow script, and then letting a CI/CD pipeline deploy it automatically to a cluster. Not a lot of extra work for ML developers and data scientists.</p><p id="c077">Now, if you live a world where you have to <strong class="markup--p-strong">build and operate clusters on top of your actual ML job</strong>, that’s not such a exciting proposition any more. Plumbing, large bills, fire, brimstone… you know the story.</p><p id="7083">Let’s look at setting up and managing large, distributed training jobs with Horovod: here’s the <a href="https://github.com/uber/horovod/blob/master/docs/running.md" target="_blank">documentation</a>. Once you’ve got everything figured out (<a href="https://github.com/uber/horovod/blob/master/docs/docker.md" target="_blank">Docker</a>, <a href="https://github.com/kubeflow/kubeflow/blob/master/kubeflow/openmpi/" target="_blank">Kubeflow</a>, <a href="https://github.com/kubeflow/mpi-operator/" target="_blank">MPI Operator</a>, <a href="https://github.com/kubernetes/charts/tree/master/stable/horovod/" target="_blank">Helm Chart</a>, and <a href="https://github.com/IBM/FfDL/tree/master/etc/examples/horovod/" target="_blank">FfDL</a>), here’s how to run a training job on 4 machines with 4 GPUs each:</p><pre class="graf--pre" id="760b">$ mpirun -np 16 \<br/>    -H server1:4,server2:4,server3:4,server4:4 \<br/>    -bind-to none -map-by slot \<br/>    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \<br/>    -mca pml ob1 -mca btl ^openib \<br/>    python train.py</pre><p class="graf-after--pre" id="760c">Don’t get me wrong: Docker, Kubernetes, Horovod and so on are impressive<strong class="markup--p-strong"> </strong>projects, but if you insist on building and maintaining everything yourself (or if Hyperborean Harald sneers that it’s “<em class="markup--p-em">the only proper way to do it</em>”), you should know what you’re stepping into, as you will be using this all day long.</p><blockquote class="graf--pullquote" id="f535"><strong class="markup--pullquote-strong">Is this what you really need? </strong>Maybe, maybe not. <strong class="markup--pullquote-strong">Please make up your own mind.</strong></blockquote><h3 class="graf-after--pullquote" id="e0b6">Amazon SageMaker</h3><p id="bace">One more option to go: <a href="http://aws.amazon.com/sagemaker" target="_blank">Amazon SageMaker</a>. I’ve discussed it at lengths in previous posts and talks (start <a href="https://medium.com/@julsimon/talk-from-notebook-to-production-with-amazon-sagemaker-ee2a2036c0fe" target="_blank">here</a> for an recent overview), and as the most recent service of the bunch, I’ve saved it for last to see where it improves on previous options with respect to training large jobs.</p><p id="e701">A quick reminder:</p><ul class="postList"><li id="9402">All activity in SageMaker is driven by a <strong class="markup--li-strong">high-level </strong><a href="https://github.com/aws/sagemaker-python-sdk" target="_blank"><strong class="markup--li-strong">Python SDK</strong></a>.</li><li id="c505">Training is based on on-demand, fully-managed instances. <strong class="markup--li-strong">Zero</strong> infrastructure work. Spot instances are not available.</li><li id="b3c7">Models may be trained using <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html" target="_blank"><strong class="markup--li-strong">AWS-maintained built-in algorithms</strong></a> and <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html" target="_blank"><strong class="markup--li-strong">optimized frameworks</strong></a> (same ones as in the Deep Learning AMI), as well as <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html" target="_blank"><strong class="markup--li-strong">custom algorithms</strong></a>.</li><li id="70ce">Distributed training is built-in. <strong class="markup--li-strong">Zero</strong> setup.</li><li id="6493">Plenty of <strong class="markup--li-strong">examples</strong> are available <a href="https://github.com/awslabs/amazon-sagemaker-examples" target="_blank">here</a>.</li></ul><p id="73e5">For instance, here’s how you would <strong class="markup--p-strong">train a TensorFlow model</strong>. First, put your data in Amazon S3 (it’s hopefully already there). Then, configure your training job: pass your code, the instance type, the number of instances. Finally, call <em class="markup--p-em">fit()</em>.</p><pre class="graf--pre" id="36ad">from sagemaker.tensorflow import TensorFlow<br/><br/>tf_model = TensorFlow(entry_point='model.py',<br/>                             role=role,<br/>                             framework_version='1.12.0',<br/>                             training_steps=1000, <br/>                             evaluation_steps=100,<br/>                             train_instance_count=2,<br/>                             train_instance_type='ml.c4.xlarge')<br/><br/>tf_model.fit(inputs)</pre><p class="graf-after--pre" id="2bbc">That’s it. <strong class="markup--p-strong">This is at least 10x (100x ?) less code than any automation you’d be using with EC2 or containers</strong>.</p><p id="abe0">And lock-in? Well, none: you’re free to <strong class="markup--p-strong">take your TensorFlow code and run it anywhere</strong> else.</p><h4 id="9a6f">EC2 or SageMaker?</h4><p id="52e2">Compared to EC2, <strong class="markup--p-strong">SageMaker saves you from managing any infrastructure</strong>, and probably a lot of framework containers too. That might not be a big deal when you’re working with a couple of instances, but it sure it when you start scaling to tens or hundreds, running all kinds of different jobs.</p><p id="062c">SageMaker also terminates training clusters automatically once training is complete.</p><blockquote class="graf--pullquote" id="6df4"><strong class="markup--pullquote-strong">You will never overpay for training</strong>.</blockquote><p class="graf-after--pullquote" id="6106">Yes, SageMaker instances are more expensive than EC2 instances. However, if you factor in less ops and automatic termination, I’d be really surprised if the gap wasn’t significantly reduced.</p><blockquote class="graf--pullquote" id="7c45">Total cost of ownership is what matters.</blockquote><h4 class="graf-after--pullquote" id="b167">EMR or SageMaker?</h4><p id="14c5">As mentioned earlier, I don’t see any compelling reason to use EMR at scale for training unless you stick to Spark MLlib. Still, if you’re asking yourself the question, you’re probably already using EMR... so how about both? As it happens, SageMaker also includes a <a href="https://github.com/aws/sagemaker-spark" target="_blank">Spark SDK</a>. I’ve covered this topic before.</p><div class="graf--mixtapeEmbed" id="09b8"><a class="markup--mixtapeEmbed-anchor" href="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7" title="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7"><strong class="markup--mixtapeEmbed-strong">Mixing Spark with Sagemaker ?</strong><br/><em class="markup--mixtapeEmbed-em">This short post comes from a question asked by Manel Maragal (thanks!) on my YouTube channel. It’s a really good…</em>medium.com</a><a class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="e4bb9a19d5b9a5107b19b5eb5c4d1aa2" href="https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7"></a></div><div class="graf--mixtapeEmbed graf-after--mixtapeEmbed" id="7ae0"><a class="markup--mixtapeEmbed-anchor" href="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243" title="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243"><strong class="markup--mixtapeEmbed-strong">Apache Spark and Amazon SageMaker, the Infinity Gems of analytics</strong><br/><em class="markup--mixtapeEmbed-em">In a previous post, I showed you how to build a spam classifier by running PySpark on an Amazon SageMaker notebook…</em>medium.com</a><a class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="be04f50314b23f24ff9403ba470e7300" data-thumbnail-img-id="0*IMN1uGhTOkn7ccgv." href="https://medium.com/@julsimon/apache-spark-and-amazon-sagemaker-the-infinity-gems-of-analytics-8bd780b07243" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*IMN1uGhTOkn7ccgv.);"></a></div><p class="graf-after--mixtapeEmbed" id="f9f5">The short version is: separate concerns.</p><blockquote class="graf--pullquote" id="6f61"><strong class="markup--pullquote-strong">Spark for large-scale ETL</strong>, <strong class="markup--pullquote-strong">SageMaker for large-scale training</strong>.</blockquote><h4 class="graf-after--pullquote" id="3441">Container services or SageMaker?</h4><p id="1125">There is a myriad of technical details that separate these two approaches, but at the end of the day, I think the choice does come down to <strong class="markup--p-strong">engineering culture </strong>and <strong class="markup--p-strong">focus</strong>.</p><p id="28d7">Some teams are convinced <strong class="markup--p-strong">doing everything themselves creates value for the company</strong> ( in some cases, it does), and some other teams would rather <strong class="markup--p-strong">rely on managed services in order to iterate as fast as possible</strong>. Some teams feel better about <strong class="markup--p-strong">putting all their eggs in a single basket</strong> (“we run everything on Docker clusters”), some other teams are happier with using <strong class="markup--p-strong">different services for different things</strong>. No one but them can judge what’s best for their particular use case.</p><blockquote class="graf--blockquote" id="8b3e">My personal choice would still go to SageMaker, because unlike ECS and EKS, <strong class="markup--blockquote-strong">SageMaker is built for Machine Learning only: the team is obsessed with simplifying and optimizing the service for ML users, and ML users only</strong>. No offence to the ECS and EKS teams, but their focus is different, as they have to accommodate literally every possible workload.</blockquote><p class="graf-after--blockquote" id="562a">These services are all based on containers anyway, and if you’re able to run distributed TensorFlow with Horovod on Kubernetes, the SageMaker SDK will feel like a breeze! Give it a try and let me know what you think.</p><p id="55bc">That’s the end of the second part. In the next post, we’ll talk about optimizing training from a framework perspective. Plenty more to come, we haven’t even talked about prediction yet!</p><p id="e56c">As always, thanks for reading. Agree? Disagree? Great! Happy to discuss here or on <a href="https://twitter.com/julsimon" rel="noopener nofollow noopener" target="_blank">Twitter</a>.</p><figure id="1ef4"><img class="graf-image" src="image06.webp"/></figure><p id="9b5e"><strong class="markup--p-strong">Follow us on </strong><a href="https://twitter.com/joinfaun" target="_blank"><strong class="markup--p-strong">Twitter</strong></a><strong class="markup--p-strong"> </strong>🐦<strong class="markup--p-strong"> and </strong><a href="https://www.facebook.com/faun.dev/" target="_blank"><strong class="markup--p-strong">Facebook</strong></a><strong class="markup--p-strong"> </strong>👥<strong class="markup--p-strong"> and join our </strong><a href="https://www.facebook.com/groups/364904580892967/" target="_blank"><strong class="markup--p-strong">Facebook Group</strong></a><strong class="markup--p-strong"> </strong>💬<strong class="markup--p-strong">.</strong></p><p id="c972"><strong class="markup--p-strong">To join our community Slack </strong>🗣️ <strong class="markup--p-strong">and read our weekly Faun topics </strong>🗞️,<strong class="markup--p-strong"> click here⬇</strong></p></div><div><figure id="83d1"><a href="https://www.faun.dev/join/?utm_source=medium.com%2Ffaun&amp;utm_medium=medium&amp;utm_campaign=faunmediumbanner" target="_blank"><img class="graf-image" src="image02.webp"/></a></figure></div><div><h4 id="3062">If this post was helpful, please click the clap 👏 button below a few times to show your support for the author! ⬇</h4></div></div></section>
</section>
</article></body></html>
