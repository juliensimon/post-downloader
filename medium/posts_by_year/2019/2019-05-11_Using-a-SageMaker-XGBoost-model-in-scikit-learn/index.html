<!DOCTYPE html>
<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Using a SageMaker XGBoost model in scikit-learn</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">


<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="df0a">Using a SageMaker XGBoost model in scikit-learn</h3><p id="701c">This is a quick post answering a question I get a lot: “<em class="markup--p-em">how can I use in scikit-learn an XGBoost model that I trained on SageMaker?</em>”.</p><p id="898d">Here it goes. Once you’ve trained your <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html" target="_blank">XGBoost</a> model in SageMaker (examples <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms" target="_blank">here</a>), grab the training job name and the location of the model artifact.</p><blockquote class="graf--blockquote" id="5a70">I’m using the CLI here, but you can of course use any of the AWS language SDKs.</blockquote><pre class="graf--pre graf-after--blockquote" id="8db2">$ export TRAINING_JOB_NAME='xgboost-190511-0830-010-14f41137'</pre><pre class="graf--pre graf-after--pre" id="65be">$ export MODEL_ARTIFACT=`aws sagemaker describe-training-job \<br/>--training-job-name $TRAINING_JOB_NAME \<br/>--query ModelArtifacts.S3ModelArtifacts \<br/>--output text`<br/><br/>$ echo $MODEL_ARTIFACT<br/>s3://sagemaker-eu-west-1-ACCOUNT_NUMBER/sagemaker/DEMO-hpo-xgboost-dm/output/xgboost-190511-0830-010-14f41137/output/model.tar.gz</pre><p class="graf-after--pre" id="9311">Then, download the artifact and extract the model.</p><pre class="graf--pre" id="c1d3">$ aws s3 cp $MODEL_ARTIFACT .</pre><pre class="graf--pre graf-after--pre" id="a1a7">$ tar xvfz model.tar.gz<br/>x xgboost-model</pre><p class="graf-after--pre" id="f3af">The model is a pickled Python object, so let’s now switch to Python and load the model.</p><pre class="graf--pre" id="59d9">$ python3<br/>&gt;&gt;&gt; import sklearn, pickle<br/>&gt;&gt;&gt; model = pickle.load(open("xgboost-model", "rb"))<br/>&gt;&gt;&gt; type(model)<br/>&lt;class 'xgboost.core.Booster'&gt;</pre><p class="graf-after--pre" id="e3d8">You’re done. From now on, you can use the model as if you’d trained it locally. For example, you can dump it and visualize it.</p><pre class="graf--pre" id="778b">&gt;&gt;&gt; model.dump_model('model.txt')<br/>&gt;&gt;&gt; exit()</pre><pre class="graf--pre graf-after--pre" id="5a20">$ head model.txt<br/>booster[0]:<br/>0:[f2&lt;512] yes=1,no=2,missing=1<br/> 1:[f1&lt;3.5] yes=3,no=4,missing=3<br/>  3:[f2&lt;1.5] yes=7,no=8,missing=7<br/>   7:[f42&lt;0.5] yes=15,no=16,missing=15<br/>    15:leaf=0.508301735<br/>    16:leaf=1.51004589<br/>   8:leaf=1.72906268<br/>  4:[f52&lt;0.5] yes=9,no=10,missing=9<br/>   9:leaf=1.39554036</pre><p class="graf-after--pre" id="61b6">See? That was super easy :)</p><p id="dc25">Thanks for reading. Happy to answer questions here or on <a href="https://twitter.com/julsimon" target="_blank">Twitter</a>.</p></div></div></section>
</section>
</article></body></html>
