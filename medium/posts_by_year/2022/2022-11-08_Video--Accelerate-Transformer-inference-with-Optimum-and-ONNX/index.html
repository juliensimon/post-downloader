<!DOCTYPE html>
<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Video: Accelerate Transformer inference with Optimum and ONNX</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">


<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="2eec">Video: Accelerate Transformer inference with Optimum andÂ ONNX</h3><p id="bfad">In this video, I show you how to accelerate Transformer inference with <a href="https://github.com/huggingface/optimum" target="_blank">Optimum</a>, an open source library by <a href="http://huggingface.co" target="_blank">Hugging Face</a>, and ONNX.</p><p id="d08a">I start from a DistilBERT model fine-tuned for text classification, export it to ONNX format, then optimize it, and finally quantize it. Running benchmarks on an AWS c6i instance (Intel Ice Lake architecture), we speed up the original model more than 2.5x and divide its size by 50%, with just a few lines of simple Python code and without any accuracy drop!</p><figure id="14a4"><iframe frameborder="0" height="393" scrolling="no" src="https://www.youtube.com/embed/_AKFDOnrZz8?feature=oembed" width="700"></iframe></figure></div></div></section>
</section>
</article></body></html>
