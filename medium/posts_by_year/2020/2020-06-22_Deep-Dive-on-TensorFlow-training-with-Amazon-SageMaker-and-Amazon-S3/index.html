<!DOCTYPE html>

<html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>Deep Dive on TensorFlow training with Amazon SageMaker and Amazon S3</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<section class="e-content" data-field="body">
<section class="section"><div><hr/></div><div><div><h3 id="d857">Deep Dive on TensorFlow training with Amazon SageMaker and Amazon S3</h3><p id="e7f2"><em class="markup--p-em">This is a guest post by </em><strong class="markup--p-strong"><em class="markup--p-em">Chaim Rand</em></strong><em class="markup--p-em">, </em><strong class="markup--p-strong"><em class="markup--p-em">Machine Learning Algorithm Developer</em></strong><em class="markup--p-em"> at </em><strong class="markup--p-strong"><em class="markup--p-em">Mobileye</em></strong><em class="markup--p-em">. You can also read </em><a href="https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233" target="_blank"><em class="markup--p-em">part 1</em></a><em class="markup--p-em"> and </em><a href="https://towardsdatascience.com/tensorflow-performance-analysis-314b56dceb59" target="_blank"><em class="markup--p-em">part 3</em></a><em class="markup--p-em"> for more!</em></p><p id="0fe8">In a previous post, I told you the story of how my team at <a href="https://www.mobileye.com/" target="_blank">Mobileye</a> (officially known as Mobileye, an Intel company), transitioned to using the <a href="https://aws.amazon.com/sagemaker/" target="_blank"><strong class="markup--p-strong">Amazon SageMaker</strong></a> service to train our <a href="https://www.tensorflow.org/" target="_blank">TensorFlow</a> neural networks in the cloud. In particular, I told you about how one could use SageMaker <a href="https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/" target="_blank"><strong class="markup--p-strong">Pipe Mode</strong></a> to stream training data directly from <a href="https://aws.amazon.com/s3/" target="_blank">Amazon S3</a> storage to training instances, and how this leads to reductions in both training time and cost.</p><p id="a968">The easiest way to adopt Pipe Mode, is to use <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html" target="_blank"><em class="markup--p-em">PipeModeDataset</em></a>, a SageMaker implementation of the TensorFlow <a href="https://www.tensorflow.org/datasets" target="_blank"><em class="markup--p-em">Dataset</em></a> interface, which hides all the low level pipe management from you. Using <em class="markup--p-em">PipeModeDataset</em> requires reformatting your training data into one of the supported file formats (text records, <em class="markup--p-em">TFRecord</em> and <em class="markup--p-em">Protobuf</em>). We chose the <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank"><em class="markup--p-em">TFRecord</em></a> format. See the <a href="https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233" target="_blank">post</a> to learn more about how we converted our data, as well as additional challenges we faced and how we overcame them.</p><p id="54d4">In this post I want to discuss some <strong class="markup--p-strong">alternatives to using <em class="markup--p-em">PipeModeDataset</em></strong>. Of course, if you have a small dataset (e.g. a couple of hundred MB or a dataset that can easily fit on your training instance’s default EBS storage), you have the option of using <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html" target="_blank"><strong class="markup--p-strong">File Mode</strong></a>, which downloads all your data to your local training instance one time before starting training. But recall that we are training with mountains of data, often <strong class="markup--p-strong">more than 100 TB of data</strong>!</p><h3 id="ef70">Motivation</h3><p id="c547">You are, no doubt, wondering why would anyone need to consider alternatives to <em class="markup--p-em">PipeModeDataset</em>? Allow me to present a few scenarios.</p><h4 id="5b41">Increased control</h4><p id="dc03">As it stands today, when you choose to use Pipe Mode, you are handing over some of the control over your training flow to SageMaker. There may be situations when you want to have complete control. You may want to control precisely what files are being fed into your pipeline and in what order. You may want to control the mechanism being used to shuffle the files before each epoch. You might be facing some issues when using Pipe Mode, and want to use a method with greater visibility into the inner workings and better ability to debug. We will describe a way to do this using the built-in TensorFlow <a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" target="_blank"><em class="markup--p-em">TFRecordDataset</em></a>.</p><h4 id="078c">Dynamic boosting</h4><p id="ea87">In my previous post, I shared how we use boosting in our training to artificially increase the representation of certain subclasses of data (e.g. pink cars), and how we implemented this when training with Pipe Mode. Our solution was to configure our pipes with a SageMaker manifest file, pointing at <em class="markup--p-em">TFRecord</em> files for different classes, and weighted according to our desired boosting weights.</p><p id="04c1">In some cases, this might not be sufficient.We may need to perform <strong class="markup--p-strong">dynamic boosting</strong>, in which we continuously update the weights of each class during training. SageMaker does not (yet?) support modifying the manifest file during training. We will show two ways to implement this.</p><h4 id="8700">Complex data processing</h4><p id="4f3f">Suppose that you need to apply heavy processing on the input data pipeline. This processing could of course be implemented using TensorFlow operations directly on the <em class="markup--p-em">TFRecord</em> files, but there is no arguing that many operations can be implemented much more easily in Python using <em class="markup--p-em">numpy</em>, <em class="markup--p-em">Scikit-learn</em>, <em class="markup--p-em">opencv</em> and other libraries.</p><p id="efb0">Of course, you can create Python code using <em class="markup--p-em">tensorflow.py_func</em>, but this can easily become your bottleneck due to a combination of the Python Global Interpreter Lock (GIL) and the manner in which TensorFlow parallelizes the <em class="markup--p-em">py_func</em> in the <em class="markup--p-em">dataset</em> map call. If you have not heard of the python GIL, count your lucky blessings. Suffice to say, if you are trying to optimize your input data pipeline, you should try to avoid <em class="markup--p-em">py_func</em>.</p><p id="5bee">It would be preferable to stream your data in a Python friendly format, such as <em class="markup--p-em">pickle</em> or <em class="markup--p-em">numpy</em> files, perform the heavy processing in Python (using multiple processes to bypass the GIL limitations), and only then hand it over to TensorFlow. We will show how to do this by reading directly from the SageMaker pipe and using the <em class="markup--p-em">tf.from_generator</em> dataset.</p><h4 id="421a">Using other storage services</h4><p id="eb66">There may be reasons (though I can’t think of any good ones) why converting your data to one of the required data formats may be infeasible. Or reasons why you require random access to any of your records at any point in time. You want to be able to train as if you had all of your data stored locally, in whatever format. Such scenarios beg the use of the <a href="https://aws.amazon.com/fsx/" target="_blank">Amazon FSx</a>, as we describe below.</p><h3 id="76f1">Measuring Throughput</h3><p id="5ce6">Surely, I have convinced you that a discussion on alternatives to Pipe Mode is warranted. Before we jump into some of the options at your disposal, a few words about throughput.</p><p id="3b76">For us, one of the primary goals is to maximize the throughput of our training, that is, the number of training steps per second. Each of the options we will present have its pros and cons, but the key indicator by which we will compare them is how they impact throughput, i.e. at what rate can we pull the data from S3 and enter it into the training pipeline. If the input pipeline is not (and will not be) your bottleneck, or if you have all the time (and money) in the world to train, this might not be of concern to you. But it most certainly is to us.</p><p id="4cca">One easy way to isolate and test the throughput over the network is to simply iterate over your dataset without actually performing any training, as shown in the example below (written in TensorFlow 1.14). You can compare this to your training throughput to assess whether your bottleneck is IO.</p><pre class="graf--pre" id="c07c">iter = ds.make_one_shot_iterator()<br/>n = iter.get_next()<br/>count = 0<br/>begin = time.time()<br/>with tf.Session() as sess:<br/>    stime = time.time()<br/>    while count &lt; 10000:<br/>        sess.run(n)<br/>        count = count + 1<br/>        if count % 100 == 0:<br/>            etime = time.time()<br/>            print(“step: “+str(count)+” step/sec:  <br/>                {}”.format(float(100) / (etime — stime)))<br/>            stime = time.time()</pre><p class="graf-after--pre" id="ec24">Another consideration should be the network bandwidth of your training instance. AWS has published a <a href="https://aws.amazon.com/sagemaker/pricing/instance-types/" target="_blank">table</a> of the maximum expected network performance per instance type. You can use this table to assess whether you are getting the most bang for your buck. For example, if you are streaming 10 gigabits of data on an <em class="markup--p-em">ml.p3.8xlarge</em>, you will know that you can’t improve on that.</p><p id="7a26">However, there are some additional things to keep in mind. In particular, just because your instance has a certain bandwidth does not mean that you will be utilizing the full bandwidth when streaming from S3, EC2, etc. This might depend on a host of other things, such as the network bandwidth of the source, whether <a href="https://aws.amazon.com/blogs/aws/elastic-network-adapter-high-performance-network-interface-for-amazon-ec2/" target="_blank">Elastic Network Adapter</a> (ENA) is enabled, and to what degree the streaming functionality has been optimized (using multi-part downloading and other black magic).</p><p id="99c0">The advantage of Pipe Mode is that you don’t need to worry about all this. You can sleep easy knowing that Amazon SageMaker has highly optimized Pipe Mode and <em class="markup--p-em">PipeModeDataset</em>, and that you are most likely utilizing your bandwidth to its maximum. But, as you have already been convinced, there are situations where other options should be considered. We will present four alternatives:</p><ul class="postList"><li id="24dc"><em class="markup--li-em">TFRecordDatasets</em> pointing directly to files in S3,</li><li id="9312">Parsing the raw SageMaker input pipe,</li><li id="c5ad">Downloading data files directly from <a href="https://aws.amazon.com/fsx/" target="_blank">Amazon FSx for Lustre</a>,</li><li id="e702">Downloading data files directly from S3.</li></ul><h3 id="9c84">TFRecordDataset</h3><p id="e793"><a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" target="_blank"><em class="markup--p-em">TFRecordDataset</em></a> is a dataset implemented by Tensorflow that takes a list or a dataset of <em class="markup--p-em">TFRecord</em> file paths as input, and iterates over the <em class="markup--p-em">TFRecords</em> in each of the files.</p><p id="822e">The good news is that the <strong class="markup--p-strong">files paths can be S3 paths</strong>. Furthermore, the <em class="markup--p-em">TFRecordDataset</em> constructor has two control parameters, <em class="markup--p-em">buffer_size</em> and <em class="markup--p-em">num_parallel_reads</em> that can be used to optimize the IO throughput when reading from a remote location. Here is a simple example of how one can initialize a <em class="markup--p-em">TFRecordDataset</em> with the list of files from a SageMaker manifest file.</p><pre class="graf--pre" id="ad1f">def get_list_from_manifest(manifest_file):<br/>    with open(manifest_file) as f:<br/>        manifest = json.load(f)<br/>        prefix = manifest[0][‘prefix’]<br/>        return [path_join(prefix, tf) for tf in manifest[1:]]</pre><pre class="graf--pre graf-after--pre" id="b60f">tf_record_filepaths = get_list_from_manifest(manifest_file)</pre><pre class="graf--pre graf-after--pre" id="1b67">ds = tf.data.TFRecordDataset(tf_record_filepaths,  <br/>                             num_parallel_reads=10,    <br/>                             buffer_size=100000000)</pre><p class="graf-after--pre" id="4e7f">Note the settings for <em class="markup--p-em">buffer_size</em> and <em class="markup--p-em">num_parallel_reads</em>. With this example, <strong class="markup--p-strong">we were able to reach comparable throughput performance to that of <em class="markup--p-em">PipeModeDatase</em></strong><em class="markup--p-em">t</em> on an <em class="markup--p-em">ml.p3.8xlarge</em> instance (see evaluation below.)</p><p id="2301">There are a number of significant advantages to this method compared to <em class="markup--p-em">PipeModeDataset.</em></p><h4 id="2457">Greater control</h4><p id="ad4b">As mentioned above, <em class="markup--p-em">PipeModeDataset</em> is a bit of a black box. You provide a list of files, and rely on SageMaker to shuffle and stream these with little visibility (to date) into the internal workings, including what file is being streamed at any given moment. You are not able to change the list of files during training, and you cannot implement dynamic boosting.</p><p id="32b1">With <em class="markup--p-em">PipeModeDataset</em>, you can have greater control and greater visibility. This is demonstrated in the following pseudo-example showing how to implement dynamic boosting with two classes, while implementing our own shuffling and printing each file being streamed. This can be further optimized, and extended to a greater number of classes.</p><pre class="graf--pre" id="747c">import random, numpy as np, tensorflow as tf</pre><pre class="graf--pre graf-after--pre" id="4131">class MyIter(object):<br/>    def __init__(self,class1_list, class2_list)<br/>        self.class1_list = class1_list<br/>        self.class1_list_len = len(class1_list)<br/>        self.class1_index = 0<br/>        self.class2_list = class2_list<br/>        self.class2_list_len = len(class2_list)<br/>        self.class2_index = 0<br/>        self.boost_weight = 0.5<br/>        self.cont = True</pre><pre class="graf--pre graf-after--pre" id="b550"># this can be used to update weight mid-training<br/>def update_boost_weight(self, weight):<br/>    self.boost_weight = weight</pre><pre class="graf--pre graf-after--pre" id="eb6b">def hault(self):<br/>    self.cont = False</pre><pre class="graf--pre graf-after--pre" id="3aed">def from_class1(self):<br/>    if self.class1_index == 0:<br/>        random.shuffle(self.class1_list)<br/>    file_path = self.class1_list[self.class1_index]<br/>    self.class1_index = (self.class1_index+1)%self.class1_list_len<br/>    return file_path</pre><pre class="graf--pre graf-after--pre" id="b68c">def from_class2(self):<br/>    if self.class2_index == 0:<br/>    random.shuffle(self.class2_list)<br/>    file_path = self.class2_list[self.class2_index]<br/>    self.class2_index = (self.class2_index+1)%self.class2_list_len<br/>    return file_path</pre><pre class="graf--pre graf-after--pre" id="0586">def generator(self):<br/>    while self.cont:<br/>        class_type = np.random.choice(2, 1, <br/>            p=[self.boost_weight,1-self.boost_weight])[0]<br/>        next_file = self.from_class1() if class_type == 1 <br/>                   else self.from_class2()<br/>        print(“feeding file “+next_file)<br/>        yield next_file</pre><pre class="graf--pre graf-after--pre" id="6b84">myiter = MyIter(list1,list2)<br/>filepaths = tf.data.Dataset.from_generator(<br/>               myiter.generator,<br/>               output_types=tf.string,<br/>               output_shapes=())</pre><pre class="graf--pre graf-after--pre" id="5268">ds = tf.data.TFRecordDataset(<br/>               filepaths, <br/>               num_parallel_reads=10, <br/>               buffer_size=100000000)</pre><h4 class="graf-after--pre" id="5d30">Debugging</h4><p id="ec3d"><em class="markup--p-em">TFRecordDataset</em> offers advantages over <em class="markup--p-em">PipeModeDataset</em> when it comes to debugging. Other than the obvious advantages of debugging when you have greater visibility (e.g. identifying a corrupted <em class="markup--p-em">TFRecord</em> file), <strong class="markup--p-strong"><em class="markup--p-em">TFRecordDataset</em> also supports local mode </strong>unlike <em class="markup--p-em">PipeModeDataset</em>. This means that you can test your full pipeline, including the input pipeline, locally before uploading to a SageMaker instance.</p><h4 id="7578">Channel limit</h4><p id="e92e">Contrary to SageMaker Pipe Mode which has a 20 channels per device limit, there is no (documented) limitation to the number of <em class="markup--p-em">TFRecordDataset</em> you can create. Of course, each dataset requires system resources, so eventually, you will inevitably run up on some limit. To be honest, I did not check whether this is greater than 20, it probably depends on the system you are running on.</p><h4 id="29b3"><em class="markup--h4-em">TFRecordDataset or PipeModeDataset?</em></h4><p id="9ada">As discussed,<em class="markup--p-em"> TFRecordDataset</em> is very flexible. Still<em class="markup--p-em">, PipeModeDataset</em> has a number of advantages<em class="markup--p-em">.</em></p><ol class="postList"><li id="e1f2">When you use <em class="markup--li-em">TFRecordDataset,</em> you are inevitably using system resources (memory and CPU) to download and buffer your data. If you are CPU or memory bound, that will hurt. When you use Pipe Mode, Sagemaker provides these resources for this.</li><li id="54bb">Reaching comparable throughput performance with <em class="markup--li-em">TFRecordDataset</em> to that of pipe mode is no piece of cake. You will need to find the optimal control settings (e.g. <em class="markup--li-em">buffer_size</em> and <em class="markup--li-em">num_parallel_reads</em>) and even then there are no guarantees, as we explained above.</li><li id="96be"><em class="markup--li-em">PipeModeDataset</em> includes an API for distributing (sharding) the data across multiple instances. If you use <em class="markup--li-em">TFRecordDataset</em> you would need to program this explicitly.</li></ol><h3 id="cc6e">Parsing the Raw Pipe</h3><p id="bfeb">While <em class="markup--p-em">TFRecordDataset</em> supports a limited set of formats, you can stream anything you want over SageMaker pipes. For example, you could stream Python pickle files. Now, you probably won’t want to stream small pickle files, as this will have a negative impact on the pipe throughput, but rather you will want to concatenate many pickle files together into files, roughly of size 100MB. You can adopt the format of <em class="markup--p-em">TFRecord</em> files which is a concatenation of records of the format:</p><ul class="postList"><li id="fbbd">Length — 8 bytes</li><li id="eac3">Length CRC — 4 bytes</li><li id="1c94">Data — ‘Length’ bytes</li><li id="0633">Data CRC — 4 bytes</li></ul><p id="889a">In the example below, we are using this technique to create a dataset from a generator that manually parses our <em class="markup--p-em">TFRecord</em> files. This code snippet can be easily modified to parse files with concatenated <em class="markup--p-em">pickle</em> files, which you can follow up with your heavy python processing (using multiprocess to work around the GIL).</p><pre class="graf--pre" id="f66a">def get_pipe_generator(channel_name, num_epochs=1):<br/>    import struct<br/>    def generator():<br/>        for epoch in range(num_epochs):<br/>            fifo_path = ‘{0}/{1}_{2}’.format(data_dir, <br/>                                            channel_name, epoch)</pre><pre class="graf--pre graf-after--pre" id="9f00">with open(fifo_path, ‘rb’, buffering=0) as fifo:<br/>    cont = True<br/>    while cont:<br/>        try:<br/>            recordLen = fifo.read(8)<br/>            recordLen = struct.unpack(‘Q’, recordLen)[0]<br/>            # TODO: don’t be lazy!! Check the crc<br/>            crcCheck = struct.unpack(‘I’, fifo.read(4))[0]<br/>            # TODO: verify that you read recordLen bytes<br/>            rec = fifo.read(recordLen)<br/>            # TODO: don’t be lazy!! Check the crc<br/>            crcCheck = struct.unpack(‘I’, fifo.read(4))[0]<br/>            yield rec<br/>       except:<br/>            cont = False<br/>    return generator</pre><pre class="graf--pre graf-after--pre" id="4dcb">ds = tf.data.Dataset.from_generator(<br/>         get_pipe_generator(‘train’),<br/>         output_types=tf.string,<br/>         output_shapes=())</pre><p class="graf-after--pre" id="c63f">Working with raw pipes also offers you added flexibility. For example, suppose your data is divided into two separate classes streaming over two separate pipes. You could control the rate at which you sample data from each pipe. You can probably see where I am going with this… <strong class="markup--p-strong"><em class="markup--p-em">dynamic boosting!!</em></strong></p><h3 id="bb8d">Downloading and Parsing Data Files</h3><p id="184e">If you want both the visibility granted by working directly with S3 (as with the <em class="markup--p-em">TFRecordDataset</em> option), as well as the flexibility of working with a Python format such as <em class="markup--p-em">pickle</em> files, you will probably want to download and parse your data files on your own.</p><p id="3ff7">Here, we show an example in which we use multiple parallel threads to download and parse our <em class="markup--p-em">TFRecord</em> files. The records are fed into a shared queue, which can be fed into data processor threads, or directly into a dataset (using <em class="markup--p-em">tf.data.Dataset.from_generator</em>). Once again, this can easily be modified to work with other formats.</p><pre class="graf--pre" id="afe5">import struct, boto3, io<br/>from threading import Thread<br/>from queue import Queue as Q</pre><pre class="graf--pre graf-after--pre" id="81d3">def file_parser(index, file_list, batch_queue):<br/>    session = boto3.session.Session()<br/>    s3 = session.resource(‘s3’)<br/>    for s3_path in file_list:<br/>        bucket, key = s3_path.replace(‘s3://’,’’).split(‘/’, 1)<br/>        tc = boto3.s3.transfer.TransferConfig(<br/>            max_concurrency=10,<br/>            multipart_chunksize=8388608, <br/>            num_download_attempts=5,<br/>            max_io_queue=100, <br/>            io_chunksize=262144)<br/>    ioStream = io.BytesIO()<br/>    s3.meta.client.download_fileobj(bucket, key, ioStream, Config=tc)<br/>    ioStream.seek(0)<br/>    recordLen = ioStream.read(8)</pre><pre class="graf--pre graf-after--pre" id="ade7">    while recordLen:<br/>        recordLen = struct.unpack(‘Q’, recordLen)[0]<br/>        # TODO: don’t be lazy!! Check the crc<br/>        crcCheck = struct.unpack(‘I’, fifo.read(4))[0]<br/>        # TODO: verify that you read recordLen bytes<br/>        rec = fifo.read(recordLen)<br/>        # TODO: don’t be lazy!! Check the crc<br/>        crcCheck = struct.unpack(‘I’, fifo.read(4))[0]<br/>        batch_queue.put(rec,block=True)<br/>        recordLen = ioStream.read(8)<br/>        <br/>q = Q(maxsize=10)<br/>num_threads = 40</pre><pre class="graf--pre graf-after--pre" id="bea2">for i in range(num_threads):<br/>    w = Thread(target=file_parser, args=[files[i::num_threads], q])<br/>    w.start()</pre><p class="graf-after--pre" id="4c9a">The implementation above is somewhat naive. Reaching comparable throughput to that of <em class="markup--p-em">PipeModeDataset </em>will likely be very challenging. There are a number of controls you can use for tuning, including <em class="markup--p-em">num_threads, max_concurrency, multipart_chunksize,</em> and more.</p><h3 id="9aa8">Using Amazon FSx for Lustre</h3><p id="c6d9">Training on SageMaker using <a href="https://aws.amazon.com/fsx/" target="_blank">Amazon FSx</a> is a good solution for people who want to have traditional, file-system style access to their data, but cannot, or don’t want to, download their datasets to local EBS storage. Using FSx requires appropriate configuration as described <a href="https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/" target="_blank">here</a>.</p><p id="fc02">When you choose the storage capacity of your file system, make note of the throughput capacity, as this will impact the speed at which you will be able to access your data. Typically, the larger the storage capacity, the larger the throughput capacity. Unfortunately, the larger the storage capacity, the higher the cost of FSx. One advantage, however, is that the same file system can be used across multiple SageMaker jobs, which can reduce the cost per-session if you have many training sessions that access the same data.</p><p id="69d7">One thing to keep in mind is that according to the <a href="https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-sagemaker-works-with-amazon-fsx-lustre-amazon-efs-model-training/" target="_blank">documentation</a>, the files are only copied from S3 to FSx the first time they are accessed. This means that the throughput measured on your first epoch might be measurably lower that the throughput on subsequent epochs. Also, there is an option to <a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/import-data-repository.html" target="_blank"><strong class="markup--p-strong">preload</strong></a> data.</p><p id="a024">Bottom line, whether or not FSx is the right solution for you depends on a number of factors, including dataset size, FSx cost, the number of training sessions, and how badly you need file-system style access to your data.</p><h3 id="1e8e">Evaluation</h3><p id="e9d0">In the chart below we compare the throughput, measured in batches per second, for five different methods of pulling the training data from S3:</p><p id="a965">1. Using <em class="markup--p-em">PipeModeDataset</em> (‘<em class="markup--p-em">pipe</em>’)</p><p id="de9a">2. Using <em class="markup--p-em">TFRecordDataset</em> pointing to file objects in S3 (‘<em class="markup--p-em">tfr</em>’)</p><p id="9301">3. Reading and parsing the <em class="markup--p-em">TFRecord</em> files from raw SageMaker pipe, and wrapping these with <em class="markup--p-em">tf.data.Dataset.from_generator</em> (‘<em class="markup--p-em">pipe gen</em>’)</p><p id="7a00">4. Downloading the <em class="markup--p-em">TFRecord</em> files directly from S3, reading and parsing the them and wrapping them with <em class="markup--p-em">tf.data.Dataset.from_generator</em> (‘<em class="markup--p-em">file gen</em>’)</p><p id="39dc">5. Using <em class="markup--p-em">TFRecordDataset</em> pointing to the files in FSx (‘<em class="markup--p-em">fsx</em>’). <strong class="markup--p-strong">Please note that preloading was not used in this case.</strong></p><p id="80f7">All the trials were run on <em class="markup--p-em">ml.p3.8xlarge</em> (10 Gigabit per second network performance).</p><figure id="3b47"><img class="graf-image" src="image01.webp"/></figure><p id="3480">While the absolute values of the throughput should not mean anything to you (as they are dependent on our own data record size, batch size, etc.), the relative values clearly show <strong class="markup--p-strong">the throughput of the <em class="markup--p-em">tfr</em>, and <em class="markup--p-em">pipe gen</em> options to be comparable to <em class="markup--p-em">PipeModeDataset</em></strong>. Note that the performance of the <em class="markup--p-em">TFRecordDataset</em> is more choppy than the other options. If the throughput of the end to end training pipeline is sensitive to such choppiness, this may be something to consider.</p><p id="df86">The <em class="markup--p-em">file gen</em> trial timed out after only 7500 iterations… but I think you get the gist… Recall that this was a naive implementation and that it can most certainly be improved (as shown by the performance of <em class="markup--p-em">TFRecordDataset</em>).</p><p id="def9">The <em class="markup--p-em">fsx</em> throughput starts high, and then drops suddenly before timing out. The reason for this is that the while for the first 9600 iterations, we are accessing files that have already been copied to the file system, we then begin to read files that are being accessed for the first time. This means that FSx is only first copying them from S3. Remember, <strong class="markup--p-strong">this latency can be avoided by preloading all of your data</strong>.</p><h3 id="6498">Summary</h3><p id="d446">Amazon Sagemaker Pipe Mode is a great default choice. It is relatively easy to set up, maximizes network bandwidth utilization and hides all the nitty gritty. But, for those sticky situations such as those mentioned above, it’s good to know that there are other options, isn’t it?</p></div></div></section>
</section>
</article></body></html>
