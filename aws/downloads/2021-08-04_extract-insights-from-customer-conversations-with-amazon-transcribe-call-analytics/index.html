<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <style>
   body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        h1 { font-size: 2.5em; }
        h2 { font-size: 2em; }
        h3 { font-size: 1.5em; }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1em 0;
        }
        /* Image alignment classes for text wraparound */
        img.alignleft {
            float: left;
            margin: 0.5em 1.5em 1em 0;
        }
        img.alignright {
            float: right;
            margin: 0.5em 0 1em 1.5em;
        }
        img.aligncenter {
            display: block;
            margin: 1em auto;
            float: none;
        }
        img.alignnone {
            float: none;
            margin: 1em 0;
        }
        /* Clear floats after images */
        .wp-block-image::after,
        p:has(img.alignleft)::after,
        p:has(img.alignright)::after {
            content: "";
            display: table;
            clear: both;
        }
        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 1em;
            overflow-x: auto;
        }
        code {
            background: #f8f9fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 1em 0;
            padding-left: 1em;
            font-style: italic;
            color: #7f8c8d;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        p {
            margin-bottom: 1em;
        }
  </style>
 </head>
 <body>
  <h1>
   Extract Insights From Customer Conversations with Amazon Transcribe Call Analytics
  </h1>
  <p style="color: #666; font-style: italic; margin-bottom: 1em;">
   Published: 2021-08-04
  </p>
  <p style="color: #666; font-style: italic; margin-bottom: 2em;">
   Originally published at
   <a href="https://aws.amazon.com/blogs/aws/extract-insights-from-customer-conversations-with-amazon-transcribe-call-analytics/">
    https://aws.amazon.com/blogs/aws/extract-insights-from-customer-conversations-with-amazon-transcribe-call-analytics/
   </a>
  </p>
  <p>
   In 2017, we launched
   <a href="https://aws.amazon.com/transcribe/">
    Amazon Transcribe
   </a>
   , an automatic speech recognition (ASR) service that makes it easy to add speech-to-text capabilities to any application. Today, I’m very happy to announce the availability of
   <a href="https://aws.amazon.com/transcribe/call-analytics/">
    Amazon Transcribe Call Analytics
   </a>
   , a new feature that lets you easily extract valuable insights from customer conversations with a single API call.
  </p>
  <p>
   Each discussion with potential or existing customers is an opportunity to learn about their needs and expectations. For example, it’s important for customer service teams to figure out the main reasons why customers are calling them, and measure customer satisfaction during these calls. Likewise, salespeople try to gauge customer interest, and their reaction to a particular sales pitch.
  </p>
  <p>
   Thus, many customers and partners would like to add call analytics capabilities in different applications, regardless of their contact center provider. They often need to analyze more than phone calls, for example web-based audio and video calls. So far, they’ve typically done this by stitching AI services and dedicated ML models together, and they’ve asked us for a simpler solution.
  </p>
  <p>
   We got to work and built
   <a href="https://aws.amazon.com/transcribe/call-analytics/">
    Amazon Transcribe Call Analytics
   </a>
   , a new addition to Transcribe and a key enhancement to
   <a href="https://aws.amazon.com/machine-learning/contact-center-intelligence/">
    AWS Contact Center Intelligence
   </a>
   . If you can’t wait to try it, feel free to jump now to the
   <a href="https://console.aws.amazon.com/transcribe/#jobsAnalytics">
    AWS console
   </a>
   . If you’d like to learn more, read on!
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Introducing Amazon Transcribe Call Analytics
    </strong>
   </span>
   <br/>
   Based on ASR implemented in
   <span title="Amazon Transcribe">
    Transcribe
   </span>
   , Transcribe Call Analytics adds natural language processing (NLP) capabilities specifically trained on customer calls, and optimized to provide highly accurate call transcripts and actionable insights. With a simple API call, developers can now easily add call analytics to any application, and extract customer insights from conversations without having to build AI pipelines and train custom ML models.
  </p>
  <p>
   Key features of Transcribe Call Analytics include:
  </p>
  <ul>
   <li>
    Timestamped turn-by-turn call transcription in 21 languages.
   </li>
   <li>
    Issue detection, which picks up the shortest set of contiguous words in a conversation turn that represents the reason why the customer is calling. This works out of the box without any configuration or training.
   </li>
   <li>
    Call categorization based on conversational characteristics:
    <ul>
     <li>
      Matching specific words and phrases,
     </li>
     <li>
      Detecting non-talk time,
     </li>
     <li>
      Detecting interruptions,
     </li>
     <li>
      Analyzing sentiment for the customer and the agent.
     </li>
    </ul>
   </li>
   <li>
    Call characteristics such as:
    <ul>
     <li>
      How quickly and loudly a customer or agent are speaking,
     </li>
     <li>
      Detecting non-talk time,
     </li>
     <li>
      Detecting interruptions.
     </li>
    </ul>
   </li>
   <li>
    Redaction of sensitive data from the text transcript and the corresponding audio file.
   </li>
  </ul>
  <p>
   For example, you can create rules to flag calls where customers interrupt the agent, exhibit negative sentiment, and say “I want to speak with the manager”. These calls certainly did not go well, and are worth analyzing in detail! You can also look for calls where agents don’t use pre-defined greetings (“Welcome to ACME Support, how can I help you today?”) within the first 15 seconds, to measure script compliance and help supervisors identify agent coaching opportunities. Another popular scenario is to create rules that flag mentions of your specific products and services (“Your ACME Turbo 2000 vacuum cleaner isn’t working like it should”), in order to pick up any emerging trends you’d need to be aware of.
  </p>
  <p>
   Last but not least, you can further process the text transcript with other AI services such as
   <a href="https://aws.amazon.com/translate/">
    Amazon Translate
   </a>
   , or with custom NLP models built with
   <a href="https://aws.amazon.com/sagemaker/">
    Amazon SageMaker
   </a>
   .
  </p>
  <p>
   Now, let’s do a quick demo.
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Extracting Insights with Amazon Transcribe Call Analytics
    </strong>
   </span>
   <br/>
   Here’s a fictitious support call, where a lady calls her bank to report that she’s lost her credit and debit cards. The sound file is a stereo WAV file (16-bit, 8KHz).
  </p>
  <p>
   Transcribe Call Analytics requires that the agent and the customer are recorded in their own channel. We’ll also need to tell which is the agent channel. In a stereo file, the left channel is usually the first channel (channel #0), and the right channel is the second one (channel #1). This is the case for this call.
  </p>
  <p>
   If you’re not sure which is which, you can easily use the versatile
   <a href="https://ffmpeg.org/">
    <code>
     ffmpeg
    </code>
   </a>
   open source tool to extract each channel to a separate audio file.
  </p>
  <p>
   <code>
    $ ffmpeg -i demo-call.wav -map_channel 0.0.0 channel0.wav -map_channel 0.0.1 channel1.wav
   </code>
  </p>
  <p>
   You can use the same technique to extract audio channels from other file types, such as video files, and recombine them to a stereo audio file. You’ll find more information in the
   <a href="https://trac.ffmpeg.org/wiki/AudioChannelManipulation">
    <code>
     ffmpeg
    </code>
    documentation
   </a>
   .
  </p>
  <p>
   Now that I’m sure that the agent is in channel #1, I use the
   <a href="https://aws.amazon.com/cli/">
    AWS CLI
   </a>
   to upload the audio file to an S3 bucket.
  </p>
  <p>
   <code>
    $ aws s3 cp launch-call.wav s3://jsimon-transcribe-useast1/demo-call.wav --region us-east-1
   </code>
  </p>
  <p>
   Opening the Transcribe Call Analytics
   <a href="https://console.aws.amazon.com/transcribe/#jobsAnalytics">
    console
   </a>
   , I see that call category templates are available.
  </p>
  <p>
   <img alt="Call categories" class="wp-image-53569 size-full alignnone" height="577" src="image01.webp" style="border: 1px solid black;" width="1015"/>
  </p>
  <p>
   I decide to create one for supervisor escalations. Then, with a couple of clicks, I create a custom call category named
   <code>
    welcome-message
   </code>
   , to check if the agent starts the call with an appropriate welcome. I could add several phrases to check for if needed. We recommend that you use short sentences to minimize the chance of filler words popping up (‘hmm’, ‘err’, and so on).
  </p>
  <p>
   <img alt="Call category" class="size-full wp-image-53568 aligncenter" height="371" loading="lazy" src="image02.webp" style="border: 1px solid black;" width="719"/>
  </p>
  <p>
   Then, I create a call analytics job using the general model available in Transcribe. I also enable automatic language detection.
  </p>
  <p>
   <img alt="Creating a job" class="alignnone size-full wp-image-53570" height="651" loading="lazy" src="image03.webp" style="border: 1px solid black;" width="807"/>
  </p>
  <p>
   Then, I define the location of the audio file in S3, flagging channel #1 as the agent channel.
  </p>
  <p>
   <img alt="Creating a job" class="size-full wp-image-53577 aligncenter" height="284" loading="lazy" src="image04.webp" style="border: 1px solid black;" width="807"/>
  </p>
  <p>
   I decide to store the transcript in the default S3 bucket created by Transcribe in my account. I could also use my own bucket if needed. Then, I pick an
   <a href="https://aws.amazon.com/iam/">
    AWS Identity and Access Management (IAM)
   </a>
   role with sufficient permissions, and I launch the job.
  </p>
  <p>
   A minute later or so, the job is complete. The console contains a preview of the text transcript, as well as a link to the full JSON transcript.
  </p>
  <p>
   <img alt="Viewing the transcript" class="size-full wp-image-53578 aligncenter" height="285" loading="lazy" src="image05.webp" style="border: 1px solid black;" width="991"/>
  </p>
  <p>
   As the agent used the proper welcome sentence in the first 15 seconds, the call is tagged with the category I created earlier.
  </p>
  <p>
   <img alt="Call categories" class="alignnone size-full wp-image-53579" height="160" loading="lazy" src="image06.webp" style="border: 1px solid black;" width="969"/>
  </p>
  <p>
   Downloading the JSON transcript, each sentence in the conversation is enriched with metadata on per-word loudness, measured on a 0-100 range with 100 being extremely loud. Here’s the first sentence:
  </p>
  <p>
   <code>
    "BeginOffsetMillis":440,"EndOffsetMillis":4960,
    <br/>
    "Sentiment":"NEUTRAL",
    <br/>
    "ParticipantRole":"AGENT",
    <br/>
    "LoudnessScores":[78.68,80.4,81.91,78.95,82.34],
    <br/>
    "Content":"Hello and thank you for calling the bank. This is Ashley speaking, how may I help you today?"
   </code>
  </p>
  <p>
   Looking at the next sentence, I see that Transcribe Call Analytics automatically detected what the customer issue is. The corresponding text is in bold:
  </p>
  <p>
   <code>
    "Content": "Hi um uh you just need to
    <strong>
     cancel my card
    </strong>
    . Um I have a debit card and a credit card.",
   </code>
   <br/>
   <code>
    "IssuesDetected":
   </code>
   <code>
    [
   </code>
   <code>
    {
   </code>
   <code>
    "UnredactedCharacterOffsets":
   </code>
   <code>
    {
   </code>
   <code>
    "Begin": 26,
   </code>
   <code>
    "End": 40
   </code>
   <code>
    }}. . .
   </code>
  </p>
  <p>
   At the end of the transcript, I see global call statistics (duration, talk time, words per minute, matched categories). Transcribe also gives me overall sentiment information, meaured from -5 (extremely negative) to +5 (extremely positive). I also get a a breakdown in four quarters.
  </p>
  <p>
   <code>
    "Sentiment":{"OverallSentiment":{"AGENT":2.6,"CUSTOMER":0.2},
    <br/>
    "SentimentByPeriod":{"QUARTER":
    <br/>
    {"AGENT":[
    <br/>
    {"Score":1.9,"BeginOffsetMillis":0,"EndOffsetMillis":68457},
    <br/>
    {"Score":-0.7,"BeginOffsetMillis":68457,"EndOffsetMillis":136915},
    <br/>
    {"Score":5.0,"BeginOffsetMillis":136915,"EndOffsetMillis":205372},
    <br/>
    {"Score":3.0,"BeginOffsetMillis":205372,"EndOffsetMillis":273830}],
    <br/>
    "CUSTOMER":[
    <br/>
    {"Score":-1.7,"BeginOffsetMillis":0,"EndOffsetMillis":68165},
    <br/>
    {"Score":0.0,"BeginOffsetMillis":68165,"EndOffsetMillis":136330},
    <br/>
    {"Score":0.0,"BeginOffsetMillis":136330,"EndOffsetMillis":204495},
    <br/>
    {"Score":2.1,"BeginOffsetMillis":204495,"EndOffsetMillis":272660}]}}}
   </code>
  </p>
  <p>
   We can see that the customer started the call with negative sentiment, moving quickly to neutral sentiment, and ending the call with positive sentiment. This is a good sign that the call was handled satisfactorily, and that the customer problem was solved.
  </p>
  <p>
   If you’d like to convert the transcript to a Word document with additional visualizations, my colleague Andrew Kane has built a nice tool and made it available on
   <a href="https://github.com/aws-samples/amazon-transcribe-output-word-document">
    Github
   </a>
   . Here’s a sample report produced by his tool.
  </p>
  <p>
   <a href="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/08/03/Capture-d’écran-2021-08-03-à-18.53.03.png">
    <img alt="Andrew's tool" class="size-large wp-image-53794 aligncenter" height="859" loading="lazy" src="image07.webp" style="border: 1px solid black;" width="1024"/>
   </a>
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     AWS Customers and Partners Are Using Amazon Transcribe Call Analytics
    </strong>
   </span>
  </p>
  <p>
   <a href="https://www.linkedin.com/in/benrigby/">
    Ben Rigby
   </a>
   , the SVP, Global Head of Product &amp; Engineering, Artificial Intelligence, Automation, and Workforce at
   <a href="https://www.talkdesk.com/">
    Talkdesk
   </a>
   told us, “
   <em>
    Our customers are processing millions of customer service calls in their contact centers a year and have a critical need to extract actionable conversation insights to ensure positive business outcomes. As an AWS Contact Center Intelligence partner, we further enhanced our call transcription capabilities with Amazon Transcribe. With the launch of Amazon Transcribe Call Analytics, we’re excited to add even more AI capabilities to our Speech Analytics and QM Assist products. These deeper insights can provide agents and supervisors with the data they need to improve the speed and quality of their customer service while boosting workforce productivity.
   </em>
   ”
  </p>
  <p>
   <a href="https://www.linkedin.com/in/praphulkumar/">
    Praphul Kumar
   </a>
   , the Chief Product Officer of
   <a href="https://successkpi.com/">
    SuccessKPI
   </a>
   adds, “
   <em>
    Amazon Transcribe Call Analytics API enables us to add ML-based capabilities to our platform faster and at a lower cost. This new API removes the need to integrate multiple AI services together and develop custom machine learning models in certain areas. With Transcribe Call Analytics, we will be able to provide conversation insights such as sentiment, non-talk time, and call categories to gauge agent performance. This helps to drive better call outcomes, reduce agent turnover, uncover agent coaching opportunities, and measure call script compliance. Combining AWS services into SuccessKPI’s experience analytics platform was a no brainer. We are looking forward to bringing this valuable capability into the hands of large enterprises and government agencies.
   </em>
   ”
  </p>
  <p>
   <span style="text-decoration: underline;">
    <strong>
     Getting Started
    </strong>
   </span>
   <br/>
   A single API call is all it takes to extract rich insights from your customer conversations. You can start using
   <a href="https://aws.amazon.com/transcribe/call-analytics/">
    Amazon Transcribe Call Analytics
   </a>
   today in the following regions:
  </p>
  <ul>
   <li>
    US West (Oregon), US East (N. Virginia),
   </li>
   <li>
    Canada (Central),
   </li>
   <li>
    Europe (London), Europe (Frankfurt),
   </li>
   <li>
    Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Seoul), Asia Pacific (Tokyo), Asia Pacific (Sydney).
   </li>
  </ul>
  <p>
   Please give this new feature a try in the
   <a href="https://console.aws.amazon.com/transcribe/#jobsAnalytics">
    AWS console
   </a>
   , and let us know what you think. We always look forward to your feedback! You can send it through your usual AWS Support contacts or post it on the
   <a href="https://forums.aws.amazon.com/forum.jspa?forumID=278">
    AWS Forum
   </a>
   for Amazon Transcribe.
  </p>
  <p>
   One last thing: if you’re looking for an easy to use omnichannel cloud contact center, you should definitely take a look at
   <a href="https://aws.amazon.com/connect/">
    Amazon Connect
   </a>
   and its ML powered analytics,
   <a href="https://aws.amazon.com/connect/contact-lens/">
    Contact Lens
   </a>
   .
  </p>
  <a href="https://aws.amazon.com/developer/community/evangelists/julien-simon/">
   - Julien
  </a>
  <!-- '"` -->
 </body>
</html>
